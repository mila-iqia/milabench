bert-fp16:
  arg: --batch-size
  model:
    1: 4108.75 MiB
  optimized: 1
bert-fp32:
  arg: --batch-size
  model:
    1: 4206.75 MiB
  optimized: 1
bert-tf32:
  arg: --batch-size
  model:
    1: 4204.75 MiB
  optimized: 1
bert-tf32-fp16:
  arg: --batch-size
  model:
    1: 4108.75 MiB
  optimized: 1
bf16: {}
brax:
  arg: --batch-size
  model:
    1024: 4912.25 MiB
  optimized: 1
cleanrljax:
  arg: --num_steps
  optimized: 128
convnext_large-fp16:
  arg: --batch-size
  model:
    1: 3228.75 MiB
  optimized: 1
convnext_large-fp32:
  arg: --batch-size
  model:
    1: 3268.75 MiB
  optimized: 1
convnext_large-tf32:
  arg: --batch-size
  model:
    1: 3268.75 MiB
  optimized: 1
convnext_large-tf32-fp16:
  arg: --batch-size
  model:
    1: 3228.75 MiB
  optimized: 1
davit_large:
  arg: --batch-size
  model:
    1: 4882.75 MiB
  optimized: 1
davit_large-multi:
  arg: --batch-size
  model:
    1: 4862.75 MiB
  optimized: 1
diffusion-gpus:
  arg: --batch_size
  model:
    1: 23082 MiB
  optimized: 1
diffusion-nodes:
  arg: --batch_size
  model:
    1: 21686.75 MiB
  optimized: 1
diffusion-single:
  arg: --batch_size
  model:
    1: 21654.75 MiB
  optimized: 1
dimenet:
  arg: --batch-size
  model:
    2: 452.6875 MiB
  optimized: 1
dinov2-giant-gpus:
  arg: train.batch_size_per_gpu={batch_size}
  model:
    1: 32240.25 MiB
  optimized: 1
dinov2-giant-nodes:
  arg: train.batch_size_per_gpu={batch_size}
dinov2-giant-single:
  arg: train.batch_size_per_gpu={batch_size}
  model:
    1: 20682.25 MiB
  optimized: 1
dlrm: {}
dqn:
  arg: --buffer_batch_size
  model:
    1024: 81.81005859375 MiB
  optimized: 1
focalnet:
  arg: --batch-size
  model:
    1: 3128.75 MiB
  optimized: 1
fp16: {}
fp32: {}
lightning:
  arg: --batch-size
  model:
    1: 1054.25 MiB
  optimized: 1
lightning-gpus:
  arg: --batch-size
  model:
    1: 4542 MiB
  optimized: 1
llama: {}
llava-gpus:
  arg: --batch_size
  optimized: 1
llava-single:
  arg: --batch_size
  model:
    1: 72614.25 MiB
  optimized: 1
llm-full-mp-gpus:
  arg: batch_size={batch_size}
  model:
    1: 48964.25 MiB
  optimized: 1
llm-full-mp-nodes:
  arg: batch_size={batch_size}
  model:
    1: 37340.25 MiB
  optimized: 1
llm-lora-ddp-gpus:
  arg: batch_size={batch_size}
  model:
    1: 12418.75 MiB
  optimized: 1
llm-lora-ddp-nodes:
  arg: batch_size={batch_size}
  model:
    2: 17202.25 MiB
  optimized: 1
llm-lora-mp-gpus:
  arg: batch_size={batch_size}
  model:
    2: 38166.25 MiB
  optimized: 1
llm-lora-single:
  arg: batch_size={batch_size}
  model:
    1: 23196.75 MiB
  optimized: 1
opt-1_3b:
  arg: --per_gpu_batch_size
  model:
    1: 38102.375 MiB
  optimized: 1
opt-1_3b-multinode:
  arg: --per_gpu_batch_size
  model:
    1: 42126 MiB
  optimized: 1
opt-6_7b:
  arg: --per_gpu_batch_size
opt-6_7b-multinode:
  arg: --per_gpu_batch_size
  model:
    1: 55380 MiB
  optimized: 1
pna:
  arg: --batch-size
  model:
    4096: 39554.25 MiB
  optimized: 1
ppo:
  arg: --num_steps
  model:
    8: 80.791748046875 MiB
  optimized: 1
recursiongfn:
  arg: --batch_size
  model:
    2: 1134.75 MiB
  optimized: 1
reformer:
  arg: --batch-size
  model:
    1: 1916.75 MiB
  optimized: 1
regnet_y_128gf:
  arg: --batch-size
  model:
    1: 6876.75 MiB
  optimized: 1
resnet152:
  arg: --batch-size
  model:
    1: 2710.75 MiB
  optimized: 1
resnet152-ddp:
  arg: --batch-size
resnet152-ddp-gpus:
  arg: --batch-size
  model:
    1: 2084.75 MiB
  optimized: 1
resnet152-multi:
  arg: --batch-size
  model:
    1: 2600.75 MiB
  optimized: 1
resnet50:
  arg: --batch-size
  model:
    1: 1962.75 MiB
  optimized: 1
resnet50-noio:
  arg: --batch-size
  model:
    1: 1594.25 MiB
  optimized: 1
rlhf-gpus:
  arg: --per_device_train_batch_size
  model:
    1: 13448.25 MiB
  optimized: 1
rlhf-single:
  arg: --per_device_train_batch_size
  model:
    1: 8590.25 MiB
  optimized: 1
rwkv:
  arg: --micro_bsz
  model:
    1: 3602.75 MiB
  optimized: 1
stargan:
  arg: --batch_size
  model:
    1: 37896.75 MiB
  optimized: 1
super-slomo:
  arg: --train_batch_size
  model:
    1: 3016.75 MiB
  optimized: 1
t5:
  arg: --batch-size
  model:
    1: 4396.75 MiB
  optimized: 1
tf32: {}
torchatari:
  arg: --num-steps
  model:
    1: 1124.75 MiB
  optimized: 1
vjepa-gpus:
  arg: --batch_size
  model:
    1: 27196.25 MiB
  optimized: 1
vjepa-single:
  arg: --batch_size
  model:
    1: 6644.25 MiB
  optimized: 1
whisper:
  arg: --batch-size
  model:
    1: 2070.75 MiB
  optimized: 1
