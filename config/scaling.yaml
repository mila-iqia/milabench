bert-fp16:
  arg: --batch-size
  model:
    1: 4108.75 MiB
    2: 475.0 MiB
    4: 1840.375 MiB
    8: 8614.75 MiB
    16: 475.0 MiB
    32: 24604.75 MiB
    40: 34157.9375 MiB
    64: 47212.375 MiB
    80: 57619.9375 MiB
    96: 69812.375 MiB
    112: 81140.75 MiB
  optimized: 128
bert-fp32:
  arg: --batch-size
  model:
    1: 4206.75 MiB
    2: 475.0 MiB
    4: 6652.375 MiB
    8: 10240.75 MiB
    16: 475.0 MiB
    24: 28007.9375 MiB
    32: 31568.75 MiB
    64: 61196.375 MiB
    80: 76034.75 MiB
  optimized: 128
bert-tf32:
  arg: --batch-size
  model:
    1: 4204.75 MiB
    2: 475.0 MiB
    4: 6654.375 MiB
    8: 10242.75 MiB
    16: 475.0 MiB
    24: 28009.9375 MiB
    32: 31570.75 MiB
    64: 61198.375 MiB
    80: 76036.75 MiB
  optimized: 128
bert-tf32-fp16:
  arg: --batch-size
  model:
    1: 4108.75 MiB
    2: 475.0 MiB
    4: 1840.375 MiB
    8: 8614.75 MiB
    16: 475.0 MiB
    32: 24604.75 MiB
    40: 34157.9375 MiB
    64: 47212.375 MiB
    80: 57619.9375 MiB
    96: 69812.375 MiB
    112: 81140.75 MiB
  optimized: 128
bf16: {}
brax: {}
convnext_large-fp16:
  arg: --batch-size
  model:
    1: 3228.75 MiB
    2: 3844.375 MiB
    8: 4726.75 MiB
    16: 6254.75 MiB
    32: 9418.75 MiB
    40: 10940.75 MiB
    64: 15238.75 MiB
    128: 27466.75 MiB
    144: 34449.9375 MiB
    256: 51768.375 MiB
    288: 57966.375 MiB
    304: 65001.9375 MiB
    384: 76248.375 MiB
    416: 80628.75 MiB
  optimized: 128
convnext_large-fp32:
  arg: --batch-size
  model:
    1: 3268.75 MiB
    2: 3480.375 MiB
    4: 2060.75 MiB
    8: 5824.75 MiB
    16: 8774.75 MiB
    32: 14548.75 MiB
    64: 26274.75 MiB
    72: 31731.9375 MiB
    128: 47312.375 MiB
    160: 58608.375 MiB
    192: 70002.375 MiB
    216: 80694.75 MiB
  optimized: 128
convnext_large-tf32:
  arg: --batch-size
  model:
    1: 3268.75 MiB
    2: 3480.375 MiB
    8: 5824.75 MiB
    16: 1768.75 MiB
    32: 14548.75 MiB
    64: 26274.75 MiB
    72: 33081.9375 MiB
    128: 49678.375 MiB
    160: 61560.375 MiB
    192: 73548.375 MiB
    216: 80694.75 MiB
  optimized: 128
convnext_large-tf32-fp16:
  arg: --batch-size
  model:
    1: 3228.75 MiB
    2: 3844.375 MiB
    8: 4726.75 MiB
    16: 6254.75 MiB
    32: 9418.75 MiB
    40: 10940.75 MiB
    64: 15238.75 MiB
    128: 27466.75 MiB
    144: 34449.9375 MiB
    256: 51768.375 MiB
    288: 57966.375 MiB
    304: 61102.375 MiB
    384: 76248.375 MiB
    416: 80628.75 MiB
  optimized: 128
davit_large:
  arg: --batch-size
  model:
    1: 4882.75 MiB
    8: 6330.75 MiB
    16: 8216.75 MiB
    24: 10182.75 MiB
    32: 12240.75 MiB
    40: 16213.9375 MiB
    64: 19422.75 MiB
    96: 23732.375 MiB
    104: 29025.9375 MiB
    128: 34492.75 MiB
    224: 50258.375 MiB
    240: 57341.9375 MiB
    256: 56762.375 MiB
    288: 63488.375 MiB
    328: 81502.75 MiB
  optimized: 128
davit_large-multi:
  arg: --batch-size
  model:
    1: 4862.75 MiB
    8: 6330.75 MiB
    16: 8216.75 MiB
    24: 10730.75 MiB
    32: 12240.75 MiB
    40: 18087.9375 MiB
    64: 19422.75 MiB
    96: 25380.375 MiB
    104: 31099.9375 MiB
    128: 34248.75 MiB
    224: 52542.375 MiB
    240: 59663.9375 MiB
    256: 59080.375 MiB
    288: 65910.375 MiB
    328: 81742.75 MiB
  optimized: 128
diffusion-gpus:
  arg: --batch_size
  model:
    1: 23082 MiB
    2: 21818.75 MiB
    4: 23478.75 MiB
    8: 26500.75 MiB
    16: 36436.75 MiB
    32: 57808 MiB
    48: 80698 MiB
  optimized: 32
diffusion-nodes:
  arg: --batch_size
  model:
    1: 21686.75 MiB
    2: 21930.75 MiB
    4: 23510.75 MiB
diffusion-single:
  arg: --batch_size
  model:
    1: 21654.75 MiB
    2: 21818.75 MiB
    4: 23478.75 MiB
dimenet: {}
dinov2-giant-gpus:
  arg: train.batch_size_per_gpu={batch_size}
  model:
    32: 69614 MiB
  optimized: 32
dinov2-giant-single:
  arg: train.batch_size_per_gpu={batch_size}
dlrm: {}
focalnet:
  arg: --batch-size
  model:
    1: 3128.75 MiB
    4: 3320.375 MiB
    8: 4368.75 MiB
    16: 5608.75 MiB
    24: 10291.9375 MiB
    32: 8566.75 MiB
    40: 9850.75 MiB
    64: 14750.75 MiB
    128: 26398.75 MiB
    144: 29995.9375 MiB
    256: 44272.375 MiB
    288: 49730.375 MiB
    312: 56993.9375 MiB
    384: 66894.375 MiB
    424: 81368.75 MiB
  optimized: 128
fp16: {}
fp32: {}
lightning:
  arg: --batch-size
lightning-gpus:
  arg: --batch-size
  model:
    1: 4542 MiB
    2: 1158.75 MiB
    4: 1156.75 MiB
    8: 1260.75 MiB
    16: 4150.75 MiB
    128: 15858 MiB
  optimized: 16
llama: {}
llm-full-mp-gpus:
  arg: batch_size={batch_size}
llm-full-mp-nodes:
  arg: batch_size={batch_size}
llm-lora-ddp-gpus:
  arg: batch_size={batch_size}
  model:
    1: 12418.75 MiB
llm-lora-ddp-nodes:
  arg: batch_size={batch_size}
llm-lora-mp-gpus:
  arg: batch_size={batch_size}
llm-lora-single:
  arg: batch_size={batch_size}
  model:
    1: 23196.75 MiB
    2: 27694.75 MiB
    16: 45076.75 MiB
opt-1_3b:
  arg: --per_gpu_batch_size
  model:
    1: 38102.375 MiB
  optimized: 1
opt-1_3b-multinode:
  arg: --per_gpu_batch_size
  model:
    1: 42126 MiB
  optimized: 1
opt-6_7b:
  arg: --per_gpu_batch_size
opt-6_7b-multinode:
  arg: --per_gpu_batch_size
  model:
    1: 55380 MiB
  optimized: 1
recursiongfn:
  arg: --batch_size
  model:
    2: 1134.75 MiB
    4: 1140.75 MiB
reformer:
  arg: --batch-size
  model:
    1: 1916.75 MiB
    4: 3004.375 MiB
    8: 4512.75 MiB
    16: 7082.75 MiB
    24: 10470.75 MiB
    32: 13454.75 MiB
    64: 25408.75 MiB
    72: 32287.9375 MiB
    128: 49276.375 MiB
    160: 61212.375 MiB
    192: 73148.375 MiB
    208: 79120.75 MiB
  optimized: 128
regnet_y_128gf:
  arg: --batch-size
  model:
    1: 6876.75 MiB
    2: 475.0 MiB
    4: 9062.375 MiB
    8: 8524.75 MiB
    16: 1234.75 MiB
    24: 18523.9375 MiB
    32: 18324.75 MiB
    56: 31165.9375 MiB
    64: 31558.75 MiB
    128: 54094.375 MiB
    136: 61245.9375 MiB
    160: 64990.375 MiB
    184: 78714.75 MiB
  optimized: 128
resnet152:
  arg: --batch-size
  model:
    1: 2710.75 MiB
    8: 3298.75 MiB
    16: 4164.75 MiB
    32: 6202.75 MiB
    40: 9819.9375 MiB
    64: 10120.75 MiB
    72: 10860.75 MiB
    96: 11546.375 MiB
    104: 16105.9375 MiB
    128: 18076.75 MiB
    224: 24584.375 MiB
    256: 27310.375 MiB
    448: 46894.375 MiB
    472: 52891.9375 MiB
    512: 52622.375 MiB
    576: 58588.375 MiB
    640: 81354.75 MiB
  optimized: 128
resnet152-ddp:
  arg: --batch-size
resnet152-ddp-gpus:
  arg: --batch-size
  model:
    1: 2084.75 MiB
    2: 2122.75 MiB
    4: 2260.75 MiB
resnet152-multi:
  arg: --batch-size
  model:
    1: 2600.75 MiB
    8: 3374.75 MiB
    16: 4148.75 MiB
    32: 6374.75 MiB
    40: 11181.9375 MiB
    64: 10338.75 MiB
    72: 10582.75 MiB
    96: 13170.375 MiB
    104: 17773.9375 MiB
    128: 18104.75 MiB
    224: 27566.375 MiB
    256: 29176.375 MiB
    448: 50024.375 MiB
    472: 56233.9375 MiB
    512: 55924.375 MiB
    576: 62102.375 MiB
    640: 81820.75 MiB
  optimized: 128
resnet50:
  arg: --batch-size
  model:
    1: 1962.75 MiB
    8: 2134.75 MiB
    16: 2460.75 MiB
    32: 3206.75 MiB
    40: 7439.9375 MiB
    64: 4734.75 MiB
    96: 6478.375 MiB
    112: 11103.9375 MiB
    128: 8242.75 MiB
    184: 11072.75 MiB
    256: 14854.75 MiB
    264: 19031.9375 MiB
    512: 27900.75 MiB
    544: 29358.375 MiB
    560: 34017.9375 MiB
    1024: 53806.375 MiB
    1152: 60310.375 MiB
    1440: 74694.375 MiB
    1552: 81146.75 MiB
    1560: 81590.75 MiB
  optimized: 64
resnet50-noio:
  arg: --batch-size
rwkv:
  arg: --micro_bsz
  model:
    1: 3602.75 MiB
    8: 4530.75 MiB
    16: 5594.75 MiB
    64: 11452.75 MiB
    128: 19448.75 MiB
    632: 81880.75 MiB
  optimized: 16
stargan:
  arg: --batch_size
  model:
    1: 37896.75 MiB
    8: 19165.75 MiB
    16: 62478.375 MiB
    32: 73824.75 MiB
  optimized: 16
super-slomo:
  arg: --train_batch_size
  model:
    1: 3016.75 MiB
    2: 3506.75 MiB
    4: 5884.375 MiB
    8: 10288.75 MiB
    16: 16914.75 MiB
    24: 29777.9375 MiB
    32: 33934.375 MiB
    56: 61837.9375 MiB
    64: 66072.375 MiB
    80: 81180.75 MiB
  optimized: 32
t5:
  arg: --batch-size
  model:
    1: 4396.75 MiB
    2: 6384.375 MiB
    4: 10620.375 MiB
    8: 18684.75 MiB
    16: 33990.75 MiB
    24: 54479.9375 MiB
    32: 66760.375 MiB
  optimized: 128
tf32: {}
torchatari:
  arg: --num-steps
  model:
    1: 1124.75 MiB
    2: 1138.75 MiB
    4: 1166.75 MiB
whisper:
  arg: --batch-size
  model:
    1: 2070.75 MiB
    4: 3828.375 MiB
    8: 6108.75 MiB
    16: 10540.75 MiB
    24: 18887.9375 MiB
    32: 19282.75 MiB
    48: 31841.9375 MiB
    64: 36728.75 MiB
    96: 54086.375 MiB
    104: 62409.9375 MiB
    128: 71634.375 MiB
    144: 80412.75 MiB
  optimized: 128


llava-single:
  arg: --batch_size
  optimized: 1

llava-gpus:
  arg: --batch_size
  optimized: 1

rlhf-single:
  arg: --per_device_train_batch_size
  optimized: 64

rlhf-gpus:
  arg: --per_device_train_batch_size
  optimized: 64

vjepa-single:
  arg: --batch_size
  optimized: 24

vjepa-gpus:
  arg: --batch_size
  optimized: 24

ppo:
  arg: --num_minibatches
  optimized: 32

dqn:
  arg: --buffer_batch_size
  optimized: 128