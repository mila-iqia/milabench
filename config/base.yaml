_defaults:
  max_duration: 600
  voir:
    options:
      stop: 60
      interval: "1s"

  validation:
    usage:
      gpu_load_threshold: 0.5
      gpu_mem_threshold: 0.5

_torchvision:
  inherits: _defaults
  definition: ../benchmarks/torchvision
  group: torchvision
  install_group: torch
  plan:
    method: per_gpu
  argv:
    --precision: 'tf32-fp16'
    --lr: 0.01
    --no-stdout: true
    --epochs: 50
    --num-workers: "auto({n_worker}, 8)"
    --loader: pytorch
    --data: "{milabench_data}/FakeImageNet"


_torchvision_ddp:
  inherits: _defaults
  definition: ../benchmarks/torchvision_ddp
  group: torchvision
  install_group: torch
  plan:
    method: njobs
    n: 1
  argv:
    --epochs: 10
    --num-workers: "auto({n_worker}, 8)"
    --loader: pytorch
    --data: "{milabench_data}/FakeImageNet"

_flops:
  inherits: _defaults
  definition: ../benchmarks/flops
  group: flops
  install_group: torch
  plan:
    method: per_gpu
  
  tags:
    - diagnostic
    - flops
  
  argv:
    --number: 10
    --repeat: 90

llama:
  inherits: _defaults
  definition: ../benchmarks/llama
  group: llm
  install_group: torch
  max_duration: 800
  tags:
    - nlp
    - llm
    - inference

  voir:
    options:
      stop: 30
      interval: "1s"
    
  plan:
    method: per_gpu

  # Note: when NOT using pretrained model
  # the benchmark becomes much harder as no end token is ever outputted by the model
  # which makes inference much slower
  # argv:
  #  --pretrained: true

_hf:
  inherits: _defaults
  definition: ../benchmarks/huggingface
  group: hf
  install_group: torch
  argv:
    --precision: 'tf32-fp16'
    --num-workers: "auto({n_worker}, 8)"

  plan:
    method: per_gpu

_timm:
  inherits: _defaults
  definition: ../benchmarks/timm
  group: timm
  install_group: torch
  tags:
    - timm
  plan:
    method: per_gpu
  argv:
    --amp: true
    --amp-dtype: bfloat16
    --device: '{device_name}'
    --val-split: ''
    --data-dir: "{milabench_data}"
    --dataset: "FakeImageNet"
    --workers: "auto({n_worker}, 8)"


_accelerate_opt:
  inherits: _defaults
  tags:
    - nlp
    - language-modeling
    - transformer
    - huggingface
    - llm
  definition: ../benchmarks/accelerate_opt
  group: opt
  install_group: torch
  plan:
    method: njobs
    n: 1
  
  # Accelerate
  # accelerate:
  #   - --mixed_precision=bf16
  #   - --dynamo_backend=no
  #   - --gradient_accumulation_steps=1

  # Script Args
  argv:
    --max_train_steps: 100
    --dataset_name: "wikitext"
    --dataset_config_name: "wikitext-103-v1"
    --dataset_rev: "b08601e"
    --validation_split_percentage: 5
    --per_gpu_batch_size: 1
    --cpus_per_gpu: "auto({n_worker}, 8)"
    --cache: "{milabench_cache}"

  # Accelerate Args
  use_deepspeed: true
  num_machines: 1


fp16:
  inherits: _flops

  argv:
    --number: 30
    --repeat: 90
    --m: 8192
    --n: 8192
    --dtype: fp16

bf16:
  inherits: _flops
 
  argv:
    --m: 8192
    --n: 8192
    --dtype: bf16

tf32:
  inherits: _flops
 
  argv:
    --m: 8192
    --n: 8192
    --dtype: fp32
    --tf32: true

fp32:
  inherits: _flops
 
  argv:
    --m: 8192
    --n: 8192
    --dtype: fp32  

resnet50:
  inherits: _torchvision
  tags:
    - vision
    - classification
    - convnet
    - resnet
  
  argv:
    --model: resnet50
    --batch-size: 256
    --num-workers: "auto({n_worker}, 8)"
    --loader: pytorch
  
resnet50-noio:
  inherits: _torchvision
  tags:
    - vision
    - classification
    - convnet
    - resnet
    - noio
  
  argv:
    --model: resnet50
    --batch-size: 256
    --loader: synthetic_fixed

resnet152-ddp-gpus:
  inherits: _torchvision_ddp
  tags:
    - vision
    - classification
    - convnet
    - resnet
  
  argv:
    --model: resnet152
    --batch-size: 256
    --num-workers: "auto({n_worker}, 8)"
    --loader: torch

_convnext_large-base:
  inherits: _torchvision
  tags:
    - vision
    - classification
    - convnet
    - precision-showcase
  argv:
    --model: convnext_large
    --batch-size: 128
  voir:
    options:
      stop: 30

convnext_large-fp32:
  inherits: _convnext_large-base
  argv:
    --precision: 'fp32'

convnext_large-fp16:
  inherits: _convnext_large-base
  argv:
    --precision: 'fp16'

convnext_large-tf32:
  inherits: _convnext_large-base
  argv:
    --precision: 'tf32'

convnext_large-tf32-fp16:
  inherits: _convnext_large-base
  argv:
    --precision: 'tf32-fp16'

regnet_y_128gf:
  inherits: _torchvision
  tags:
    - vision
    - classification
    - convnet
    - resnet
    - lstm
  argv:
    --model: regnet_y_128gf
    --batch-size: 64

_bert-base:
  inherits: _hf
  tags:
    - nlp
    - language-modeling
    - transformer
    - huggingface
    - precision-showcase
    - noio
  argv:
    --model: "Bert"
    --batch-size: 32
  voir:
    options:
      stop: 30

bert-fp32:
  inherits: _bert-base
  argv:
    --precision: 'fp32'

bert-fp16:
  inherits: _bert-base
  argv:
    --precision: 'fp16'

bert-tf32:
  inherits: _bert-base
  argv:
    --precision: 'tf32'

bert-tf32-fp16:
  inherits: _bert-base
  argv:
    --precision: 'tf32-fp16'

t5:
  inherits: _hf
  tags:
    - nlp
    - language-modeling
    - transformer
    - huggingface
    - noio
  argv:
    --model: "T5"
    --batch-size: 16

reformer:
  inherits: _hf
  tags:
    - nlp
    - language-modeling
    - transformer
    - huggingface
    - noio
  argv:
    --model: "Reformer"
    --batch-size: 64

whisper:
  inherits: _hf
  tags:
    - audio
    - huggingface
    - noio
  argv:
    --model: "Whisper"
    --batch-size: 64

focalnet:
  inherits: _timm
  tags:
    - vision
    - classification
    - convnet
  plan:
    method: per_gpu
  argv:
    --model: focalnet_base_lrf

super-slomo:
  inherits: _defaults
  tags:
    - vision
    - video-interpolation
    - unet
    - convnet
  definition: ../benchmarks/super-slomo
  group: super-slomo
  install_group: torch
  plan:
    method: per_gpu
  argv:
    --train_batch_size: 64
    --dataset_root: "{milabench_data}/FakeImageNet"
    --loader: pytorch
    --num_workers: "auto({n_worker}, 8)"

brax:
  inherits: _defaults
  tags:
    - rl
    - jax
  definition: ../benchmarks/brax
  group: brax
  install_group: torch
  plan:
    method: njobs
    n: 1
  argv:
    --episode-length: 20
    --batch-size: 1024
    --num-minibatches: 32
    --num-envs: 8192


_diffusion:
  inherits: _defaults
  definition: ../benchmarks/diffusion
  install_group: torch
  plan:
    method: njobs
    n: 1
  
  argv:
    --num_epochs: 5
    --batch_size: 32
    --num_workers: "auto({n_worker}, 8)"
    --cache: "{milabench_cache}"

diffusion-single:
  inherits: _diffusion
  num_machines: 1
  plan:
    method: njobs

diffusion-gpus:
  inherits: _diffusion
  num_machines: 1

diffusion-nodes:
  tags:
    - multinode
  inherits: _diffusion
  num_machines: 2
  requires_capabilities:
    - "len(nodes) >= ${num_machines}"

_lightning:
  inherits: _defaults
  definition: ../benchmarks/lightning
  install_group: torch
  argv:
    --epochs: 10
    --num-workers: "auto({n_worker}, 8)"
    --loader: pytorch
    --data: "{milabench_data}/FakeImageNet"
    --model: resnet152
    --batch-size: 256

lightning:
  inherits: _lightning
  num_machines: 1
  plan:
    method: per_gpu

lightning-gpus:
  inherits: _lightning
  num_machines: 1
  plan:
    method: njobs
    n: 1

_dinov2:
  inherits: _defaults
  definition: ../benchmarks/dinov2
  install_group: torch
  num_machines: 1
  plan:
    method: njobs
    n: 1

  argv:
    --output-dir: "{milabench_extra}/output"
    --no-resume: true

dinov2-giant-single:
  inherits: _dinov2
  plan:
    method: per_gpu

  argv:
    --config-file: "{benchmark_folder}/src/dinov2/configs/train/vitg14.yaml"
    # THOSE NEED TO BE LAST
    train.dataset_path=ImageNet:split=TRAIN:root={milabench_data}/FakeImageNet:extra={milabench_data}/FakeImageNet: true
    train.batch_size_per_gpu=32: true
    train.saveckp_freq=100: true
    train.num_workers=10: true

dinov2-giant-gpus:
  inherits: _dinov2
  argv:
    --config-file: "{benchmark_folder}/src/dinov2/configs/train/vitg14.yaml"
    # THOSE NEED TO BE LAST
    train.dataset_path=ImageNet:split=TRAIN:root={milabench_data}/FakeImageNet:extra={milabench_data}/FakeImageNet: true
    train.batch_size_per_gpu=32: true
    train.saveckp_freq=100: true
    train.num_workers=10: true

dinov2-giant-nodes:
  enabled: false
  tags:
    - multinode
  max_duration: 3600
  inherits: _dinov2
  argv:
    --config-file: "{benchmark_folder}/src/dinov2/configs/train/vitg14.yaml"
    # THOSE NEED TO BE LAST
    train.dataset_path=ImageFolder:root={milabench_data}/FakeImageNet: true
    train.batch_size_per_gpu=12: true
    train.saveckp_freq=100: true
    train.num_workers=10: true

  num_machines: 2
  requires_capabilities:
    - "len(nodes) >= ${num_machines}"

_llm:
  voir:
    options:
      stop: 30
  
  max_duration: 1200
  num_machines: 1
  inherits: _defaults
  definition: ../benchmarks/llm
  install_group: torch


llm-lora-single:
  inherits: _llm
  plan:
    method: per_gpu

  argv:
    "{milabench_code}/recipes/lora_finetune_single_device.py": true
    --config: "{milabench_code}/configs/llama3_8B_lora_single_device.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_8B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_8B/original: true
    checkpointer.output_dir={milabench_data}/llama3_8B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-8B": true
    batch_size=8: true
    gradient_accumulation_steps=8: true


llm-lora-ddp-gpus:
  inherits: _llm
  plan:
    method: njobs
    n: 1
  
  argv:
    "{milabench_code}/recipes/lora_finetune_distributed.py": true
    --config: "{milabench_code}/configs/llama3_8B_lora_single_device.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_8B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_8B/original: true
    checkpointer.output_dir={milabench_data}/llama3_8B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-8B": true
    batch_size=8: true
    gradient_accumulation_steps=8: true


llm-lora-ddp-nodes:
  tags:
    - multinode
  max_duration: 3600
  inherits: _llm
  plan:
    method: njobs
    n: 1
  
  argv:
    "{milabench_code}/recipes/lora_finetune_distributed.py": true
    --config: "{milabench_code}/configs/llama3_8B_lora_single_device.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_8B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_8B/original: true
    checkpointer.output_dir={milabench_data}/llama3_8B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-8B": true
    batch_size=8: true
    gradient_accumulation_steps=8: true

  num_machines: 2
  requires_capabilities:
    - "len(nodes) >= ${num_machines}"


llm-lora-mp-gpus:
  inherits: _llm
  plan:
    method: njobs
    n: 1

  argv:
    "{milabench_code}/recipes/lora_finetune_distributed.py": true
    --config: "{milabench_code}/configs/llama3_70B_lora.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_70B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_70B: true
    checkpointer.output_dir={milabench_data}/llama3_70B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-70B": true
    batch_size=8: true
    gradient_accumulation_steps=1: true


llm-full-mp-gpus:
  inherits: _llm
  plan:
    method: njobs
    n: 1

  argv:
    "{milabench_code}/recipes/full_finetune_distributed.py": true
    --config: "{milabench_code}/configs/llama3_70B_full.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_70B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_70B: true
    checkpointer.output_dir={milabench_data}/llama3_70B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-70B": true
    safetensors=true: true
    batch_size=2: true
    gradient_accumulation_steps=1: true


llm-full-mp-nodes:
  tags:
    - multinode
  max_duration: 3600
  inherits: _llm
  plan:
    method: njobs
    n: 1

  argv:
    "{milabench_code}/recipes/full_finetune_distributed.py": true
    --config: "{milabench_code}/configs/llama3_70B_full.yaml"
    epochs=1: true
    output_dir={milabench_extra}/output: true
    tokenizer.path={milabench_data}/llama3_70B/original/tokenizer.model: true
    checkpointer.checkpoint_dir={milabench_data}/llama3_70B: true
    checkpointer.output_dir={milabench_data}/llama3_70B/: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-70B": true
    safetensors=true: true
    batch_size=2: true
    gradient_accumulation_steps=1: true

  num_machines: 2
  requires_capabilities:
    - "len(nodes) >= ${num_machines}"


_geo_gnn:
  inherits: _defaults
  definition: .
  # FIXME: torch cluster is laging behind pytorch
  # we are forced to use torch==2.3 instead of torch==2.4
  install_group: gnn
  group: geo_gnn
  definition: ../benchmarks/geo_gnn
  plan:
    method: per_gpu

dimenet:
  inherits: _geo_gnn
  argv:
    --model: 'DimeNet'
    --num-samples: 10000
    --use3d: True


recursiongfn:
  inherits: _defaults
  definition: ../benchmarks/recursiongfn
  install_group: gnn
  group: recursiongfn_gnn
  plan:
    method: per_gpu

  argv:
    --batch_size: 128
    --num_workers: 8
    --num_steps: 100
    --layer_width: 128
    --num_layers: 4


torchatari:
  inherits: _defaults
  definition: ../benchmarks/torchatari
  install_group: torch
  plan:
    method: per_gpu

  argv:
    --num-minibatches: 16
    --update-epochs: 4
    --num-steps: 128
    --num-envs: auto({cpu_per_gpu}, 128)
    --total-timesteps: 1000000
    --env-id: Breakout-v5
