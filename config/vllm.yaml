include:
  - inference.yaml
  - standard.yaml

_defaults:
  enabled: false
  max_duration: 600
  voir:
    options:
      stop: 500
      gpu_poll: 0.25

  validation:
    usage:
      gpu_load_threshold: 0.5
      gpu_mem_threshold: 0.5


vllm-dense-physics-gpus:
  enabled: true

vllm-moe-code-gpus:
  enabled: true

whisper-transcribe-single:
  enabled: true

txt-to-image-gpus:
  enabled: true

llm-chat-completion:
  enabled: true

vllm-sweep-dense:
  matrix:
    # effective_max_context_per_sequence ≤ max_num_batched_tokens / max_num_seqs
    concurrency: [8, 64, 512]

  job:
    name: 'vllm-sweep-dense-conc{concurrency}'
    inherits: _vllm
    enabled: true
    weight: 1
    tags:
      - multigpu
      - mistral

    num_machines: 1
    plan:
      method: njobs
      n: 1

    client:
      argv:
          --dataset-name: hf
          --dataset-path: hendrydong/gpqa_diamond
          --hf-name: hendrydong/gpqa_diamond
          --num-prompts: "expr({concurrency} * 30)"
          --model: mistralai/Mistral-Small-3.1-24B-Instruct-2503
          --max-concurrency: "{concurrency}"

    server:
      argv:
          mistralai/Mistral-Small-3.1-24B-Instruct-2503: true
          --tensor-parallel-size: "{gpu_count}"
          --tokenizer-mode: mistral
          --config_format: mistral
          --load_format: mistral 
          --tool-call-parser: mistral
          --enable-auto-tool-choice: true
          --limit-mm-per-prompt: '{"image":10}' 
          --max-num-seqs: "{concurrency}"

vllm-sweep-moe:
  matrix:
    # effective_max_context_per_sequence ≤ max_num_batched_tokens / max_num_seqs

    concurrency: [8, 64, 512]

    # max-num-batched-tokens: 8192: is fixed in InferenceMax
    # max-num-batched-tokens ≥ max-model-len × max-num-seqs
    max_batched_token: [4096]

    # max-model-len: 10240 is fixed in InferenceMAX
    max_context: [10240]       # [16384, 32768]

  job:
    name: 'vllm-sweep-conc{concurrency}-mxbt{max_batched_token}-moe'
    inherits: _vllm
    enabled: true
    max_duration: 3600
    tags:
      - multigpu
  
    plan:
      method: njobs
      n: 1

    client:
      argv:
        --dataset-name: hf
        --dataset-path: vdaita/edit_10k_char
        --hf-name: vdaita/edit_10k_char
        --num-prompts: "expr({concurrency} * 30)"
        --model: meta-llama/Llama-4-Scout-17B-16E
        --max-concurrency: "{concurrency}"

    server:
      argv:
          meta-llama/Llama-4-Scout-17B-16E: true
          --tensor-parallel-size: "{gpu_count}"
          --limit-mm-per-prompt: '{"image":10}' 
          --served-model-name: model
          --dtype: bfloat16

          # ---
          --gpu-memory-utilization: 0.9
          --async-scheduling: true
          --no-enable-prefix-caching: true

          --max-num-seqs: "{concurrency}"

          # # We recommend you set max_num_batched_tokens > 2048 for throughput.
          --max-num-batched-tokens: "{max_batched_token}"

          # # Model context length (prompt and output)
          --max-model-len: "{max_context}"