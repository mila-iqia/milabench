_vllm:
  inherits: _defaults
  definition: ../benchmarks/vllm
  install_group: torch
  enabled: true
  tags:
    - inference

  # Inference Server Args
  server:
   argv:
      --served-model-name: model
      --dtype: bfloat16

  # Benchmarking client Args
  client:
    argv:
      --model: meta-llama/Meta-Llama-3-8B-Instruct
      --served-model-name: model
      --request-rate: inf
      --dataset-name: random
      --label: milabench
      --backend: openai 
      --num-prompts: 198


vllm-dense-physics-gpus:
  inherits: _vllm
  tags:
    - multigpu

  num_machines: 1
  plan:
    method: njobs
    n: 1

  client:
    argv:
      --dataset-name: hf
      --dataset-path: hendrydong/gpqa_diamond
      --hf-name: hendrydong/gpqa_diamond
      --num-prompts: 500

  server:
   argv:
      mistralai/Mistral-Small-3.1-24B-Instruct-2503: true
      --tensor-parallel-size: "{gpu_count}"
      --tokenizer-mode: mistral
      --config_format: mistral
      --load_format: mistral 
      --tool-call-parser: mistral
      --enable-auto-tool-choice: true
      --limit-mm-per-prompt: '{"image":10}' 
  

vllm-moe-math-gpus:
  inherits: _vllm

  num_machines: 1
  tags:
    - multigpu

  plan:
    method: njobs
    n: 1

  client:
    argv:
      --dataset-name: hf
      --dataset-path: HuggingFaceH4/MATH-500
      --hf-name: HuggingFaceH4/MATH-500
      --num-prompts: 198

  server:
   argv:
      # Out of Memory on L40S
      # meta-llama/Llama-4-Maverick-17B-128E-Instruct: true
      meta-llama/Llama-4-Scout-17B-16E-Instruct: true
      --dtype: bfloat16
      --tensor-parallel-size: "{gpu_count}"


_inference:
  inherits: _defaults
  definition:  ../benchmarks/inference
  install_group: torch
  enabled: true
  tags:
    - inference

whisper-transcribe-single:
  inherits: _inference
  tags:
    - monogpu
  plan:
    method: per_gpu

  url: 
    - https://huggingface.co/openai/whisper-large-v3
    - https://huggingface.co/datasets/openslr/librispeech_asr

  argv:
    --mode: whisper
    --model: openai/whisper-large-v3
    --dataset: openslr/librispeech_asr
    --subset: "clean"
    --split: "train.100"
    --batch_size: 32


txt-to-image-gpus:
  inherits: _inference
  tags:
    - monogpu
  plan:
    method: per_gpu
  
  url: https://huggingface.co/black-forest-labs/FLUX.1-dev
  argv:
    --mode: flux
    --model: black-forest-labs/FLUX.1-dev
    --dataset: jackyhate/text-to-image-2M
    --batch_size: 1
  

llm-chat-completion:
  inherits: _inference
  tags:
    - monogpu
  plan:
    method: per_gpu

  url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
  argv:
    --mode: chat
    --model: meta-llama/Llama-3.1-8B-Instruct
    --dataset: wikitext/wikitext-103-v1
    --batch_size: 32
