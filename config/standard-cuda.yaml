defaults: &defaults
  dirs:
    code: code/{name}
    venv: venv/{name}
    data: data
    runs: runs

  venv:
    type: virtualenv

  voir:
    --forward:
      - "#stdout"
      - "#stderr"
      - "#end"
      - loss
      - compute_rate
      - train_rate
      - loading_rate
      - gpudata
      - gpu_time
      - cpu_dispatch_time
      - cpu_walltime
      - tflops
    # --loading-rate: true
    --poll-gpu: 10


torchvision: &torchvision
  <<<: *defaults

  group: torchvision
  install_group: torchvision

  dirs:
    code: code/torchvision
    venv: venv/torchvision

  plan:
    method: per_gpu

  voir:
    --stop: 120
    --metric: step
    --metric-interval: 1s
    --metric-skip: 10s

  argv:
    --with-amp: true
    --lr: 0.01

  definition: ../benchmarks/torchvision


torchbench: &torchbench
  <<<: *defaults

  group: torchbench

  dirs:
    code: code/torchbench
    venv: venv/torchbench

  plan:
    method: per_gpu

  definition: ../benchmarks/torchbench

  pip:
    constraints:
      - gym < 0.26.0

  voir:
    --stop: 120
    --metric: step
    --metric-interval: 1s
    --metric-skip: 10s

  argv:
    -d: cuda
    -m: eager
    -t: train


lightning: &lightning
  <<<: *defaults

  group: lightning
  install_group: lightning

  dirs:
    code: code/lightning
    venv: venv/lightning

  plan:
    method: njobs
    n: 1

  voir:
    --metric: wrap
    --metric-interval: 1
    --metric-skip: 1

  definition: ../benchmarks/pytorch-lightning

  argv:
    --gpus: -1
    --max_epochs: 1
    --max_steps: 100
    --limit_val_batches: 100
    --devices: -1
    --enable_progress_bar: "False"
    --accelerator: "auto"
    --enable_checkpointing: "False"


sb3: &sb3
    <<<: *defaults

    group: sb3
    install_group: sb3

    dirs:
      code: code/sb3
      venv: venv/sb3

    definition: ../benchmarks/stable_baselines3

    plan:
      method: per_gpu

    voir:
      --stop: 20
      --metric: wrap
      --metric-interval: 100
      --metric-skip: 100


benchmarks:
  resnet50:
    <<<: *torchvision

    # Automatically select batch size
    memory:
      slope: 82
      intercept: 2728
      multiple: 8

    tag:
      - vision
      - fp16

    argv:
      --model: resnet50
      --batch-size: 936
      --with-amp: True
      --fixed-batch: True

  squeezenet1_1:
    <<<: *torchvision
    
    # Automatically select batch size
    memory:
      slope: 19
      intercept: 3166
      multiple: 8

    tag:
      - vision
      - fp32
    
    # Memory cannot go beyond 22810.75 
    argv:
      --model: squeezenet1_1
      --batch-size: 4016

  efficientnet_b0:
    <<<: *torchvision
      
    # Automatically select batch size
    memory:
      slope: 47
      intercept: 2570
      multiple: 8
      
    # Memory cannot go beyond 49816.75
    argv:
      --model: efficientnet_b0
      --batch-size: 1632

  efficientnet_b4:
    <<<: *torchvision
    
    # Automatically select batch size
    memory:
      slope: 115
      intercept: 2765
      multiple: 8
      
    argv:
      --model: efficientnet_b4
      # use 79280.75
      --batch-size: 664

  efficientnet_b7:
    <<<: *torchvision
    
    # Automatically select batch size
    memory:
      slope: 264
      intercept: 3236
      multiple: 8
      
    argv:
      # use 80798.75 
      --model: efficientnet_b7
      --batch-size: 288

  convnext_large:
    <<<: *torchvision

    # Automatically select batch size
    memory:
      slope: 190
      intercept: 4090
      multiple: 8
    
    argv:
      # use 79114.75
      --model: convnext_large
      --batch-size: 392

  regnet_y_128gf:
    <<<: *torchvision
    argv:
      --model: regnet_y_128gf
      --batch-size: 32
      --with-amp: null

  bert:
    <<<: *torchbench

    # Automatically select batch size
    memory:
      slope: 162
      intercept: 4207
      multiple: 8
    
    model: BERT_pytorch
    argv:
      -d: cuda
      -m: eager
      -t: train

      # use 81042.75
      --bs: 464

  hf_reformer:
    <<<: *torchbench

    model: hf_Reformer
    argv:
      -d: cuda
      -m: eager
      -t: train
      --bs: 32

  hf_t5:
    <<<: *torchbench

    model: hf_T5
    argv:
      -d: cuda
      -m: eager
      -t: train
      --bs: 16

  dlrm:
    <<<: *torchbench

    model: dlrm
    # argv:
    #   Batch size here has no impact
    #   --bs: 4096
  
  soft_actor_critic:
    <<<: *torchbench
    model: soft_actor_critic

  speech_transformer:
    <<<: *torchbench

    model: speech_transformer
    argv:
      -d: cuda
      -m: eager
      -t: train
      #   Batch size here has no impact
      # --bs: 1

  super_slomo:
    <<<: *torchbench

    model: Super_SloMo
    argv:
      -d: cuda
      -m: eager
      -t: train
      --bs: 24

  stargan:
    <<<: *torchbench
    model: pytorch_stargan

  # cyclegan:
  #   <<<: *torchbench

  #   plan:
  #     method: njobs
  #     n: 1

  #   model: pytorch_CycleGAN_and_pix2pix

  learning_to_paint:
    <<<: *torchbench

    model: LearningToPaint

  ppo:
    <<<: *sb3

    plan:
      method: njobs
      n: 1

    argv:
      --algo: ppo
      --env: HalfCheetahBulletEnv-v0
      -n: '-1'
      --num-threads: '-1'
      --seed: '0'
      --vec-env: subproc
      --device: auto
      --: [-params, n_envs:16, n_steps:512, n_epochs:20, n_timesteps:50000]

  td3:
    <<<: *sb3

    argv:
      --algo: td3
      --env: HalfCheetahBulletEnv-v0 # Default: CartPole-v1
      --n-eval-envs: '1'
      --n-timesteps: '50000' # Default: '-1'
      --num-threads: '-1'
      --log-interval: '-1'
      --eval-episodes: '5'
      --save-freq: '-1'
      --seed: '0' # Default: -1
      --vec-env: subproc # Default: dummy
      --device: auto
      --n-trials: '10' # Default: 500
      --n-jobs: '1'

  resnet152:
    <<<: *lightning
    argv:
      --backbone: "resnet152"
      --strategy: "dp"
      --precision: 32
      # Batch size does not work
      # --batch_size: 1

  vit_l_32:
    <<<: *lightning
    argv:
      --backbone: "vit_l_32"  # (larger model)
      --strategy: "fsdp"  # Fully-Sharded Data-Parallel
      --precision: 32
      # Batch size does not work
      # --batch_size: 1
