  PYTHON: 3.9
  branch: overhead
  origin: https://github.com/mila-iqia/milabench.git
  config: /Tmp/slurm.4115007.0/milabench/config/standard.yaml
     env: ./env
    args: 
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.1.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.1.0



## Package Plan ##

  environment location: /Tmp/slurm.4115007.0/env

  added / updated specs:
    - python=3.9


The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main 
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu 
  ca-certificates    pkgs/main/linux-64::ca-certificates-2023.12.12-h06a4308_0 
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1 
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_0 
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 
  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 
  openssl            pkgs/main/linux-64::openssl-3.0.13-h7f8727e_0 
  pip                pkgs/main/linux-64::pip-23.3.1-py39h06a4308_0 
  python             pkgs/main/linux-64::python-3.9.18-h955ad1f_0 
  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 
  setuptools         pkgs/main/linux-64::setuptools-68.2.2-py39h06a4308_0 
  sqlite             pkgs/main/linux-64::sqlite-3.41.2-h5eee18b_0 
  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0 
  tzdata             pkgs/main/noarch::tzdata-2023d-h04d1e81_0 
  wheel              pkgs/main/linux-64::wheel-0.41.2-py39h06a4308_0 
  xz                 pkgs/main/linux-64::xz-5.4.5-h5eee18b_0 
  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0 



Downloading and Extracting Packages

Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
#
# To activate this environment, use
#
#     $ conda activate /Tmp/slurm.4115007.0/env
#
# To deactivate an active environment, use
#
#     $ conda deactivate

Cloning into 'milabench'...
Obtaining file:///Tmp/slurm.4115007.0/milabench
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Collecting voir@ git+https://github.com/breuleux/voir@master (from milabench==0.1.0)
  Cloning https://github.com/breuleux/voir (to revision master) to /tmp/pip-install-ffz5ww1z/voir_fe72015123d94a7cba9f364de1b17923
  Running command git clone --filter=blob:none --quiet https://github.com/breuleux/voir /tmp/pip-install-ffz5ww1z/voir_fe72015123d94a7cba9f364de1b17923
  Resolved https://github.com/breuleux/voir to commit 08b19eb9bce3a38cbf38bb59ee117c0cb6f116da
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting GitPython<4.0.0,>=3.1.24 (from milabench==0.1.0)
  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)
Collecting PyYAML<7.0,>=6.0 (from milabench==0.1.0)
  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting blessed<2.0.0,>=1.19.1 (from milabench==0.1.0)
  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.4/58.4 kB 4.6 MB/s eta 0:00:00
Collecting coleo<0.4.0,>=0.3.0 (from milabench==0.1.0)
  Downloading coleo-0.3.3-py3-none-any.whl.metadata (13 kB)
Collecting cp-template<0.4.0,>=0.3.0 (from milabench==0.1.0)
  Downloading cp_template-0.3.0-py3-none-any.whl (3.3 kB)
Collecting giving<0.5.0,>=0.4.0 (from milabench==0.1.0)
  Downloading giving-0.4.2-py3-none-any.whl (28 kB)
Collecting hrepr<0.5.0,>=0.4.0 (from milabench==0.1.0)
  Downloading hrepr-0.4.1-py3-none-any.whl (21 kB)
Collecting importlib-resources<7.0.0,>=6.1.0 (from milabench==0.1.0)
  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)
Collecting nox<2022.0.0,>=2021.10.1 (from milabench==0.1.0)
  Downloading nox-2021.10.1-py3-none-any.whl (49 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.7/49.7 kB 30.8 MB/s eta 0:00:00
Collecting numpy>=1.23.0 (from milabench==0.1.0)
  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 33.5 MB/s eta 0:00:00
Collecting omegaconf<3.0.0,>=2.3.0 (from milabench==0.1.0)
  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 44.0 MB/s eta 0:00:00
Collecting ovld<0.4.0,>=0.3.2 (from milabench==0.1.0)
  Downloading ovld-0.3.2-py3-none-any.whl (16 kB)
Collecting pandas<2.0.0,>=1.4.2 (from milabench==0.1.0)
  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 104.6 MB/s eta 0:00:00
Collecting pathspec<0.10.0,>=0.9.0 (from milabench==0.1.0)
  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)
Collecting pip-tools<7.0.0,>=6.12.3 (from milabench==0.1.0)
  Downloading pip_tools-6.14.0-py3-none-any.whl.metadata (23 kB)
Collecting psutil<6.0.0,>=5.9.5 (from milabench==0.1.0)
  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)
Collecting psycopg2-binary<3.0.0,>=2.9.6 (from milabench==0.1.0)
  Downloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
Collecting ptera<2.0.0,>=1.2.0 (from milabench==0.1.0)
  Downloading ptera-1.4.1-py3-none-any.whl (39 kB)
Collecting py-cpuinfo<10.0.0,>=9.0.0 (from milabench==0.1.0)
  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Collecting pymongo<5.0.0,>=4.3.3 (from milabench==0.1.0)
  Downloading pymongo-4.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pynvml<12.0.0,>=11.4.1 (from milabench==0.1.0)
  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 34.7 MB/s eta 0:00:00
Collecting requests<3.0.0,>=2.26.0 (from milabench==0.1.0)
  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting rich<14.0.0,>=13.3.2 (from milabench==0.1.0)
  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)
Collecting sqlalchemy<3.0.0,>=2.0.15 (from milabench==0.1.0)
  Downloading SQLAlchemy-2.0.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
Collecting tqdm<5.0.0,>=4.64.1 (from milabench==0.1.0)
  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 37.2 MB/s eta 0:00:00
Collecting wcwidth>=0.1.4 (from blessed<2.0.0,>=1.19.1->milabench==0.1.0)
  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)
Collecting six>=1.9.0 (from blessed<2.0.0,>=1.19.1->milabench==0.1.0)
  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting pystache<0.7.0,>=0.6.0 (from cp-template<0.4.0,>=0.3.0->milabench==0.1.0)
  Downloading pystache-0.6.5-py3-none-any.whl.metadata (14 kB)
Collecting gitdb<5,>=4.0.1 (from GitPython<4.0.0,>=3.1.24->milabench==0.1.0)
  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)
Collecting asttokens<3.0.0,>=2.2.1 (from giving<0.5.0,>=0.4.0->milabench==0.1.0)
  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)
Collecting reactivex<5.0.0,>=4.0.0 (from giving<0.5.0,>=0.4.0->milabench==0.1.0)
  Downloading reactivex-4.0.4-py3-none-any.whl (217 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 217.8/217.8 kB 102.4 MB/s eta 0:00:00
Collecting varname<0.11.0,>=0.10.0 (from giving<0.5.0,>=0.4.0->milabench==0.1.0)
  Downloading varname-0.10.0-py3-none-any.whl (22 kB)
Collecting zipp>=3.1.0 (from importlib-resources<7.0.0,>=6.1.0->milabench==0.1.0)
  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)
Collecting argcomplete<2.0,>=1.9.4 (from nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading argcomplete-1.12.3-py2.py3-none-any.whl (38 kB)
Collecting colorlog<7.0.0,>=2.6.1 (from nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)
Collecting packaging>=20.9 (from nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
Collecting py<2.0.0,>=1.4.0 (from nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 63.4 MB/s eta 0:00:00
Collecting virtualenv>=14.0.0 (from nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading virtualenv-20.25.0-py3-none-any.whl.metadata (4.5 kB)
Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0.0,>=2.3.0->milabench==0.1.0)
  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 68.9 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting python-dateutil>=2.8.1 (from pandas<2.0.0,>=1.4.2->milabench==0.1.0)
  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 108.5 MB/s eta 0:00:00
Collecting pytz>=2020.1 (from pandas<2.0.0,>=1.4.2->milabench==0.1.0)
  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)
Collecting build (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0)
  Downloading build-1.0.3-py3-none-any.whl.metadata (4.2 kB)
Collecting click>=8 (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0)
  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
Requirement already satisfied: pip>=22.2 in ./env/lib/python3.9/site-packages (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0) (23.3.1)
Requirement already satisfied: setuptools in ./env/lib/python3.9/site-packages (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0) (68.2.2)
Requirement already satisfied: wheel in ./env/lib/python3.9/site-packages (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0) (0.41.2)
Collecting tomli (from pip-tools<7.0.0,>=6.12.3->milabench==0.1.0)
  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)
Collecting codefind<0.2.0,>=0.1.2 (from ptera<2.0.0,>=1.2.0->milabench==0.1.0)
  Downloading codefind-0.1.3-py3-none-any.whl (3.1 kB)
Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=4.3.3->milabench==0.1.0)
  Downloading dnspython-2.5.0-py3-none-any.whl.metadata (5.8 kB)
Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.26.0->milabench==0.1.0)
  Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.26.0->milabench==0.1.0)
  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)
Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.26.0->milabench==0.1.0)
  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)
Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.26.0->milabench==0.1.0)
  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)
Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.3.2->milabench==0.1.0)
  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Collecting pygments<3.0.0,>=2.13.0 (from rich<14.0.0,>=13.3.2->milabench==0.1.0)
  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)
Collecting typing-extensions>=4.6.0 (from sqlalchemy<3.0.0,>=2.0.15->milabench==0.1.0)
  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)
Collecting greenlet!=0.4.17 (from sqlalchemy<3.0.0,>=2.0.15->milabench==0.1.0)
  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython<4.0.0,>=3.1.24->milabench==0.1.0)
  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.2->milabench==0.1.0)
  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Collecting importlib-metadata>=4.6 (from pystache<0.7.0,>=0.6.0->cp-template<0.4.0,>=0.3.0->milabench==0.1.0)
  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)
Collecting executing<2.0,>=1.1 (from varname<0.11.0,>=0.10.0->giving<0.5.0,>=0.4.0->milabench==0.1.0)
  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)
Collecting distlib<1,>=0.3.7 (from virtualenv>=14.0.0->nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)
Collecting filelock<4,>=3.12.2 (from virtualenv>=14.0.0->nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
Collecting platformdirs<5,>=3.9.1 (from virtualenv>=14.0.0->nox<2022.0.0,>=2021.10.1->milabench==0.1.0)
  Downloading platformdirs-4.2.0-py3-none-any.whl.metadata (11 kB)
Collecting pyproject_hooks (from build->pip-tools<7.0.0,>=6.12.3->milabench==0.1.0)
  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)
Downloading coleo-0.3.3-py3-none-any.whl (12 kB)
Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.4/196.4 kB 93.2 MB/s eta 0:00:00
Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)
Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 101.1 MB/s eta 0:00:00
Downloading pip_tools-6.14.0-py3-none-any.whl (55 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 kB 36.0 MB/s eta 0:00:00
Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.2/288.2 kB 116.4 MB/s eta 0:00:00
Downloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 95.9 MB/s eta 0:00:00
Downloading pymongo-4.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (676 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 676.3/676.3 kB 105.1 MB/s eta 0:00:00
Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 738.9/738.9 kB 111.7 MB/s eta 0:00:00
Downloading requests-2.31.0-py3-none-any.whl (62 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 44.3 MB/s eta 0:00:00
Downloading rich-13.7.0-py3-none-any.whl (240 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.6/240.6 kB 97.2 MB/s eta 0:00:00
Downloading SQLAlchemy-2.0.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 66.5 MB/s eta 0:00:00
Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 51.2 MB/s eta 0:00:00
Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)
Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 89.7 MB/s eta 0:00:00
Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.3/142.3 kB 82.4 MB/s eta 0:00:00
Downloading click-8.1.7-py3-none-any.whl (97 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 64.4 MB/s eta 0:00:00
Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)
Downloading dnspython-2.5.0-py3-none-any.whl (305 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 305.4/305.4 kB 120.4 MB/s eta 0:00:00
Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 44.3 MB/s eta 0:00:00
Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 614.3/614.3 kB 110.7 MB/s eta 0:00:00
Downloading idna-3.6-py3-none-any.whl (61 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 1.1 MB/s eta 0:00:00
Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 58.2 MB/s eta 0:00:00
Downloading packaging-23.2-py3-none-any.whl (53 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 37.5 MB/s eta 0:00:00
Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 95.1 MB/s eta 0:00:00
Downloading pystache-0.6.5-py3-none-any.whl (81 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 53.4 MB/s eta 0:00:00
Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 114.2 MB/s eta 0:00:00
Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)
Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.9/120.9 kB 75.0 MB/s eta 0:00:00
Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 108.6 MB/s eta 0:00:00
Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)
Downloading build-1.0.3-py3-none-any.whl (18 kB)
Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 468.9/468.9 kB 104.6 MB/s eta 0:00:00
Downloading filelock-3.13.1-py3-none-any.whl (11 kB)
Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)
Downloading platformdirs-4.2.0-py3-none-any.whl (17 kB)
Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
Building wheels for collected packages: milabench, antlr4-python3-runtime, voir
  Building editable for milabench (pyproject.toml): started
  Building editable for milabench (pyproject.toml): finished with status 'done'
  Created wheel for milabench: filename=milabench-0.1.0-cp39-cp39-manylinux_2_27_x86_64.whl size=2539 sha256=5bf02acc3cb540bac8830b107ee0a169216a6d50ef8530c6a49d67e867a405fa
  Stored in directory: /tmp/pip-ephem-wheel-cache-4p412th1/wheels/5a/1b/03/0a97f52f3b0b1608d01d1c9f7f3de6ebbbc3ed0c24f6716d7e
  Building wheel for antlr4-python3-runtime (setup.py): started
  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'
  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=8263bfa9d3498ee2680ff6958cb448dcdc015831456ce1bde5a9e56d0053ad97
  Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a
  Building wheel for voir (pyproject.toml): started
  Building wheel for voir (pyproject.toml): finished with status 'done'
  Created wheel for voir: filename=voir-0.2.12-py3-none-any.whl size=35680 sha256=a328d0b6870c176ef1902b308f705c1564ba35107cfca2b30b9449414c8b6602
  Stored in directory: /tmp/pip-ephem-wheel-cache-4p412th1/wheels/84/84/4a/c80daeea0e92bba98cd19cb2fd7d2e9cc6075cef2dedbd09df
Successfully built milabench antlr4-python3-runtime voir
Installing collected packages: wcwidth, pytz, py-cpuinfo, executing, distlib, argcomplete, antlr4-python3-runtime, zipp, varname, urllib3, typing-extensions, tqdm, tomli, smmap, six, PyYAML, pynvml, pygments, py, psycopg2-binary, psutil, platformdirs, pathspec, packaging, ovld, numpy, mdurl, idna, greenlet, filelock, dnspython, colorlog, codefind, click, charset-normalizer, certifi, virtualenv, sqlalchemy, requests, reactivex, python-dateutil, pyproject_hooks, pymongo, omegaconf, markdown-it-py, importlib-resources, importlib-metadata, hrepr, gitdb, blessed, asttokens, rich, pystache, pandas, nox, giving, GitPython, build, ptera, pip-tools, voir, coleo, cp-template, milabench
Successfully installed GitPython-3.1.41 PyYAML-6.0.1 antlr4-python3-runtime-4.9.3 argcomplete-1.12.3 asttokens-2.4.1 blessed-1.20.0 build-1.0.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 codefind-0.1.3 coleo-0.3.3 colorlog-6.8.2 cp-template-0.3.0 distlib-0.3.8 dnspython-2.5.0 executing-1.2.0 filelock-3.13.1 gitdb-4.0.11 giving-0.4.2 greenlet-3.0.3 hrepr-0.4.1 idna-3.6 importlib-metadata-7.0.1 importlib-resources-6.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 milabench-0.1.0 nox-2021.10.1 numpy-1.26.4 omegaconf-2.3.0 ovld-0.3.2 packaging-23.2 pandas-1.5.3 pathspec-0.9.0 pip-tools-6.14.0 platformdirs-4.2.0 psutil-5.9.8 psycopg2-binary-2.9.9 ptera-1.4.1 py-1.11.0 py-cpuinfo-9.0.0 pygments-2.17.2 pymongo-4.6.1 pynvml-11.5.0 pyproject_hooks-1.0.0 pystache-0.6.5 python-dateutil-2.8.2 pytz-2024.1 reactivex-4.0.4 requests-2.31.0 rich-13.7.0 six-1.16.0 smmap-5.0.1 sqlalchemy-2.0.25 tomli-2.0.1 tqdm-4.66.1 typing-extensions-4.9.0 urllib3-2.2.0 varname-0.10.0 virtualenv-20.25.0 voir-0.2.12 wcwidth-0.2.13 zipp-3.17.0

The following have been reloaded with a version change:
  1) gcc/7.4.0 => gcc/9.3.0

[=== Module cudatoolkit/11.8 loaded ===]

Install
-------
llama [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt [at 2024-02-06 11:54:50.388340]
llama [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
llama [stdout] Collecting aiohttp==3.8.6 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 9))
llama [stdout]   Downloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
llama [stdout] Collecting aiosignal==1.3.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 13))
llama [stdout]   Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
llama [stdout] Collecting antlr4-python3-runtime==4.9.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 15))
llama [stdout]   Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl
llama [stdout] Collecting asttokens==2.4.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 17))
llama [stdout]   Using cached asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)
llama [stdout] Collecting async-timeout==4.0.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 19))
llama [stdout]   Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)
llama [stdout] Collecting attrs==23.1.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 21))
llama [stdout]   Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 5.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting certifi==2023.7.22 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 23))
llama [stdout]   Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)
llama [stdout] Collecting charset-normalizer==3.3.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 25))
llama [stdout]   Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
llama [stdout] Collecting codefind==0.1.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 29))
llama [stdout]   Using cached codefind-0.1.3-py3-none-any.whl (3.1 kB)
llama [stdout] Collecting datasets==2.14.6 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 31))
llama [stdout]   Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)
llama [stdout] Collecting dill==0.3.7 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 33))
llama [stdout]   Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)
llama [stdout] Collecting executing==1.2.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 37))
llama [stdout]   Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)
llama [stdout] Collecting fairscale==0.4.13 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 39))
llama [stdout]   Downloading fairscale-0.4.13.tar.gz (266 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.3/266.3 kB 23.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout]   Installing build dependencies: started
llama [stdout]   Installing build dependencies: finished with status 'done'
llama [stdout]   Getting requirements to build wheel: started
llama [stdout]   Getting requirements to build wheel: finished with status 'done'
llama [stdout]   Installing backend dependencies: started
llama [stdout]   Installing backend dependencies: finished with status 'done'
llama [stdout]   Preparing metadata (pyproject.toml): started
llama [stdout]   Preparing metadata (pyproject.toml): finished with status 'done'
llama [stdout] Collecting filelock==3.13.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 41))
llama [stdout]   Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
llama [stdout] Collecting fire==0.5.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 47))
llama [stdout]   Downloading fire-0.5.0.tar.gz (88 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 53.5 MB/s eta 0:00:00
llama [stdout] 
llama [stdout]   Preparing metadata (setup.py): started
llama [stdout]   Preparing metadata (setup.py): finished with status 'done'
llama [stdout] Collecting frozenlist==1.4.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 49))
llama [stdout]   Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)
llama [stdout] Collecting fsspec==2023.10.0 (from fsspec[http]==2023.10.0->-r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 53))
llama [stdout]   Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)
llama [stdout] Collecting giving==0.4.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 58))
llama [stdout]   Using cached giving-0.4.2-py3-none-any.whl (28 kB)
llama [stdout] Collecting huggingface-hub==0.17.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 62))
llama [stdout]   Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)
llama [stdout] Collecting idna==3.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 67))
llama [stdout]   Downloading https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 6.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting jinja2==3.1.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 71))
llama [stdout]   Downloading https://download.pytorch.org/whl/Jinja2-3.1.2-py3-none-any.whl (133 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 11.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting markdown-it-py==3.0.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 73))
llama [stdout]   Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
llama [stdout] Collecting markupsafe==2.1.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 75))
llama [stdout]   Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)
llama [stdout] Collecting mdurl==0.1.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 77))
llama [stdout]   Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
llama [stdout] Collecting mpmath==1.3.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 79))
llama [stdout]   Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 32.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting multidict==6.0.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 81))
llama [stdout]   Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 61.9 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting multiprocess==0.70.15 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 85))
llama [stdout]   Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)
llama [stdout] Collecting networkx==3.2.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 87))
llama [stdout]   Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 92.1 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting numpy==1.26.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 89))
llama [stdout]   Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 38.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting omegaconf==2.3.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 96))
llama [stdout]   Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)
llama [stdout] Collecting ovld==0.3.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 98))
llama [stdout]   Using cached ovld-0.3.2-py3-none-any.whl (16 kB)
llama [stdout] Collecting packaging==23.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 100))
llama [stdout]   Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
llama [stdout] Collecting pandas==2.1.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 105))
llama [stdout]   Downloading pandas-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)
llama [stdout] Collecting ptera==1.4.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 107))
llama [stdout]   Using cached ptera-1.4.1-py3-none-any.whl (39 kB)
llama [stdout] Collecting pyarrow==14.0.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 109))
llama [stdout]   Downloading pyarrow-14.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
llama [stdout] Collecting pygments==2.16.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 111))
llama [stdout]   Downloading Pygments-2.16.1-py3-none-any.whl.metadata (2.5 kB)
llama [stdout] Collecting pynvml==11.5.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 113))
llama [stdout]   Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)
llama [stdout] Collecting python-dateutil==2.8.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 115))
llama [stdout]   Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
llama [stdout] Collecting pytz==2023.3.post1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 117))
llama [stdout]   Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)
llama [stdout] Collecting pyyaml==6.0.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 119))
llama [stdout]   Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
llama [stdout] Collecting reactivex==4.0.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 125))
llama [stdout]   Using cached reactivex-4.0.4-py3-none-any.whl (217 kB)
llama [stdout] Collecting regex==2023.10.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 127))
llama [stdout]   Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 24.1 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting requests==2.31.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 129))
llama [stdout]   Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
llama [stdout] Collecting rich==13.6.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 135))
llama [stdout]   Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)
llama [stdout] Collecting safetensors==0.4.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 137))
llama [stdout]   Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
llama [stdout] Collecting sentencepiece==0.1.99 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 139))
llama [stdout]   Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 68.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting six==1.16.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 141))
llama [stdout]   Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
llama [stdout] Collecting sympy==1.12 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 146))
llama [stdout]   Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 101.7 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting termcolor==2.3.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 148))
llama [stdout]   Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)
llama [stdout] Collecting tokenizers==0.14.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 150))
llama [stdout]   Downloading tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
llama [stdout] Collecting torch==2.1.0+cu118 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 152))
llama [stdout]   Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp39-cp39-linux_x86_64.whl (2325.9 MB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 GB 5.8 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting tqdm==4.66.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 156))
llama [stdout]   Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)
llama [stdout] Collecting transformers==4.35.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 161))
llama [stdout]   Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.1/123.1 kB 64.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting triton==2.1.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 163))
llama [stdout]   Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 MB 60.7 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting typing-extensions==4.8.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 165))
llama [stdout]   Downloading https://download.pytorch.org/whl/typing_extensions-4.8.0-py3-none-any.whl (31 kB)
llama [stdout] Collecting tzdata==2023.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 170))
llama [stdout]   Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 112.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Collecting urllib3==2.0.7 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 172))
llama [stdout]   Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)
llama [stdout] Collecting varname==0.10.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 174))
llama [stdout]   Using cached varname-0.10.0-py3-none-any.whl (22 kB)
llama [stdout] Collecting voir==0.2.11 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 176))
llama [stdout]   Downloading voir-0.2.11-py3-none-any.whl.metadata (791 bytes)
llama [stdout] Collecting xxhash==3.4.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 178))
llama [stdout]   Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
llama [stdout] Collecting yarl==1.9.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt (line 180))
llama [stdout]   Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)
llama [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.4/269.4 kB 61.5 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 8.7 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached asttokens-2.4.1-py2.py3-none-any.whl (27 kB)
llama [stdout] Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)
llama [stdout] Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.3/158.3 kB 81.8 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)
llama [stdout] Downloading datasets-2.14.6-py3-none-any.whl (493 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 101.8 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading dill-0.3.7-py3-none-any.whl (115 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 68.2 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached filelock-3.13.1-py3-none-any.whl (11 kB)
llama [stdout] Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 95.9 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 83.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 102.7 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
llama [stdout] Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.3/133.3 kB 5.6 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 101.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached packaging-23.2-py3-none-any.whl (53 kB)
llama [stdout] Downloading pandas-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 105.9 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading pyarrow-14.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.1 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 34.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading Pygments-2.16.1-py3-none-any.whl (1.2 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 111.1 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.5/502.5 kB 106.7 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)
llama [stdout] Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.3/773.3 kB 106.8 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached requests-2.31.0-py3-none-any.whl (62 kB)
llama [stdout] Downloading rich-13.6.0-py3-none-any.whl (239 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.8/239.8 kB 104.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 107.9 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 108.5 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)
llama [stdout] Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 84.9 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 kB 74.0 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Downloading voir-0.2.11-py3-none-any.whl (35 kB)
llama [stdout] Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)
llama [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 95.3 MB/s eta 0:00:00
llama [stdout] 
llama [stdout] Building wheels for collected packages: fairscale, fire
llama [stdout]   Building wheel for fairscale (pyproject.toml): started
llama [stdout]   Building wheel for fairscale (pyproject.toml): finished with status 'done'
llama [stdout]   Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=ffeec670dace0f0301897a77d1fab227d313a78e190b421ba7eed818716c629a
llama [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/10/ea/7f/8f35af83599829bb4790bdc16949dd99aeeb62e9a1faf47d47
llama [stdout]   Building wheel for fire (setup.py): started
llama [stdout]   Building wheel for fire (setup.py): finished with status 'done'
llama [stdout]   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=1a25765c0a4877d6fd3f7dcb85274a38c5e87f280747f9ee22b0769c17d4d0fe
llama [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6
llama [stdout] Successfully built fairscale fire
llama [stdout] Installing collected packages: sentencepiece, pytz, mpmath, executing, antlr4-python3-runtime, xxhash, varname, urllib3, tzdata, typing-extensions, tqdm, termcolor, sympy, six, safetensors, regex, pyyaml, pynvml, pygments, packaging, ovld, numpy, networkx, multidict, mdurl, markupsafe, idna, fsspec, frozenlist, filelock, dill, codefind, charset-normalizer, certifi, attrs, async-timeout, yarl, triton, requests, reactivex, python-dateutil, pyarrow, omegaconf, multiprocess, markdown-it-py, jinja2, fire, asttokens, aiosignal, torch, rich, pandas, huggingface-hub, giving, aiohttp, tokenizers, ptera, fairscale, voir, transformers, datasets
llama [stdout] Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 asttokens-2.4.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.2 codefind-0.1.3 datasets-2.14.6 dill-0.3.7 executing-1.2.0 fairscale-0.4.13 filelock-3.13.1 fire-0.5.0 frozenlist-1.4.0 fsspec-2023.10.0 giving-0.4.2 huggingface-hub-0.17.3 idna-3.4 jinja2-3.1.2 markdown-it-py-3.0.0 markupsafe-2.1.3 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.15 networkx-3.2.1 numpy-1.26.1 omegaconf-2.3.0 ovld-0.3.2 packaging-23.2 pandas-2.1.2 ptera-1.4.1 pyarrow-14.0.0 pygments-2.16.1 pynvml-11.5.0 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 reactivex-4.0.4 regex-2023.10.3 requests-2.31.0 rich-13.6.0 safetensors-0.4.0 sentencepiece-0.1.99 six-1.16.0 sympy-1.12 termcolor-2.3.0 tokenizers-0.14.1 torch-2.1.0+cu118 tqdm-4.66.1 transformers-4.35.0 triton-2.1.0 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.0.7 varname-0.10.0 voir-0.2.11 xxhash-3.4.1 yarl-1.9.2
llama [stderr] 
llama [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
llama [stderr] [notice] To update, run: pip install --upgrade pip
llama [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/llama/requirements.cuda.txt [at 2024-02-06 11:56:10.813603]
fp16 [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt [at 2024-02-06 11:56:10.818186]
fp16 [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
fp16 [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 9)) (4.9.3)
fp16 [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 13)) (2.4.1)
fp16 [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 17)) (2023.7.22)
fp16 [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 21)) (3.3.2)
fp16 [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 25)) (0.1.3)
fp16 [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 29)) (1.2.0)
fp16 [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 33)) (3.13.1)
fp16 [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 38)) (2023.10.0)
fp16 [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 42)) (0.4.2)
fp16 [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 47)) (3.4)
fp16 [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 51)) (3.1.2)
fp16 [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 55)) (3.0.0)
fp16 [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 59)) (2.1.3)
fp16 [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 63)) (0.1.2)
fp16 [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 67)) (1.3.0)
fp16 [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 71)) (3.2.1)
fp16 [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 75)) (1.26.1)
fp16 [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 79)) (2.3.0)
fp16 [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 83)) (0.3.2)
fp16 [stdout] Collecting pillow==10.1.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 87))
fp16 [stdout]   Downloading Pillow-10.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)
fp16 [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 91)) (1.4.1)
fp16 [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 95)) (2.16.1)
fp16 [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 99)) (11.5.0)
fp16 [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 103)) (6.0.1)
fp16 [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 107)) (4.0.4)
fp16 [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 111)) (2.31.0)
fp16 [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 115)) (13.6.0)
fp16 [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 119)) (1.16.0)
fp16 [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 123)) (1.12)
fp16 [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 127)) (2.1.0+cu118)
fp16 [stdout] Collecting torchvision==0.16.0+cu118 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 131))
fp16 [stdout]   Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp39-cp39-linux_x86_64.whl (6.2 MB)
fp16 [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 35.0 MB/s eta 0:00:00
fp16 [stdout] 
fp16 [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 133)) (4.66.1)
fp16 [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 135)) (2.1.0)
fp16 [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 139)) (4.8.0)
fp16 [stdout] Collecting urllib3==1.26.18 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 144))
fp16 [stdout]   Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)
fp16 [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 4.6 MB/s eta 0:00:00
fp16 [stdout] 
fp16 [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 148)) (0.10.0)
fp16 [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt (line 152)) (0.2.11)
fp16 [stdout] Downloading Pillow-10.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)
fp16 [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 36.7 MB/s eta 0:00:00
fp16 [stdout] 
fp16 [stdout] Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)
fp16 [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 84.6 MB/s eta 0:00:00
fp16 [stdout] 
fp16 [stdout] Installing collected packages: urllib3, pillow, torchvision
fp16 [stdout]   Attempting uninstall: urllib3
fp16 [stdout]     Found existing installation: urllib3 2.0.7
fp16 [stdout]     Uninstalling urllib3-2.0.7:
fp16 [stdout]       Successfully uninstalled urllib3-2.0.7
fp16 [stdout] Successfully installed pillow-10.1.0 torchvision-0.16.0+cu118 urllib3-1.26.18
fp16 [stderr] 
fp16 [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
fp16 [stderr] [notice] To update, run: pip install --upgrade pip
fp16 [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/flops/requirements.cuda.txt [at 2024-02-06 11:56:13.787749]
bf16 [message] Benchmark bf16 is already installed
tf32 [message] Benchmark tf32 is already installed
fp32 [message] Benchmark fp32 is already installed
resnet50 [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt [at 2024-02-06 11:56:13.792377]
resnet50 [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
resnet50 [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 9)) (4.9.3)
resnet50 [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 13)) (2.4.1)
resnet50 [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 17)) (2023.7.22)
resnet50 [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 21)) (3.3.2)
resnet50 [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 25)) (0.1.3)
resnet50 [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 29)) (1.2.0)
resnet50 [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 33)) (3.13.1)
resnet50 [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 38)) (2023.10.0)
resnet50 [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 42)) (0.4.2)
resnet50 [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 47)) (3.4)
resnet50 [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 51)) (3.1.2)
resnet50 [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 55)) (3.0.0)
resnet50 [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 59)) (2.1.3)
resnet50 [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 63)) (0.1.2)
resnet50 [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 67)) (1.3.0)
resnet50 [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 71)) (3.2.1)
resnet50 [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 75)) (1.26.1)
resnet50 [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 79)) (2.3.0)
resnet50 [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 83)) (0.3.2)
resnet50 [stdout] Requirement already satisfied: pillow==10.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 87)) (10.1.0)
resnet50 [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 91)) (1.4.1)
resnet50 [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 95)) (2.16.1)
resnet50 [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 99)) (11.5.0)
resnet50 [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 103)) (6.0.1)
resnet50 [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 107)) (4.0.4)
resnet50 [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 111)) (2.31.0)
resnet50 [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 115)) (13.6.0)
resnet50 [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 119)) (1.16.0)
resnet50 [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 123)) (1.12)
resnet50 [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 127)) (2.1.0+cu118)
resnet50 [stdout] Requirement already satisfied: torchvision==0.16.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 131)) (0.16.0+cu118)
resnet50 [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 133)) (4.66.1)
resnet50 [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 135)) (2.1.0)
resnet50 [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 139)) (4.8.0)
resnet50 [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 144)) (1.26.18)
resnet50 [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 148)) (0.10.0)
resnet50 [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt (line 152)) (0.2.11)
resnet50 [stderr] 
resnet50 [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
resnet50 [stderr] [notice] To update, run: pip install --upgrade pip
resnet50 [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/requirements.cuda.txt [at 2024-02-06 11:56:14.964456]
convnext_large-fp32 [message] Benchmark convnext_large-fp32 is already installed
convnext_large-fp16 [message] Benchmark convnext_large-fp16 is already installed
convnext_large-tf32 [message] Benchmark convnext_large-tf32 is already installed
convnext_large-tf32-fp16 [message] Benchmark convnext_large-tf32-fp16 is already installed
regnet_y_128gf [message] Benchmark regnet_y_128gf is already installed
bert-fp32 [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt [at 2024-02-06 11:56:14.969425]
bert-fp32 [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
bert-fp32 [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 9)) (4.9.3)
bert-fp32 [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 13)) (2.4.1)
bert-fp32 [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 17)) (2023.7.22)
bert-fp32 [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 21)) (3.3.2)
bert-fp32 [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 25)) (0.1.3)
bert-fp32 [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 29)) (1.2.0)
bert-fp32 [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 33)) (3.13.1)
bert-fp32 [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 40)) (2023.10.0)
bert-fp32 [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 45)) (0.4.2)
bert-fp32 [stdout] Requirement already satisfied: huggingface-hub==0.17.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 50)) (0.17.3)
bert-fp32 [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 55)) (3.4)
bert-fp32 [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 59)) (3.1.2)
bert-fp32 [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 63)) (3.0.0)
bert-fp32 [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 67)) (2.1.3)
bert-fp32 [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 71)) (0.1.2)
bert-fp32 [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 75)) (1.3.0)
bert-fp32 [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 79)) (3.2.1)
bert-fp32 [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 83)) (1.26.1)
bert-fp32 [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 87)) (2.3.0)
bert-fp32 [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 91)) (0.3.2)
bert-fp32 [stdout] Requirement already satisfied: packaging==23.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 95)) (23.2)
bert-fp32 [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 100)) (1.4.1)
bert-fp32 [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 104)) (2.16.1)
bert-fp32 [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 108)) (11.5.0)
bert-fp32 [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 112)) (6.0.1)
bert-fp32 [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 118)) (4.0.4)
bert-fp32 [stdout] Requirement already satisfied: regex==2023.10.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 122)) (2023.10.3)
bert-fp32 [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 126)) (2.31.0)
bert-fp32 [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 131)) (13.6.0)
bert-fp32 [stdout] Requirement already satisfied: safetensors==0.4.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 135)) (0.4.0)
bert-fp32 [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 139)) (1.16.0)
bert-fp32 [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 143)) (1.12)
bert-fp32 [stdout] Requirement already satisfied: tokenizers==0.14.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 147)) (0.14.1)
bert-fp32 [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 151)) (2.1.0+cu118)
bert-fp32 [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 153)) (4.66.1)
bert-fp32 [stdout] Requirement already satisfied: transformers==4.35.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 158)) (4.35.0)
bert-fp32 [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 160)) (2.1.0)
bert-fp32 [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 164)) (4.8.0)
bert-fp32 [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 170)) (1.26.18)
bert-fp32 [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 174)) (0.10.0)
bert-fp32 [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt (line 178)) (0.2.11)
bert-fp32 [stderr] 
bert-fp32 [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
bert-fp32 [stderr] [notice] To update, run: pip install --upgrade pip
bert-fp32 [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/requirements.cuda.txt [at 2024-02-06 11:56:16.159376]
bert-fp16 [message] Benchmark bert-fp16 is already installed
bert-tf32 [message] Benchmark bert-tf32 is already installed
bert-tf32-fp16 [message] Benchmark bert-tf32-fp16 is already installed
t5 [message] Benchmark t5 is already installed
reformer [message] Benchmark reformer is already installed
whisper [message] Benchmark whisper is already installed
resnet152 [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt [at 2024-02-06 11:56:16.165208]
resnet152 [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
resnet152 [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 9)) (4.9.3)
resnet152 [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 13)) (2.4.1)
resnet152 [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 17)) (2023.7.22)
resnet152 [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 21)) (3.3.2)
resnet152 [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 25)) (0.1.3)
resnet152 [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 29)) (1.2.0)
resnet152 [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 33)) (3.13.1)
resnet152 [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 39)) (2023.10.0)
resnet152 [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 44)) (0.4.2)
resnet152 [stdout] Requirement already satisfied: huggingface-hub==0.17.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 49)) (0.17.3)
resnet152 [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 51)) (3.4)
resnet152 [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 55)) (3.1.2)
resnet152 [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 59)) (3.0.0)
resnet152 [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 63)) (2.1.3)
resnet152 [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 67)) (0.1.2)
resnet152 [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 71)) (1.3.0)
resnet152 [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 75)) (3.2.1)
resnet152 [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 79)) (1.26.1)
resnet152 [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 83)) (2.3.0)
resnet152 [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 87)) (0.3.2)
resnet152 [stdout] Requirement already satisfied: packaging==23.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 91)) (23.2)
resnet152 [stdout] Requirement already satisfied: pillow==10.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 95)) (10.1.0)
resnet152 [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 99)) (1.4.1)
resnet152 [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 103)) (2.16.1)
resnet152 [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 107)) (11.5.0)
resnet152 [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 111)) (6.0.1)
resnet152 [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 116)) (4.0.4)
resnet152 [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 120)) (2.31.0)
resnet152 [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 125)) (13.6.0)
resnet152 [stdout] Requirement already satisfied: safetensors==0.4.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 129)) (0.4.0)
resnet152 [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 131)) (1.16.0)
resnet152 [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 135)) (1.12)
resnet152 [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 139)) (2.1.0+cu118)
resnet152 [stdout] Requirement already satisfied: torchvision==0.16.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 143)) (0.16.0+cu118)
resnet152 [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 145)) (4.66.1)
resnet152 [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 149)) (2.1.0)
resnet152 [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 153)) (4.8.0)
resnet152 [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 159)) (1.26.18)
resnet152 [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 163)) (0.10.0)
resnet152 [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt (line 167)) (0.2.11)
resnet152 [stderr] 
resnet152 [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
resnet152 [stderr] [notice] To update, run: pip install --upgrade pip
resnet152 [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/timm/requirements.cuda.txt [at 2024-02-06 11:56:17.355173]
resnet152-multi [message] Benchmark resnet152-multi is already installed
davit_large [message] Benchmark davit_large is already installed
davit_large-multi [message] Benchmark davit_large-multi is already installed
focalnet [message] Benchmark focalnet is already installed
opt-1_3b [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt [at 2024-02-06 11:56:18.043974]
opt-1_3b [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
opt-1_3b [stdout] Collecting accelerate==0.24.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 9))
opt-1_3b [stdout]   Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)
opt-1_3b [stdout] Requirement already satisfied: aiohttp==3.8.6 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 11)) (3.8.6)
opt-1_3b [stdout] Requirement already satisfied: aiosignal==1.3.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 16)) (1.3.1)
opt-1_3b [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 20)) (4.9.3)
opt-1_3b [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 24)) (2.4.1)
opt-1_3b [stdout] Requirement already satisfied: async-timeout==4.0.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 28)) (4.0.3)
opt-1_3b [stdout] Requirement already satisfied: attrs==23.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 32)) (23.1.0)
opt-1_3b [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 36)) (2023.7.22)
opt-1_3b [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 40)) (3.3.2)
opt-1_3b [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 45)) (0.1.3)
opt-1_3b [stdout] Requirement already satisfied: datasets==2.14.6 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 49)) (2.14.6)
opt-1_3b [stdout] Collecting deepspeed==0.12.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 53))
opt-1_3b [stdout]   Downloading deepspeed-0.12.2.tar.gz (1.2 MB)
opt-1_3b [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 30.3 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout]   Preparing metadata (setup.py): started
opt-1_3b [stdout]   Preparing metadata (setup.py): finished with status 'done'
opt-1_3b [stdout] Requirement already satisfied: dill==0.3.7 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 55)) (0.3.7)
opt-1_3b [stdout] Collecting evaluate==0.4.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 61))
opt-1_3b [stdout]   Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)
opt-1_3b [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 63)) (1.2.0)
opt-1_3b [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 67)) (3.13.1)
opt-1_3b [stdout] Requirement already satisfied: frozenlist==1.4.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 74)) (1.4.0)
opt-1_3b [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from fsspec[http]==2023.10.0->-r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 79)) (2023.10.0)
opt-1_3b [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 86)) (0.4.2)
opt-1_3b [stdout] Collecting hjson==3.1.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 91))
opt-1_3b [stdout]   Downloading hjson-3.1.0-py3-none-any.whl (54 kB)
opt-1_3b [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 34.9 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Requirement already satisfied: huggingface-hub==0.17.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 95)) (0.17.3)
opt-1_3b [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 103)) (3.4)
opt-1_3b [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 108)) (3.1.2)
opt-1_3b [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 112)) (3.0.0)
opt-1_3b [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 116)) (2.1.3)
opt-1_3b [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 120)) (0.1.2)
opt-1_3b [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 124)) (1.3.0)
opt-1_3b [stdout] Requirement already satisfied: multidict==6.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 128)) (6.0.4)
opt-1_3b [stdout] Requirement already satisfied: multiprocess==0.70.15 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 133)) (0.70.15)
opt-1_3b [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 138)) (3.2.1)
opt-1_3b [stdout] Collecting ninja==1.11.1.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 142))
opt-1_3b [stdout]   Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)
opt-1_3b [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 146)) (1.26.1)
opt-1_3b [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 157)) (2.3.0)
opt-1_3b [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 161)) (0.3.2)
opt-1_3b [stdout] Requirement already satisfied: packaging==23.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 165)) (23.2)
opt-1_3b [stdout] Requirement already satisfied: pandas==2.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 174)) (2.1.2)
opt-1_3b [stdout] Requirement already satisfied: pillow==10.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 179)) (10.1.0)
opt-1_3b [stdout] Collecting psutil==5.9.6 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 183))
opt-1_3b [stdout]   Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)
opt-1_3b [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 188)) (1.4.1)
opt-1_3b [stdout] Collecting py-cpuinfo==9.0.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 192))
opt-1_3b [stdout]   Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
opt-1_3b [stdout] Requirement already satisfied: pyarrow==14.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 196)) (14.0.0)
opt-1_3b [stdout] Collecting pydantic==1.10.13 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 200))
opt-1_3b [stdout]   Downloading pydantic-1.10.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)
opt-1_3b [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 79.8 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 204)) (2.16.1)
opt-1_3b [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 208)) (11.5.0)
opt-1_3b [stdout] Requirement already satisfied: python-dateutil==2.8.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 213)) (2.8.2)
opt-1_3b [stdout] Requirement already satisfied: pytz==2023.3.post1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 217)) (2023.3.post1)
opt-1_3b [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 221)) (6.0.1)
opt-1_3b [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 229)) (4.0.4)
opt-1_3b [stdout] Requirement already satisfied: regex==2023.10.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 233)) (2023.10.3)
opt-1_3b [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 237)) (2.31.0)
opt-1_3b [stdout] Collecting responses==0.18.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 247))
opt-1_3b [stdout]   Downloading responses-0.18.0-py3-none-any.whl (38 kB)
opt-1_3b [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 251)) (13.6.0)
opt-1_3b [stdout] Requirement already satisfied: safetensors==0.4.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 255)) (0.4.0)
opt-1_3b [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 259)) (1.16.0)
opt-1_3b [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 264)) (1.12)
opt-1_3b [stdout] Requirement already satisfied: tokenizers==0.14.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 268)) (0.14.1)
opt-1_3b [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 272)) (2.1.0+cu118)
opt-1_3b [stdout] Collecting torchaudio==2.1.0+cu118 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 279))
opt-1_3b [stdout]   Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp39-cp39-linux_x86_64.whl (3.2 MB)
opt-1_3b [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 46.8 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Requirement already satisfied: torchvision==0.16.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 281)) (0.16.0+cu118)
opt-1_3b [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 283)) (4.66.1)
opt-1_3b [stdout] Requirement already satisfied: transformers==4.35.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 291)) (4.35.0)
opt-1_3b [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 293)) (2.1.0)
opt-1_3b [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 297)) (4.8.0)
opt-1_3b [stdout] Requirement already satisfied: tzdata==2023.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 304)) (2023.3)
opt-1_3b [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 308)) (1.26.18)
opt-1_3b [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 313)) (0.10.0)
opt-1_3b [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 317)) (0.2.11)
opt-1_3b [stdout] Requirement already satisfied: xxhash==3.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 319)) (3.4.1)
opt-1_3b [stdout] Requirement already satisfied: yarl==1.9.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt (line 324)) (1.9.2)
opt-1_3b [stdout] Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)
opt-1_3b [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.4/261.4 kB 98.7 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)
opt-1_3b [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 51.0 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)
opt-1_3b [stdout] Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)
opt-1_3b [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.6/283.6 kB 105.5 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Downloading pydantic-1.10.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)
opt-1_3b [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 83.9 MB/s eta 0:00:00
opt-1_3b [stdout] 
opt-1_3b [stdout] Building wheels for collected packages: deepspeed
opt-1_3b [stdout]   Building wheel for deepspeed (setup.py): started
opt-1_3b [stdout]   Building wheel for deepspeed (setup.py): finished with status 'done'
opt-1_3b [stdout]   Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265673 sha256=1c681f2c37d490f8903f054b1ed2fb2869c649db519438b66fad9653ee98f345
opt-1_3b [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/e1/19/3c/919d17396974990105fee67fb8161f89374c2bafde85e3113c
opt-1_3b [stdout] Successfully built deepspeed
opt-1_3b [stdout] Installing collected packages: py-cpuinfo, ninja, hjson, pydantic, psutil, responses, torchaudio, deepspeed, accelerate, evaluate
opt-1_3b [stdout] Successfully installed accelerate-0.24.1 deepspeed-0.12.2 evaluate-0.4.1 hjson-3.1.0 ninja-1.11.1.1 psutil-5.9.6 py-cpuinfo-9.0.0 pydantic-1.10.13 responses-0.18.0 torchaudio-2.1.0+cu118
opt-1_3b [stderr] 
opt-1_3b [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
opt-1_3b [stderr] [notice] To update, run: pip install --upgrade pip
opt-1_3b [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/requirements.cuda.txt [at 2024-02-06 11:56:28.359262]
opt-1_3b-multinode [message] Benchmark opt-1_3b-multinode is already installed
opt-6_7b [message] Benchmark opt-6_7b is already installed
opt-6_7b-multinode [message] Benchmark opt-6_7b-multinode is already installed
stargan [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt [at 2024-02-06 11:56:28.364804]
stargan [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
stargan [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 9)) (4.9.3)
stargan [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 13)) (2.4.1)
stargan [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 17)) (2023.7.22)
stargan [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 21)) (3.3.2)
stargan [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 25)) (0.1.3)
stargan [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 29)) (1.2.0)
stargan [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 33)) (3.13.1)
stargan [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 38)) (2023.10.0)
stargan [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 42)) (0.4.2)
stargan [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 47)) (3.4)
stargan [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 51)) (3.1.2)
stargan [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 55)) (3.0.0)
stargan [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 59)) (2.1.3)
stargan [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 63)) (0.1.2)
stargan [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 67)) (1.3.0)
stargan [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 71)) (3.2.1)
stargan [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 75)) (1.26.1)
stargan [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 79)) (2.3.0)
stargan [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 83)) (0.3.2)
stargan [stdout] Requirement already satisfied: pillow==10.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 87)) (10.1.0)
stargan [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 91)) (1.4.1)
stargan [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 95)) (2.16.1)
stargan [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 99)) (11.5.0)
stargan [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 103)) (6.0.1)
stargan [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 107)) (4.0.4)
stargan [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 111)) (2.31.0)
stargan [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 115)) (13.6.0)
stargan [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 119)) (1.16.0)
stargan [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 123)) (1.12)
stargan [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 127)) (2.1.0+cu118)
stargan [stdout] Requirement already satisfied: torchvision==0.16.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 131)) (0.16.0+cu118)
stargan [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 133)) (2.1.0)
stargan [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 137)) (4.8.0)
stargan [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 142)) (1.26.18)
stargan [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 146)) (0.10.0)
stargan [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt (line 150)) (0.2.11)
stargan [stderr] 
stargan [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
stargan [stderr] [notice] To update, run: pip install --upgrade pip
stargan [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/stargan/requirements.cuda.txt [at 2024-02-06 11:56:29.676399]
super-slomo [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt [at 2024-02-06 11:56:29.681273]
super-slomo [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
super-slomo [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 9)) (4.9.3)
super-slomo [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 13)) (2.4.1)
super-slomo [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 17)) (2023.7.22)
super-slomo [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 21)) (3.3.2)
super-slomo [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 25)) (0.1.3)
super-slomo [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 29)) (1.2.0)
super-slomo [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 33)) (3.13.1)
super-slomo [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 38)) (2023.10.0)
super-slomo [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 42)) (0.4.2)
super-slomo [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 47)) (3.4)
super-slomo [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 51)) (3.1.2)
super-slomo [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 55)) (3.0.0)
super-slomo [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 59)) (2.1.3)
super-slomo [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 63)) (0.1.2)
super-slomo [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 67)) (1.3.0)
super-slomo [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 71)) (3.2.1)
super-slomo [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 75)) (1.26.1)
super-slomo [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 80)) (2.3.0)
super-slomo [stdout] Collecting opencv-python==4.8.1.78 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 84))
super-slomo [stdout]   Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)
super-slomo [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 86)) (0.3.2)
super-slomo [stdout] Requirement already satisfied: pillow==10.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 90)) (10.1.0)
super-slomo [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 94)) (1.4.1)
super-slomo [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 98)) (2.16.1)
super-slomo [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 102)) (11.5.0)
super-slomo [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 106)) (6.0.1)
super-slomo [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 110)) (4.0.4)
super-slomo [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 114)) (2.31.0)
super-slomo [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 118)) (13.6.0)
super-slomo [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 122)) (1.16.0)
super-slomo [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 126)) (1.12)
super-slomo [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 130)) (2.1.0+cu118)
super-slomo [stdout] Requirement already satisfied: torchvision==0.16.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 134)) (0.16.0+cu118)
super-slomo [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 136)) (4.66.1)
super-slomo [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 138)) (2.1.0)
super-slomo [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 142)) (4.8.0)
super-slomo [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 147)) (1.26.18)
super-slomo [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 151)) (0.10.0)
super-slomo [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt (line 155)) (0.2.11)
super-slomo [stdout] Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)
super-slomo [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.7/61.7 MB 75.6 MB/s eta 0:00:00
super-slomo [stdout] 
super-slomo [stdout] Installing collected packages: opencv-python
super-slomo [stdout] Successfully installed opencv-python-4.8.1.78
super-slomo [stderr] 
super-slomo [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
super-slomo [stderr] [notice] To update, run: pip install --upgrade pip
super-slomo [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/requirements.cuda.txt [at 2024-02-06 11:56:32.726715]
dlrm [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt [at 2024-02-06 11:56:33.186199]
dlrm [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
dlrm [stdout] Collecting absl-py==2.0.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 9))
dlrm [stdout]   Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)
dlrm [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 13)) (4.9.3)
dlrm [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 17)) (2.4.1)
dlrm [stdout] Collecting cachetools==5.3.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 21))
dlrm [stdout]   Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)
dlrm [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 25)) (2023.7.22)
dlrm [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 29)) (3.3.2)
dlrm [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 33)) (0.1.3)
dlrm [stdout] Collecting docker==6.1.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 37))
dlrm [stdout]   Downloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)
dlrm [stdout] Collecting docstring-parser==0.8.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 41))
dlrm [stdout]   Downloading docstring_parser-0.8.1.tar.gz (14 kB)
dlrm [stdout]   Installing build dependencies: started
dlrm [stdout]   Installing build dependencies: finished with status 'done'
dlrm [stdout]   Getting requirements to build wheel: started
dlrm [stdout]   Getting requirements to build wheel: finished with status 'done'
dlrm [stdout]   Preparing metadata (pyproject.toml): started
dlrm [stdout]   Preparing metadata (pyproject.toml): finished with status 'done'
dlrm [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 45)) (1.2.0)
dlrm [stdout] Collecting fbgemm-gpu==0.5.0+cu118 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 49))
dlrm [stdout]   Downloading https://download.pytorch.org/whl/cu118/fbgemm_gpu-0.5.0%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl (227.0 MB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.0/227.0 MB 35.2 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 53)) (3.13.1)
dlrm [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 59)) (2023.10.0)
dlrm [stdout] Collecting future==0.18.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 64))
dlrm [stdout]   Downloading future-0.18.3.tar.gz (840 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 20.0 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout]   Preparing metadata (setup.py): started
dlrm [stdout]   Preparing metadata (setup.py): finished with status 'done'
dlrm [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 66)) (0.4.2)
dlrm [stdout] Collecting google-auth==2.23.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 71))
dlrm [stdout]   Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)
dlrm [stdout] Collecting google-auth-oauthlib==1.1.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 76))
dlrm [stdout]   Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)
dlrm [stdout] Collecting graphviz==0.20.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 80))
dlrm [stdout]   Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 30.4 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Collecting grpcio==1.59.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 84))
dlrm [stdout]   Downloading grpcio-1.59.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
dlrm [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 88)) (3.4)
dlrm [stdout] Collecting importlib-metadata==6.8.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 92))
dlrm [stdout]   Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)
dlrm [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 96)) (3.1.2)
dlrm [stdout] Collecting joblib==1.3.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 100))
dlrm [stdout]   Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)
dlrm [stdout] Collecting lightning-utilities==0.9.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 104))
dlrm [stdout]   Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)
dlrm [stdout] Collecting markdown==3.5.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 108))
dlrm [stdout]   Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)
dlrm [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 112)) (3.0.0)
dlrm [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 116)) (2.1.3)
dlrm [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 121)) (0.1.2)
dlrm [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 125)) (1.3.0)
dlrm [stdout] Collecting mypy-extensions==1.0.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 129))
dlrm [stdout]   Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
dlrm [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 133)) (3.2.1)
dlrm [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 137)) (1.26.1)
dlrm [stdout] Collecting oauthlib==3.2.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 145))
dlrm [stdout]   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 79.1 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 149)) (2.3.0)
dlrm [stdout] Collecting onnx==1.15.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 153))
dlrm [stdout]   Downloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)
dlrm [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 155)) (0.3.2)
dlrm [stdout] Requirement already satisfied: packaging==23.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 159)) (23.2)
dlrm [stdout] Collecting protobuf==4.23.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 165))
dlrm [stdout]   Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)
dlrm [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 170)) (1.4.1)
dlrm [stdout] Collecting pyasn1==0.5.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 174))
dlrm [stdout]   Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.9/83.9 kB 50.4 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Collecting pyasn1-modules==0.3.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 179))
dlrm [stdout]   Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 90.3 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Collecting pydot==1.4.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 183))
dlrm [stdout]   Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)
dlrm [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 185)) (2.16.1)
dlrm [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 189)) (11.5.0)
dlrm [stdout] Collecting pyparsing==3.1.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 193))
dlrm [stdout]   Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)
dlrm [stdout] Collecting pyre-extensions==0.0.30 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 197))
dlrm [stdout]   Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)
dlrm [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 201)) (6.0.1)
dlrm [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 206)) (4.0.4)
dlrm [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 210)) (2.31.0)
dlrm [stdout] Collecting requests-oauthlib==1.3.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 216))
dlrm [stdout]   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
dlrm [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 220)) (13.6.0)
dlrm [stdout] Collecting rsa==4.9 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 224))
dlrm [stdout]   Downloading rsa-4.9-py3-none-any.whl (34 kB)
dlrm [stdout] Collecting scikit-learn==1.3.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 228))
dlrm [stdout]   Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)
dlrm [stdout] Collecting scipy==1.11.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 230))
dlrm [stdout]   Downloading scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.4/60.4 kB 38.6 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 234)) (1.16.0)
dlrm [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 239)) (1.12)
dlrm [stdout] Collecting tabulate==0.9.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 243))
dlrm [stdout]   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
dlrm [stdout] Collecting tensorboard==2.15.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 247))
dlrm [stdout]   Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)
dlrm [stdout] Collecting tensorboard-data-server==0.7.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 249))
dlrm [stdout]   Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)
dlrm [stdout] Collecting threadpoolctl==3.2.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 253))
dlrm [stdout]   Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)
dlrm [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 257)) (2.1.0+cu118)
dlrm [stdout] Collecting torchmetrics==1.0.3 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 262))
dlrm [stdout]   Downloading https://download.pytorch.org/whl/torchmetrics-1.0.3-py3-none-any.whl (731 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.6/731.6 kB 83.7 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Collecting torchrec==0.5.0+cu118 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 266))
dlrm [stdout]   Downloading https://download.pytorch.org/whl/cu118/torchrec-0.5.0%2Bcu118-py3-none-any.whl (393 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 393.8/393.8 kB 75.2 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Collecting torchviz==0.0.2 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 268))
dlrm [stdout]   Downloading torchviz-0.0.2.tar.gz (4.9 kB)
dlrm [stdout]   Preparing metadata (setup.py): started
dlrm [stdout]   Preparing metadata (setup.py): finished with status 'done'
dlrm [stdout] Collecting torchx==0.5.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 270))
dlrm [stdout]   Downloading torchx-0.5.0-py3-none-any.whl (251 kB)
dlrm [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.2/251.2 kB 108.6 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 272)) (4.66.1)
dlrm [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 276)) (2.1.0)
dlrm [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 280)) (4.8.0)
dlrm [stdout] Collecting typing-inspect==0.9.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 288))
dlrm [stdout]   Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
dlrm [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 292)) (1.26.18)
dlrm [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 298)) (0.10.0)
dlrm [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 302)) (0.2.11)
dlrm [stdout] Collecting websocket-client==1.6.4 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 304))
dlrm [stdout]   Downloading websocket_client-1.6.4-py3-none-any.whl.metadata (7.7 kB)
dlrm [stdout] Collecting werkzeug==3.0.1 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 308))
dlrm [stdout]   Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)
dlrm [stdout] Collecting zipp==3.17.0 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 312))
dlrm [stdout]   Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)
dlrm [stdout] Requirement already satisfied: setuptools>=41.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from tensorboard==2.15.1->-r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt (line 247)) (69.0.3)
dlrm [stdout] Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 67.1 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)
dlrm [stdout] Downloading docker-6.1.3-py3-none-any.whl (148 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 74.8 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.3/183.3 kB 90.8 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)
dlrm [stdout] Downloading grpcio-1.59.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 93.5 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)
dlrm [stdout] Downloading joblib-1.3.2-py3-none-any.whl (302 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.2/302.2 kB 119.8 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)
dlrm [stdout] Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 kB 63.2 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 103.3 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.5/304.5 kB 120.2 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.1/103.1 kB 63.8 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 95.5 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.6/36.6 MB 88.8 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 100.2 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)
dlrm [stdout] Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)
dlrm [stdout] Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
dlrm [stdout] Downloading websocket_client-1.6.4-py3-none-any.whl (57 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.3/57.3 kB 691.7 kB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)
dlrm [stdout]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 83.3 MB/s eta 0:00:00
dlrm [stdout] 
dlrm [stdout] Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)
dlrm [stdout] Building wheels for collected packages: docstring-parser, future, torchviz
dlrm [stdout]   Building wheel for docstring-parser (pyproject.toml): started
dlrm [stdout]   Building wheel for docstring-parser (pyproject.toml): finished with status 'done'
dlrm [stdout]   Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19661 sha256=09092d818ebc394de99a17e7c0618091ec67ba4e3ba16fc3f224ad6e69c3104d
dlrm [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/35/b6/65/eda0a6497d7e3275201108c17e12c945989eb0d6e9dcc8eca2
dlrm [stdout]   Building wheel for future (setup.py): started
dlrm [stdout]   Building wheel for future (setup.py): finished with status 'done'
dlrm [stdout]   Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=4335641b7b0ac92afc98fc3a977cbf4e5777ec560b8b0f08308b0bce4084e312
dlrm [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b
dlrm [stdout]   Building wheel for torchviz (setup.py): started
dlrm [stdout]   Building wheel for torchviz (setup.py): finished with status 'done'
dlrm [stdout]   Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=51af479bce7d9b5e2a7cb378a7af768e46d79d8feaf6390216d5fafc4a9e7a6e
dlrm [stdout]   Stored in directory: /Tmp/slurm.4115007.0/base/cache/pip/wheels/29/65/6e/db2515eb1dc760fecd36b40d54df65c1e18534013f1c037e2e
dlrm [stdout] Successfully built docstring-parser future torchviz
dlrm [stdout] Installing collected packages: fbgemm-gpu, zipp, werkzeug, websocket-client, threadpoolctl, tensorboard-data-server, tabulate, scipy, pyparsing, pyasn1, protobuf, oauthlib, mypy-extensions, lightning-utilities, joblib, grpcio, graphviz, future, docstring-parser, cachetools, absl-py, typing-inspect, scikit-learn, rsa, requests-oauthlib, pydot, pyasn1-modules, onnx, importlib-metadata, docker, torchviz, torchmetrics, pyre-extensions, markdown, google-auth, torchx, torchrec, google-auth-oauthlib, tensorboard
dlrm [stdout] Successfully installed absl-py-2.0.0 cachetools-5.3.2 docker-6.1.3 docstring-parser-0.8.1 fbgemm-gpu-0.5.0+cu118 future-0.18.3 google-auth-2.23.4 google-auth-oauthlib-1.1.0 graphviz-0.20.1 grpcio-1.59.2 importlib-metadata-6.8.0 joblib-1.3.2 lightning-utilities-0.9.0 markdown-3.5.1 mypy-extensions-1.0.0 oauthlib-3.2.2 onnx-1.15.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydot-1.4.2 pyparsing-3.1.1 pyre-extensions-0.0.30 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.3.2 scipy-1.11.3 tabulate-0.9.0 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.2.0 torchmetrics-1.0.3 torchrec-0.5.0+cu118 torchviz-0.0.2 torchx-0.5.0 typing-inspect-0.9.0 websocket-client-1.6.4 werkzeug-3.0.1 zipp-3.17.0
dlrm [stderr] 
dlrm [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
dlrm [stderr] [notice] To update, run: pip install --upgrade pip
dlrm [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm/requirements.cuda.txt [at 2024-02-06 11:56:57.838117]
rwkv [start] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt [at 2024-02-06 11:56:57.842270]
rwkv [stdout] Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
rwkv [stdout] Requirement already satisfied: aiohttp==3.8.6 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 9)) (3.8.6)
rwkv [stdout] Requirement already satisfied: aiosignal==1.3.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 13)) (1.3.1)
rwkv [stdout] Requirement already satisfied: antlr4-python3-runtime==4.9.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 17)) (4.9.3)
rwkv [stdout] Requirement already satisfied: asttokens==2.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 21)) (2.4.1)
rwkv [stdout] Requirement already satisfied: async-timeout==4.0.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 25)) (4.0.3)
rwkv [stdout] Requirement already satisfied: attrs==23.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 29)) (23.1.0)
rwkv [stdout] Requirement already satisfied: certifi==2023.7.22 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 33)) (2023.7.22)
rwkv [stdout] Requirement already satisfied: charset-normalizer==3.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 37)) (3.3.2)
rwkv [stdout] Requirement already satisfied: codefind==0.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 42)) (0.1.3)
rwkv [stdout] Requirement already satisfied: deepspeed==0.12.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 46)) (0.12.2)
rwkv [stdout] Requirement already satisfied: executing==1.2.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 48)) (1.2.0)
rwkv [stdout] Requirement already satisfied: filelock==3.13.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 52)) (3.13.1)
rwkv [stdout] Requirement already satisfied: frozenlist==1.4.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 57)) (1.4.0)
rwkv [stdout] Requirement already satisfied: fsspec==2023.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from fsspec[http]==2023.10.0->-r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 62)) (2023.10.0)
rwkv [stdout] Requirement already satisfied: giving==0.4.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 67)) (0.4.2)
rwkv [stdout] Requirement already satisfied: hjson==3.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 72)) (3.1.0)
rwkv [stdout] Requirement already satisfied: idna==3.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 76)) (3.4)
rwkv [stdout] Requirement already satisfied: jinja2==3.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 81)) (3.1.2)
rwkv [stdout] Requirement already satisfied: lightning-utilities==0.9.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 85)) (0.9.0)
rwkv [stdout] Requirement already satisfied: markdown-it-py==3.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 90)) (3.0.0)
rwkv [stdout] Requirement already satisfied: markupsafe==2.1.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 94)) (2.1.3)
rwkv [stdout] Requirement already satisfied: mdurl==0.1.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 98)) (0.1.2)
rwkv [stdout] Requirement already satisfied: mpmath==1.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 102)) (1.3.0)
rwkv [stdout] Requirement already satisfied: multidict==6.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 106)) (6.0.4)
rwkv [stdout] Requirement already satisfied: networkx==3.2.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 111)) (3.2.1)
rwkv [stdout] Requirement already satisfied: ninja==1.11.1.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 115)) (1.11.1.1)
rwkv [stdout] Requirement already satisfied: numpy==1.26.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 119)) (1.26.1)
rwkv [stdout] Requirement already satisfied: omegaconf==2.3.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 125)) (2.3.0)
rwkv [stdout] Requirement already satisfied: ovld==0.3.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 129)) (0.3.2)
rwkv [stdout] Requirement already satisfied: packaging==23.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 133)) (23.2)
rwkv [stdout] Requirement already satisfied: psutil==5.9.6 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 140)) (5.9.6)
rwkv [stdout] Requirement already satisfied: ptera==1.4.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 144)) (1.4.1)
rwkv [stdout] Requirement already satisfied: py-cpuinfo==9.0.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 148)) (9.0.0)
rwkv [stdout] Requirement already satisfied: pydantic==1.10.13 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 152)) (1.10.13)
rwkv [stdout] Requirement already satisfied: pygments==2.16.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 156)) (2.16.1)
rwkv [stdout] Requirement already satisfied: pynvml==11.5.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 160)) (11.5.0)
rwkv [stdout] Collecting pytorch-lightning==1.9.5 (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 165))
rwkv [stdout]   Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)
rwkv [stdout]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 829.5/829.5 kB 21.1 MB/s eta 0:00:00
rwkv [stdout] 
rwkv [stdout] Requirement already satisfied: pyyaml==6.0.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 167)) (6.0.1)
rwkv [stdout] Requirement already satisfied: reactivex==4.0.4 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 172)) (4.0.4)
rwkv [stdout] Requirement already satisfied: requests==2.31.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 176)) (2.31.0)
rwkv [stdout] Requirement already satisfied: rich==13.6.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 180)) (13.6.0)
rwkv [stdout] Requirement already satisfied: six==1.16.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 184)) (1.16.0)
rwkv [stdout] Requirement already satisfied: sympy==1.12 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 188)) (1.12)
rwkv [stdout] Requirement already satisfied: torch==2.1.0+cu118 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 192)) (2.1.0+cu118)
rwkv [stdout] Requirement already satisfied: torchmetrics==1.0.3 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 198)) (1.0.3)
rwkv [stdout] Requirement already satisfied: tqdm==4.66.1 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 202)) (4.66.1)
rwkv [stdout] Requirement already satisfied: triton==2.1.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 207)) (2.1.0)
rwkv [stdout] Requirement already satisfied: typing-extensions==4.8.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 211)) (4.8.0)
rwkv [stdout] Requirement already satisfied: urllib3==1.26.18 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 219)) (1.26.18)
rwkv [stdout] Requirement already satisfied: varname==0.10.0 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 223)) (0.10.0)
rwkv [stdout] Requirement already satisfied: voir==0.2.11 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 227)) (0.2.11)
rwkv [stdout] Requirement already satisfied: yarl==1.9.2 in ./base/venv/torch/lib/python3.9/site-packages (from -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt (line 229)) (1.9.2)
rwkv [stdout] Installing collected packages: pytorch-lightning
rwkv [stdout] Successfully installed pytorch-lightning-1.9.5
rwkv [stderr] 
rwkv [stderr] [notice] A new release of pip is available: 23.3.2 -> 24.0
rwkv [stderr] [notice] To update, run: pip install --upgrade pip
rwkv [end] pip install -r /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/requirements.cuda.txt [at 2024-02-06 11:57:00.202729]
[DONE] Reports directory: /Tmp/slurm.4115007.0/base/runs/install.2024-02-06_11:54:49.283031

Prepare
-------
llama [config.system.arch] cuda
llama [config.system.sshkey] None
llama [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
llama [config.system.gpu.capacity] 81920 MiB
llama [config.system.self.name] local
llama [config.system.self.ip] 127.0.0.1
llama [config.system.self.port] 8123
llama [config.system.self.user] root
llama [config.system.self.main] True
llama [config.system.self.hostname] localhost
llama [config.system.self.aliaslist] []
llama [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
llama [config.system.self.local] True
llama [config.dirs.base] /Tmp/slurm.4115007.0/base
llama [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
llama [config.dirs.data] /Tmp/slurm.4115007.0/base/data
llama [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
llama [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/llm
llama [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
llama [config.group] llm
llama [config.install_group] torch
llama [config.install_variant] cuda
llama [config.run_name] prepare.2024-02-06_11:57:01.236402
llama [config.enabled] True
llama [config.capabilities.nodes] 1
llama [config.max_duration] 800
llama [config.voir.options.stop] 30
llama [config.voir.options.interval] 1s
llama [config.validation.usage.gpu_load_threshold] 0.5
llama [config.validation.usage.gpu_mem_threshold] 0.5
llama [config.config_base] /Tmp/slurm.4115007.0/milabench/config
llama [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
llama [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/llama
llama [config.plan.method] per_gpu
llama [config.tags] ['llm', 'nlp']
llama [config.weight] 1.0
llama [config.name] llama
llama [config.tag] ['llama']
llama [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256624.092656,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
llama [start] python /Tmp/slurm.4115007.0/milabench/benchmarks/llama/main.py --prepare --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 11:57:04.111442]
llama [stderr] Dataset
llama [stderr] Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]
llama [stderr] Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 63.1MB/s]
llama [stderr] Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
llama [stderr] 
llama [stderr] Downloading data:   0%|          | 0.00/722k [00:00<?, ?B/s][A
llama [stderr] 
llama [stderr] Downloading data: 100%|██████████| 722k/722k [00:00<00:00, 5.36MB/s][A
llama [stderr] Downloading data: 100%|██████████| 722k/722k [00:00<00:00, 5.35MB/s]
llama [stderr] Downloading data files:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
llama [stderr] 
llama [stderr] Downloading data:   0%|          | 0.00/156M [00:00<?, ?B/s][A
llama [stderr] 
llama [stderr] Downloading data:   3%|▎         | 4.19M/156M [00:00<00:07, 19.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:   8%|▊         | 12.6M/156M [00:00<00:03, 38.7MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  13%|█▎        | 21.0M/156M [00:00<00:02, 48.3MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  19%|█▉        | 29.4M/156M [00:00<00:02, 53.7MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  24%|██▍       | 37.7M/156M [00:00<00:02, 57.6MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  30%|██▉       | 46.1M/156M [00:00<00:01, 58.2MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  35%|███▌      | 54.5M/156M [00:01<00:02, 48.0MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  40%|████      | 62.9M/156M [00:01<00:02, 45.2MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  46%|████▌     | 71.3M/156M [00:01<00:01, 50.4MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  51%|█████     | 79.7M/156M [00:01<00:01, 51.3MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  57%|█████▋    | 88.1M/156M [00:01<00:01, 52.0MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  62%|██████▏   | 96.5M/156M [00:01<00:01, 49.7MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  67%|██████▋   | 105M/156M [00:02<00:01, 49.0MB/s] [A
llama [stderr] 
llama [stderr] Downloading data:  73%|███████▎  | 113M/156M [00:02<00:00, 53.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  78%|███████▊  | 122M/156M [00:02<00:00, 53.0MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  83%|████████▎ | 130M/156M [00:02<00:00, 39.7MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  89%|████████▉ | 138M/156M [00:02<00:00, 45.1MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  94%|█████████▍| 147M/156M [00:03<00:00, 47.8MB/s][A
llama [stderr] Downloading data: 100%|██████████| 156M/156M [00:03<00:00, 49.3MB/s]
llama [stderr] 
llama [stderr] Downloading data:   0%|          | 0.00/156M [00:00<?, ?B/s][A
llama [stderr] 
llama [stderr] Downloading data:   3%|▎         | 4.19M/156M [00:00<00:24, 6.30MB/s][A
llama [stderr] 
llama [stderr] Downloading data:   8%|▊         | 12.6M/156M [00:00<00:07, 18.8MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  13%|█▎        | 21.0M/156M [00:01<00:07, 19.0MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  19%|█▉        | 29.4M/156M [00:01<00:05, 24.0MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  24%|██▍       | 37.7M/156M [00:01<00:03, 31.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  30%|██▉       | 46.1M/156M [00:01<00:02, 38.2MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  35%|███▍      | 54.5M/156M [00:02<00:02, 35.1MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  40%|████      | 62.9M/156M [00:02<00:02, 41.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  46%|████▌     | 71.3M/156M [00:02<00:01, 46.9MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  51%|█████     | 79.7M/156M [00:02<00:01, 51.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  56%|█████▋    | 88.1M/156M [00:02<00:01, 55.7MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  62%|██████▏   | 96.5M/156M [00:02<00:01, 58.8MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  67%|██████▋   | 105M/156M [00:02<00:00, 61.4MB/s] [A
llama [stderr] 
llama [stderr] Downloading data:  73%|███████▎  | 113M/156M [00:02<00:00, 49.5MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  78%|███████▊  | 122M/156M [00:03<00:00, 46.2MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  83%|████████▎ | 130M/156M [00:03<00:00, 51.3MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  89%|████████▉ | 138M/156M [00:03<00:00, 55.2MB/s][A
llama [stderr] 
llama [stderr] Downloading data:  94%|█████████▍| 147M/156M [00:03<00:00, 58.1MB/s][A
llama [stderr] 
llama [stderr] Downloading data: 100%|█████████▉| 155M/156M [00:03<00:00, 62.0MB/s][A
llama [stderr] Downloading data: 100%|██████████| 156M/156M [00:03<00:00, 41.9MB/s]
llama [stderr] Downloading data files:  67%|██████▋   | 2/3 [00:07<00:04,  4.10s/it]
llama [stderr] 
llama [stderr] Downloading data:   0%|          | 0.00/655k [00:00<?, ?B/s][A
llama [stderr] Downloading data: 100%|██████████| 655k/655k [00:00<00:00, 11.6MB/s]
llama [stderr] Downloading data files: 100%|██████████| 3/3 [00:07<00:00,  2.36s/it]
llama [stderr] Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]
llama [stderr] Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2649.59it/s]
llama [stderr] Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]
llama [stderr] Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 698623.18 examples/s]
llama [stderr] Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]
llama [stderr] Generating train split:   8%|▊         | 140000/1801350 [00:00<00:01, 1264691.44 examples/s]
llama [stderr] Generating train split:  16%|█▌        | 280000/1801350 [00:00<00:01, 1283739.72 examples/s]
llama [stderr] Generating train split:  23%|██▎       | 420000/1801350 [00:00<00:01, 1294039.94 examples/s]
llama [stderr] Generating train split:  31%|███       | 560000/1801350 [00:00<00:00, 1293958.53 examples/s]
llama [stderr] Generating train split:  39%|███▉      | 700000/1801350 [00:00<00:00, 1301847.80 examples/s]
llama [stderr] Generating train split:  47%|████▋     | 840000/1801350 [00:00<00:00, 1303640.35 examples/s]
llama [stderr] Generating train split:  54%|█████▍    | 980675/1801350 [00:00<00:00, 1295149.09 examples/s]
llama [stderr] Generating train split:  62%|██████▏   | 1120675/1801350 [00:00<00:00, 1303534.35 examples/s]
llama [stderr] Generating train split:  70%|██████▉   | 1260675/1801350 [00:00<00:00, 1295086.47 examples/s]
llama [stderr] Generating train split:  78%|███████▊  | 1400675/1801350 [00:01<00:00, 1302015.58 examples/s]
llama [stderr] Generating train split:  86%|████████▌ | 1540675/1801350 [00:01<00:00, 1296721.80 examples/s]
llama [stderr] Generating train split:  93%|█████████▎| 1680675/1801350 [00:01<00:00, 1305936.77 examples/s]
llama [stderr] Generating train split: 100%|██████████| 1801350/1801350 [00:01<00:00, 1300219.13 examples/s]
llama [stderr] Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]
llama [stderr] Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1054888.50 examples/s]
llama [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
llama [stderr]   table = cls._concat_blocks(blocks, axis=0)
llama [stderr] Tokenizer
llama [stderr] Downloading tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]
llama [stderr] Downloading tokenizer_config.json: 100%|██████████| 700/700 [00:00<00:00, 266kB/s]
llama [stderr] Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]
llama [stderr] Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 126MB/s]
llama [stderr] Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]
llama [stderr] Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 36.0MB/s]
llama [stderr] Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]
llama [stderr] Downloading (…)cial_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 715kB/s]
llama [stderr] 
llama [end] python /Tmp/slurm.4115007.0/milabench/benchmarks/llama/main.py --prepare --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 11:57:16.661137]
fp16 [config.system.arch] cuda
fp16 [config.system.sshkey] None
fp16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
fp16 [config.system.gpu.capacity] 81920 MiB
fp16 [config.system.self.name] local
fp16 [config.system.self.ip] 127.0.0.1
fp16 [config.system.self.port] 8123
fp16 [config.system.self.user] root
fp16 [config.system.self.main] True
fp16 [config.system.self.hostname] localhost
fp16 [config.system.self.aliaslist] []
fp16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
fp16 [config.system.self.local] True
fp16 [config.dirs.base] /Tmp/slurm.4115007.0/base
fp16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
fp16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
fp16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
fp16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/flops
fp16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
fp16 [config.group] flops
fp16 [config.install_group] torch
fp16 [config.install_variant] cuda
fp16 [config.run_name] prepare.2024-02-06_11:57:01.236402
fp16 [config.enabled] True
fp16 [config.capabilities.nodes] 1
fp16 [config.max_duration] 600
fp16 [config.voir.options.stop] 60
fp16 [config.voir.options.interval] 1s
fp16 [config.validation.usage.gpu_load_threshold] 0.5
fp16 [config.validation.usage.gpu_mem_threshold] 0.5
fp16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
fp16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
fp16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/flops
fp16 [config.plan.method] per_gpu
fp16 [config.tags] ['diagnostic', 'flops']
fp16 [config.argv.--number] 30
fp16 [config.argv.--repeat] 90
fp16 [config.argv.--m] 8192
fp16 [config.argv.--n] 8192
fp16 [config.argv.--dtype] fp16
fp16 [config.weight] 0.0
fp16 [config.name] fp16
fp16 [config.tag] ['fp16']
fp16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256639.022259,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
fp16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 30 --repeat 90 --m 8192 --n 8192 --dtype fp16 [at 2024-02-06 11:57:19.040636]
fp16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 30 --repeat 90 --m 8192 --n 8192 --dtype fp16 [at 2024-02-06 11:57:19.053553]
bf16 [config.system.arch] cuda
bf16 [config.system.sshkey] None
bf16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
bf16 [config.system.gpu.capacity] 81920 MiB
bf16 [config.system.self.name] local
bf16 [config.system.self.ip] 127.0.0.1
bf16 [config.system.self.port] 8123
bf16 [config.system.self.user] root
bf16 [config.system.self.main] True
bf16 [config.system.self.hostname] localhost
bf16 [config.system.self.aliaslist] []
bf16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
bf16 [config.system.self.local] True
bf16 [config.dirs.base] /Tmp/slurm.4115007.0/base
bf16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
bf16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
bf16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
bf16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/flops
bf16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
bf16 [config.group] flops
bf16 [config.install_group] torch
bf16 [config.install_variant] cuda
bf16 [config.run_name] prepare.2024-02-06_11:57:01.236402
bf16 [config.enabled] True
bf16 [config.capabilities.nodes] 1
bf16 [config.max_duration] 600
bf16 [config.voir.options.stop] 60
bf16 [config.voir.options.interval] 1s
bf16 [config.validation.usage.gpu_load_threshold] 0.5
bf16 [config.validation.usage.gpu_mem_threshold] 0.5
bf16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
bf16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
bf16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/flops
bf16 [config.plan.method] per_gpu
bf16 [config.tags] ['diagnostic', 'flops']
bf16 [config.argv.--number] 10
bf16 [config.argv.--repeat] 90
bf16 [config.argv.--m] 8192
bf16 [config.argv.--n] 8192
bf16 [config.argv.--dtype] bf16
bf16 [config.weight] 0.0
bf16 [config.name] bf16
bf16 [config.tag] ['bf16']
bf16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256641.408493,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
bf16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype bf16 [at 2024-02-06 11:57:21.426759]
bf16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype bf16 [at 2024-02-06 11:57:21.439937]
tf32 [config.system.arch] cuda
tf32 [config.system.sshkey] None
tf32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
tf32 [config.system.gpu.capacity] 81920 MiB
tf32 [config.system.self.name] local
tf32 [config.system.self.ip] 127.0.0.1
tf32 [config.system.self.port] 8123
tf32 [config.system.self.user] root
tf32 [config.system.self.main] True
tf32 [config.system.self.hostname] localhost
tf32 [config.system.self.aliaslist] []
tf32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
tf32 [config.system.self.local] True
tf32 [config.dirs.base] /Tmp/slurm.4115007.0/base
tf32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
tf32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
tf32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
tf32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/flops
tf32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
tf32 [config.group] flops
tf32 [config.install_group] torch
tf32 [config.install_variant] cuda
tf32 [config.run_name] prepare.2024-02-06_11:57:01.236402
tf32 [config.enabled] True
tf32 [config.capabilities.nodes] 1
tf32 [config.max_duration] 600
tf32 [config.voir.options.stop] 60
tf32 [config.voir.options.interval] 1s
tf32 [config.validation.usage.gpu_load_threshold] 0.5
tf32 [config.validation.usage.gpu_mem_threshold] 0.5
tf32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
tf32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
tf32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/flops
tf32 [config.plan.method] per_gpu
tf32 [config.tags] ['diagnostic', 'flops']
tf32 [config.argv.--number] 10
tf32 [config.argv.--repeat] 90
tf32 [config.argv.--m] 8192
tf32 [config.argv.--n] 8192
tf32 [config.argv.--dtype] fp32
tf32 [config.argv.--tf32] True
tf32 [config.weight] 0.0
tf32 [config.name] tf32
tf32 [config.tag] ['tf32']
tf32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256643.765601,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
tf32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype fp32 --tf32 [at 2024-02-06 11:57:23.784959]
tf32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype fp32 --tf32 [at 2024-02-06 11:57:23.797990]
fp32 [config.system.arch] cuda
fp32 [config.system.sshkey] None
fp32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
fp32 [config.system.gpu.capacity] 81920 MiB
fp32 [config.system.self.name] local
fp32 [config.system.self.ip] 127.0.0.1
fp32 [config.system.self.port] 8123
fp32 [config.system.self.user] root
fp32 [config.system.self.main] True
fp32 [config.system.self.hostname] localhost
fp32 [config.system.self.aliaslist] []
fp32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
fp32 [config.system.self.local] True
fp32 [config.dirs.base] /Tmp/slurm.4115007.0/base
fp32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
fp32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
fp32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
fp32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/flops
fp32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
fp32 [config.group] flops
fp32 [config.install_group] torch
fp32 [config.install_variant] cuda
fp32 [config.run_name] prepare.2024-02-06_11:57:01.236402
fp32 [config.enabled] True
fp32 [config.capabilities.nodes] 1
fp32 [config.max_duration] 600
fp32 [config.voir.options.stop] 60
fp32 [config.voir.options.interval] 1s
fp32 [config.validation.usage.gpu_load_threshold] 0.5
fp32 [config.validation.usage.gpu_mem_threshold] 0.5
fp32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
fp32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
fp32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/flops
fp32 [config.plan.method] per_gpu
fp32 [config.tags] ['diagnostic', 'flops']
fp32 [config.argv.--number] 10
fp32 [config.argv.--repeat] 90
fp32 [config.argv.--m] 8192
fp32 [config.argv.--n] 8192
fp32 [config.argv.--dtype] fp32
fp32 [config.weight] 0.0
fp32 [config.name] fp32
fp32 [config.tag] ['fp32']
fp32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256646.114429,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
fp32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype fp32 [at 2024-02-06 11:57:26.133993]
fp32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/flops/prepare.py --number 10 --repeat 90 --m 8192 --n 8192 --dtype fp32 [at 2024-02-06 11:57:26.147472]
resnet50 [config.system.arch] cuda
resnet50 [config.system.sshkey] None
resnet50 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
resnet50 [config.system.gpu.capacity] 81920 MiB
resnet50 [config.system.self.name] local
resnet50 [config.system.self.ip] 127.0.0.1
resnet50 [config.system.self.port] 8123
resnet50 [config.system.self.user] root
resnet50 [config.system.self.main] True
resnet50 [config.system.self.hostname] localhost
resnet50 [config.system.self.aliaslist] []
resnet50 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
resnet50 [config.system.self.local] True
resnet50 [config.dirs.base] /Tmp/slurm.4115007.0/base
resnet50 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
resnet50 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
resnet50 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
resnet50 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
resnet50 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
resnet50 [config.group] torchvision
resnet50 [config.install_group] torch
resnet50 [config.install_variant] cuda
resnet50 [config.run_name] prepare.2024-02-06_11:57:01.236402
resnet50 [config.enabled] True
resnet50 [config.capabilities.nodes] 1
resnet50 [config.max_duration] 600
resnet50 [config.voir.options.stop] 60
resnet50 [config.voir.options.interval] 1s
resnet50 [config.validation.usage.gpu_load_threshold] 0.5
resnet50 [config.validation.usage.gpu_mem_threshold] 0.5
resnet50 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
resnet50 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
resnet50 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
resnet50 [config.plan.method] per_gpu
resnet50 [config.argv.--precision] tf32-fp16
resnet50 [config.argv.--lr] 0.01
resnet50 [config.argv.--no-stdout] True
resnet50 [config.argv.--epochs] 50
resnet50 [config.argv.--model] resnet50
resnet50 [config.argv.--batch-size] 64
resnet50 [config.tags] ['classification', 'convnet', 'resnet', 'vision']
resnet50 [config.weight] 1.0
resnet50 [config.name] resnet50
resnet50 [config.tag] ['resnet50']
resnet50 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.895,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.78,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256648.500578,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
resnet50 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model resnet50 --batch-size 64 [at 2024-02-06 11:57:28.519917]
resnet50 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
resnet50 [stdout] Generating train
resnet50 [stderr]   0%|          | 0/4096 [00:00<?, ?it/s]
resnet50 [stderr]   0%|          | 1/4096 [00:03<3:35:12,  3.15s/it]
resnet50 [stderr]   0%|          | 2/4096 [00:03<1:33:00,  1.36s/it]
resnet50 [stderr]   0%|          | 4/4096 [00:03<37:04,  1.84it/s]
resnet50 [stderr]   0%|          | 9/4096 [00:03<12:31,  5.44it/s]
resnet50 [stderr]   0%|          | 14/4096 [00:03<07:07,  9.54it/s]
resnet50 [stderr]   0%|          | 20/4096 [00:03<04:24, 15.41it/s]
resnet50 [stderr]   1%|          | 27/4096 [00:03<02:59, 22.68it/s]
resnet50 [stderr]   1%|          | 34/4096 [00:03<02:16, 29.73it/s]
resnet50 [stderr]   1%|          | 42/4096 [00:04<01:50, 36.57it/s]
resnet50 [stderr]   1%|          | 49/4096 [00:04<01:36, 41.96it/s]
resnet50 [stderr]   1%|▏         | 56/4096 [00:04<01:24, 47.97it/s]
resnet50 [stderr]   2%|▏         | 64/4096 [00:04<01:14, 53.90it/s]
resnet50 [stderr]   2%|▏         | 71/4096 [00:04<01:10, 56.90it/s]
resnet50 [stderr]   2%|▏         | 78/4096 [00:04<01:09, 57.88it/s]
resnet50 [stderr]   2%|▏         | 85/4096 [00:04<01:05, 60.83it/s]
resnet50 [stderr]   2%|▏         | 92/4096 [00:04<01:08, 58.26it/s]
resnet50 [stderr]   2%|▏         | 100/4096 [00:04<01:03, 62.64it/s]
resnet50 [stderr]   3%|▎         | 107/4096 [00:05<01:03, 62.62it/s]
resnet50 [stderr]   3%|▎         | 114/4096 [00:05<01:06, 60.32it/s]
resnet50 [stderr]   3%|▎         | 124/4096 [00:05<00:58, 67.68it/s]
resnet50 [stderr]   3%|▎         | 131/4096 [00:05<01:04, 61.49it/s]
resnet50 [stderr]   3%|▎         | 139/4096 [00:05<01:05, 60.66it/s]
resnet50 [stderr]   4%|▎         | 147/4096 [00:05<01:00, 65.14it/s]
resnet50 [stderr]   4%|▍         | 156/4096 [00:05<00:57, 68.27it/s]
resnet50 [stderr]   4%|▍         | 163/4096 [00:05<01:02, 63.18it/s]
resnet50 [stderr]   4%|▍         | 170/4096 [00:06<01:03, 62.27it/s]
resnet50 [stderr]   4%|▍         | 177/4096 [00:06<01:04, 60.53it/s]
resnet50 [stderr]   4%|▍         | 184/4096 [00:06<01:02, 62.41it/s]
resnet50 [stderr]   5%|▍         | 191/4096 [00:06<01:00, 64.10it/s]
resnet50 [stderr]   5%|▍         | 199/4096 [00:06<00:58, 66.18it/s]
resnet50 [stderr]   5%|▌         | 206/4096 [00:06<01:02, 62.10it/s]
resnet50 [stderr]   5%|▌         | 213/4096 [00:06<01:04, 60.44it/s]
resnet50 [stderr]   5%|▌         | 222/4096 [00:06<00:59, 64.69it/s]
resnet50 [stderr]   6%|▌         | 229/4096 [00:07<01:02, 61.87it/s]
resnet50 [stderr]   6%|▌         | 237/4096 [00:07<00:59, 64.59it/s]
resnet50 [stderr]   6%|▌         | 244/4096 [00:07<00:58, 65.79it/s]
resnet50 [stderr]   6%|▌         | 251/4096 [00:07<01:01, 63.02it/s]
resnet50 [stderr]   6%|▋         | 258/4096 [00:07<01:01, 62.87it/s]
resnet50 [stderr]   6%|▋         | 265/4096 [00:07<01:00, 63.81it/s]
resnet50 [stderr]   7%|▋         | 272/4096 [00:07<01:00, 63.71it/s]
resnet50 [stderr]   7%|▋         | 279/4096 [00:07<01:03, 59.93it/s]
resnet50 [stderr]   7%|▋         | 287/4096 [00:07<01:03, 60.15it/s]
resnet50 [stderr]   7%|▋         | 295/4096 [00:08<00:59, 63.54it/s]
resnet50 [stderr]   7%|▋         | 303/4096 [00:08<00:59, 63.58it/s]
resnet50 [stderr]   8%|▊         | 311/4096 [00:08<01:00, 62.87it/s]
resnet50 [stderr]   8%|▊         | 319/4096 [00:08<00:59, 63.34it/s]
resnet50 [stderr]   8%|▊         | 327/4096 [00:08<00:58, 64.35it/s]
resnet50 [stderr]   8%|▊         | 335/4096 [00:08<00:56, 66.27it/s]
resnet50 [stderr]   8%|▊         | 343/4096 [00:08<00:57, 65.08it/s]
resnet50 [stderr]   9%|▊         | 351/4096 [00:08<00:57, 64.75it/s]
resnet50 [stderr]   9%|▊         | 358/4096 [00:09<00:57, 64.51it/s]
resnet50 [stderr]   9%|▉         | 367/4096 [00:09<00:53, 70.02it/s]
resnet50 [stderr]   9%|▉         | 375/4096 [00:09<00:55, 67.21it/s]
resnet50 [stderr]   9%|▉         | 382/4096 [00:09<00:56, 65.64it/s]
resnet50 [stderr]  10%|▉         | 391/4096 [00:09<00:55, 66.59it/s]
resnet50 [stderr]  10%|▉         | 398/4096 [00:09<00:56, 65.31it/s]
resnet50 [stderr]  10%|▉         | 406/4096 [00:09<00:54, 68.00it/s]
resnet50 [stderr]  10%|█         | 414/4096 [00:09<00:58, 62.67it/s]
resnet50 [stderr]  10%|█         | 422/4096 [00:10<00:57, 64.17it/s]
resnet50 [stderr]  10%|█         | 429/4096 [00:10<00:58, 62.45it/s]
resnet50 [stderr]  11%|█         | 436/4096 [00:10<01:04, 57.15it/s]
resnet50 [stderr]  11%|█         | 443/4096 [00:10<01:02, 58.59it/s]
resnet50 [stderr]  11%|█         | 452/4096 [00:10<00:54, 66.61it/s]
resnet50 [stderr]  11%|█         | 459/4096 [00:10<00:54, 66.34it/s]
resnet50 [stderr]  11%|█▏        | 467/4096 [00:10<00:56, 64.21it/s]
resnet50 [stderr]  12%|█▏        | 474/4096 [00:10<00:55, 65.57it/s]
resnet50 [stderr]  12%|█▏        | 481/4096 [00:10<00:58, 62.20it/s]
resnet50 [stderr]  12%|█▏        | 489/4096 [00:11<00:57, 63.13it/s]
resnet50 [stderr]  12%|█▏        | 497/4096 [00:11<00:53, 67.35it/s]
resnet50 [stderr]  12%|█▏        | 505/4096 [00:11<00:56, 64.05it/s]
resnet50 [stderr]  12%|█▎        | 512/4096 [00:11<00:55, 64.90it/s]
resnet50 [stderr]  13%|█▎        | 519/4096 [00:11<00:55, 64.22it/s]
resnet50 [stderr]  13%|█▎        | 527/4096 [00:11<00:52, 67.52it/s]
resnet50 [stderr]  13%|█▎        | 534/4096 [00:11<00:55, 64.63it/s]
resnet50 [stderr]  13%|█▎        | 543/4096 [00:11<00:50, 70.27it/s]
resnet50 [stderr]  13%|█▎        | 551/4096 [00:11<00:50, 70.62it/s]
resnet50 [stderr]  14%|█▎        | 559/4096 [00:12<00:52, 67.31it/s]
resnet50 [stderr]  14%|█▍        | 566/4096 [00:12<00:51, 67.98it/s]
resnet50 [stderr]  14%|█▍        | 573/4096 [00:12<00:58, 60.33it/s]
resnet50 [stderr]  14%|█▍        | 580/4096 [00:12<00:57, 61.14it/s]
resnet50 [stderr]  14%|█▍        | 587/4096 [00:12<00:55, 63.08it/s]
resnet50 [stderr]  15%|█▍        | 594/4096 [00:12<00:56, 62.43it/s]
resnet50 [stderr]  15%|█▍        | 602/4096 [00:12<00:52, 66.35it/s]
resnet50 [stderr]  15%|█▍        | 609/4096 [00:12<00:53, 65.55it/s]
resnet50 [stderr]  15%|█▌        | 617/4096 [00:13<00:52, 65.90it/s]
resnet50 [stderr]  15%|█▌        | 624/4096 [00:13<00:54, 63.31it/s]
resnet50 [stderr]  15%|█▌        | 632/4096 [00:13<00:51, 67.35it/s]
resnet50 [stderr]  16%|█▌        | 639/4096 [00:13<00:51, 67.09it/s]
resnet50 [stderr]  16%|█▌        | 648/4096 [00:13<00:52, 66.08it/s]
resnet50 [stderr]  16%|█▌        | 656/4096 [00:13<00:51, 67.35it/s]
resnet50 [stderr]  16%|█▌        | 664/4096 [00:13<00:50, 67.66it/s]
resnet50 [stderr]  16%|█▋        | 671/4096 [00:13<00:54, 62.96it/s]
resnet50 [stderr]  17%|█▋        | 678/4096 [00:13<00:53, 64.26it/s]
resnet50 [stderr]  17%|█▋        | 686/4096 [00:14<00:50, 67.56it/s]
resnet50 [stderr]  17%|█▋        | 693/4096 [00:14<00:53, 63.13it/s]
resnet50 [stderr]  17%|█▋        | 702/4096 [00:14<00:48, 69.56it/s]
resnet50 [stderr]  17%|█▋        | 710/4096 [00:14<00:52, 64.22it/s]
resnet50 [stderr]  18%|█▊        | 718/4096 [00:14<00:50, 66.88it/s]
resnet50 [stderr]  18%|█▊        | 725/4096 [00:14<00:52, 64.75it/s]
resnet50 [stderr]  18%|█▊        | 732/4096 [00:14<00:51, 65.68it/s]
resnet50 [stderr]  18%|█▊        | 740/4096 [00:14<00:51, 65.09it/s]
resnet50 [stderr]  18%|█▊        | 748/4096 [00:15<00:49, 67.09it/s]
resnet50 [stderr]  18%|█▊        | 757/4096 [00:15<00:51, 64.91it/s]
resnet50 [stderr]  19%|█▊        | 764/4096 [00:15<00:52, 63.15it/s]
resnet50 [stderr]  19%|█▉        | 771/4096 [00:15<00:53, 61.92it/s]
resnet50 [stderr]  19%|█▉        | 779/4096 [00:15<00:50, 65.35it/s]
resnet50 [stderr]  19%|█▉        | 787/4096 [00:15<00:49, 67.34it/s]
resnet50 [stderr]  19%|█▉        | 795/4096 [00:15<00:48, 68.31it/s]
resnet50 [stderr]  20%|█▉        | 804/4096 [00:15<00:48, 67.82it/s]
resnet50 [stderr]  20%|█▉        | 812/4096 [00:15<00:46, 70.28it/s]
resnet50 [stderr]  20%|██        | 820/4096 [00:16<00:48, 67.77it/s]
resnet50 [stderr]  20%|██        | 827/4096 [00:16<00:47, 68.20it/s]
resnet50 [stderr]  20%|██        | 834/4096 [00:16<00:49, 66.15it/s]
resnet50 [stderr]  21%|██        | 842/4096 [00:16<00:48, 66.81it/s]
resnet50 [stderr]  21%|██        | 849/4096 [00:16<00:48, 67.15it/s]
resnet50 [stderr]  21%|██        | 856/4096 [00:16<00:49, 65.33it/s]
resnet50 [stderr]  21%|██        | 863/4096 [00:16<00:50, 64.48it/s]
resnet50 [stderr]  21%|██▏       | 871/4096 [00:16<00:49, 65.39it/s]
resnet50 [stderr]  21%|██▏       | 878/4096 [00:16<00:48, 65.89it/s]
resnet50 [stderr]  22%|██▏       | 885/4096 [00:17<00:50, 63.99it/s]
resnet50 [stderr]  22%|██▏       | 892/4096 [00:17<00:49, 64.23it/s]
resnet50 [stderr]  22%|██▏       | 900/4096 [00:17<00:48, 65.93it/s]
resnet50 [stderr]  22%|██▏       | 908/4096 [00:17<00:48, 65.25it/s]
resnet50 [stderr]  22%|██▏       | 916/4096 [00:17<00:49, 64.33it/s]
resnet50 [stderr]  23%|██▎       | 924/4096 [00:17<00:47, 66.62it/s]
resnet50 [stderr]  23%|██▎       | 932/4096 [00:17<00:45, 69.48it/s]
resnet50 [stderr]  23%|██▎       | 939/4096 [00:17<00:46, 67.96it/s]
resnet50 [stderr]  23%|██▎       | 946/4096 [00:18<00:47, 66.23it/s]
resnet50 [stderr]  23%|██▎       | 954/4096 [00:18<00:46, 67.31it/s]
resnet50 [stderr]  23%|██▎       | 961/4096 [00:18<00:46, 67.80it/s]
resnet50 [stderr]  24%|██▎       | 968/4096 [00:18<00:47, 66.19it/s]
resnet50 [stderr]  24%|██▍       | 975/4096 [00:18<00:49, 63.67it/s]
resnet50 [stderr]  24%|██▍       | 983/4096 [00:18<00:47, 65.71it/s]
resnet50 [stderr]  24%|██▍       | 990/4096 [00:18<00:47, 65.04it/s]
resnet50 [stderr]  24%|██▍       | 998/4096 [00:18<00:47, 65.41it/s]
resnet50 [stderr]  25%|██▍       | 1005/4096 [00:18<00:47, 65.61it/s]
resnet50 [stderr]  25%|██▍       | 1012/4096 [00:19<00:47, 65.15it/s]
resnet50 [stderr]  25%|██▍       | 1020/4096 [00:19<00:45, 67.04it/s]
resnet50 [stderr]  25%|██▌       | 1027/4096 [00:19<00:47, 64.24it/s]
resnet50 [stderr]  25%|██▌       | 1036/4096 [00:19<00:45, 67.80it/s]
resnet50 [stderr]  25%|██▌       | 1043/4096 [00:19<00:48, 62.80it/s]
resnet50 [stderr]  26%|██▌       | 1051/4096 [00:19<00:45, 67.30it/s]
resnet50 [stderr]  26%|██▌       | 1058/4096 [00:19<00:45, 66.09it/s]
resnet50 [stderr]  26%|██▌       | 1065/4096 [00:19<00:47, 64.36it/s]
resnet50 [stderr]  26%|██▌       | 1072/4096 [00:19<00:49, 60.62it/s]
resnet50 [stderr]  26%|██▋       | 1080/4096 [00:20<00:45, 65.73it/s]
resnet50 [stderr]  27%|██▋       | 1088/4096 [00:20<00:44, 67.59it/s]
resnet50 [stderr]  27%|██▋       | 1095/4096 [00:20<00:56, 52.81it/s]
resnet50 [stderr]  27%|██▋       | 1107/4096 [00:20<00:43, 68.04it/s]
resnet50 [stderr]  27%|██▋       | 1115/4096 [00:20<00:44, 66.88it/s]
resnet50 [stderr]  27%|██▋       | 1123/4096 [00:20<00:43, 67.61it/s]
resnet50 [stderr]  28%|██▊       | 1131/4096 [00:20<00:44, 66.04it/s]
resnet50 [stderr]  28%|██▊       | 1138/4096 [00:20<00:46, 64.00it/s]
resnet50 [stderr]  28%|██▊       | 1145/4096 [00:21<00:46, 62.93it/s]
resnet50 [stderr]  28%|██▊       | 1153/4096 [00:21<00:46, 63.86it/s]
resnet50 [stderr]  28%|██▊       | 1160/4096 [00:21<00:45, 63.93it/s]
resnet50 [stderr]  28%|██▊       | 1167/4096 [00:21<00:48, 60.77it/s]
resnet50 [stderr]  29%|██▊       | 1176/4096 [00:21<00:42, 67.95it/s]
resnet50 [stderr]  29%|██▉       | 1183/4096 [00:21<00:43, 67.30it/s]
resnet50 [stderr]  29%|██▉       | 1190/4096 [00:21<00:44, 64.68it/s]
resnet50 [stderr]  29%|██▉       | 1198/4096 [00:21<00:42, 68.64it/s]
resnet50 [stderr]  29%|██▉       | 1205/4096 [00:22<00:46, 62.50it/s]
resnet50 [stderr]  30%|██▉       | 1212/4096 [00:22<00:45, 63.37it/s]
resnet50 [stderr]  30%|██▉       | 1220/4096 [00:22<00:44, 64.83it/s]
resnet50 [stderr]  30%|███       | 1230/4096 [00:22<00:39, 73.03it/s]
resnet50 [stderr]  30%|███       | 1238/4096 [00:22<00:42, 67.96it/s]
resnet50 [stderr]  30%|███       | 1245/4096 [00:22<00:43, 65.30it/s]
resnet50 [stderr]  31%|███       | 1254/4096 [00:22<00:42, 66.22it/s]
resnet50 [stderr]  31%|███       | 1261/4096 [00:22<00:44, 63.69it/s]
resnet50 [stderr]  31%|███       | 1269/4096 [00:22<00:43, 64.74it/s]
resnet50 [stderr]  31%|███       | 1276/4096 [00:23<00:44, 63.42it/s]
resnet50 [stderr]  31%|███▏      | 1284/4096 [00:23<00:44, 63.32it/s]
resnet50 [stderr]  32%|███▏      | 1293/4096 [00:23<00:40, 69.89it/s]
resnet50 [stderr]  32%|███▏      | 1301/4096 [00:23<00:41, 67.31it/s]
resnet50 [stderr]  32%|███▏      | 1308/4096 [00:23<00:43, 64.43it/s]
resnet50 [stderr]  32%|███▏      | 1316/4096 [00:23<00:40, 68.43it/s]
resnet50 [stderr]  32%|███▏      | 1323/4096 [00:23<00:42, 65.33it/s]
resnet50 [stderr]  32%|███▏      | 1330/4096 [00:23<00:42, 64.50it/s]
resnet50 [stderr]  33%|███▎      | 1337/4096 [00:24<00:41, 65.76it/s]
resnet50 [stderr]  33%|███▎      | 1346/4096 [00:24<00:40, 67.50it/s]
resnet50 [stderr]  33%|███▎      | 1354/4096 [00:24<00:39, 69.10it/s]
resnet50 [stderr]  33%|███▎      | 1361/4096 [00:24<00:41, 66.48it/s]
resnet50 [stderr]  33%|███▎      | 1368/4096 [00:24<00:40, 66.96it/s]
resnet50 [stderr]  34%|███▎      | 1375/4096 [00:24<00:42, 63.99it/s]
resnet50 [stderr]  34%|███▍      | 1384/4096 [00:24<00:38, 69.82it/s]
resnet50 [stderr]  34%|███▍      | 1392/4096 [00:24<00:40, 66.49it/s]
resnet50 [stderr]  34%|███▍      | 1400/4096 [00:24<00:41, 64.95it/s]
resnet50 [stderr]  34%|███▍      | 1408/4096 [00:25<00:41, 65.28it/s]
resnet50 [stderr]  35%|███▍      | 1416/4096 [00:25<00:41, 65.27it/s]
resnet50 [stderr]  35%|███▍      | 1423/4096 [00:25<00:42, 63.45it/s]
resnet50 [stderr]  35%|███▍      | 1431/4096 [00:25<00:40, 65.06it/s]
resnet50 [stderr]  35%|███▌      | 1440/4096 [00:25<00:37, 71.34it/s]
resnet50 [stderr]  35%|███▌      | 1448/4096 [00:25<00:40, 65.42it/s]
resnet50 [stderr]  36%|███▌      | 1455/4096 [00:25<00:42, 61.53it/s]
resnet50 [stderr]  36%|███▌      | 1465/4096 [00:25<00:37, 70.10it/s]
resnet50 [stderr]  36%|███▌      | 1473/4096 [00:26<00:38, 67.71it/s]
resnet50 [stderr]  36%|███▌      | 1480/4096 [00:26<00:38, 67.09it/s]
resnet50 [stderr]  36%|███▋      | 1487/4096 [00:26<00:39, 66.27it/s]
resnet50 [stderr]  36%|███▋      | 1495/4096 [00:26<00:37, 68.52it/s]
resnet50 [stderr]  37%|███▋      | 1503/4096 [00:26<00:36, 71.08it/s]
resnet50 [stderr]  37%|███▋      | 1511/4096 [00:26<00:39, 65.70it/s]
resnet50 [stderr]  37%|███▋      | 1518/4096 [00:26<00:40, 64.21it/s]
resnet50 [stderr]  37%|███▋      | 1525/4096 [00:26<00:40, 63.28it/s]
resnet50 [stderr]  37%|███▋      | 1532/4096 [00:26<00:39, 64.20it/s]
resnet50 [stderr]  38%|███▊      | 1539/4096 [00:27<00:41, 62.34it/s]
resnet50 [stderr]  38%|███▊      | 1546/4096 [00:27<00:41, 60.97it/s]
resnet50 [stderr]  38%|███▊      | 1554/4096 [00:27<00:39, 64.55it/s]
resnet50 [stderr]  38%|███▊      | 1561/4096 [00:27<00:39, 64.91it/s]
resnet50 [stderr]  38%|███▊      | 1568/4096 [00:27<00:38, 66.01it/s]
resnet50 [stderr]  38%|███▊      | 1575/4096 [00:27<00:39, 63.16it/s]
resnet50 [stderr]  39%|███▊      | 1582/4096 [00:27<00:39, 64.33it/s]
resnet50 [stderr]  39%|███▉      | 1591/4096 [00:27<00:36, 68.45it/s]
resnet50 [stderr]  39%|███▉      | 1598/4096 [00:27<00:38, 65.58it/s]
resnet50 [stderr]  39%|███▉      | 1607/4096 [00:28<00:35, 69.20it/s]
resnet50 [stderr]  39%|███▉      | 1614/4096 [00:28<00:35, 69.36it/s]
resnet50 [stderr]  40%|███▉      | 1621/4096 [00:28<00:38, 64.26it/s]
resnet50 [stderr]  40%|███▉      | 1630/4096 [00:28<00:36, 67.10it/s]
resnet50 [stderr]  40%|███▉      | 1637/4096 [00:28<00:36, 67.16it/s]
resnet50 [stderr]  40%|████      | 1644/4096 [00:28<00:38, 63.11it/s]
resnet50 [stderr]  40%|████      | 1652/4096 [00:28<00:36, 67.59it/s]
resnet50 [stderr]  41%|████      | 1659/4096 [00:28<00:37, 64.46it/s]
resnet50 [stderr]  41%|████      | 1666/4096 [00:28<00:37, 64.80it/s]
resnet50 [stderr]  41%|████      | 1673/4096 [00:29<00:39, 60.87it/s]
resnet50 [stderr]  41%|████      | 1682/4096 [00:29<00:36, 65.81it/s]
resnet50 [stderr]  41%|████▏     | 1690/4096 [00:29<00:35, 67.89it/s]
resnet50 [stderr]  41%|████▏     | 1697/4096 [00:29<00:36, 66.29it/s]
resnet50 [stderr]  42%|████▏     | 1704/4096 [00:29<00:36, 65.20it/s]
resnet50 [stderr]  42%|████▏     | 1711/4096 [00:29<00:36, 65.26it/s]
resnet50 [stderr]  42%|████▏     | 1718/4096 [00:29<00:36, 65.19it/s]
resnet50 [stderr]  42%|████▏     | 1725/4096 [00:29<00:36, 64.85it/s]
resnet50 [stderr]  42%|████▏     | 1734/4096 [00:30<00:35, 67.36it/s]
resnet50 [stderr]  43%|████▎     | 1742/4096 [00:30<00:35, 66.01it/s]
resnet50 [stderr]  43%|████▎     | 1749/4096 [00:30<00:36, 64.11it/s]
resnet50 [stderr]  43%|████▎     | 1759/4096 [00:30<00:34, 67.21it/s]
resnet50 [stderr]  43%|████▎     | 1766/4096 [00:30<00:35, 64.92it/s]
resnet50 [stderr]  43%|████▎     | 1774/4096 [00:30<00:34, 66.66it/s]
resnet50 [stderr]  44%|████▎     | 1782/4096 [00:30<00:34, 68.03it/s]
resnet50 [stderr]  44%|████▎     | 1789/4096 [00:30<00:35, 64.86it/s]
resnet50 [stderr]  44%|████▍     | 1797/4096 [00:30<00:34, 67.44it/s]
resnet50 [stderr]  44%|████▍     | 1804/4096 [00:31<00:35, 65.31it/s]
resnet50 [stderr]  44%|████▍     | 1811/4096 [00:31<00:35, 64.48it/s]
resnet50 [stderr]  44%|████▍     | 1818/4096 [00:31<00:34, 65.19it/s]
resnet50 [stderr]  45%|████▍     | 1827/4096 [00:31<00:34, 65.17it/s]
resnet50 [stderr]  45%|████▍     | 1835/4096 [00:31<00:34, 65.62it/s]
resnet50 [stderr]  45%|████▌     | 1844/4096 [00:31<00:33, 67.10it/s]
resnet50 [stderr]  45%|████▌     | 1852/4096 [00:31<00:32, 69.46it/s]
resnet50 [stderr]  45%|████▌     | 1860/4096 [00:31<00:31, 70.31it/s]
resnet50 [stderr]  46%|████▌     | 1868/4096 [00:32<00:33, 66.22it/s]
resnet50 [stderr]  46%|████▌     | 1877/4096 [00:32<00:31, 71.15it/s]
resnet50 [stderr]  46%|████▌     | 1885/4096 [00:32<00:32, 68.19it/s]
resnet50 [stderr]  46%|████▌     | 1892/4096 [00:32<00:33, 65.25it/s]
resnet50 [stderr]  46%|████▋     | 1901/4096 [00:32<00:31, 70.25it/s]
resnet50 [stderr]  47%|████▋     | 1909/4096 [00:32<00:34, 64.16it/s]
resnet50 [stderr]  47%|████▋     | 1916/4096 [00:32<00:33, 64.12it/s]
resnet50 [stderr]  47%|████▋     | 1925/4096 [00:32<00:31, 68.44it/s]
resnet50 [stderr]  47%|████▋     | 1932/4096 [00:33<00:32, 66.46it/s]
resnet50 [stderr]  47%|████▋     | 1940/4096 [00:33<00:32, 67.07it/s]
resnet50 [stderr]  48%|████▊     | 1948/4096 [00:33<00:31, 67.16it/s]
resnet50 [stderr]  48%|████▊     | 1956/4096 [00:33<00:33, 64.20it/s]
resnet50 [stderr]  48%|████▊     | 1964/4096 [00:33<00:34, 61.33it/s]
resnet50 [stderr]  48%|████▊     | 1973/4096 [00:33<00:31, 66.68it/s]
resnet50 [stderr]  48%|████▊     | 1981/4096 [00:33<00:31, 68.01it/s]
resnet50 [stderr]  49%|████▊     | 1989/4096 [00:33<00:31, 67.62it/s]
resnet50 [stderr]  49%|████▉     | 1997/4096 [00:33<00:30, 69.40it/s]
resnet50 [stderr]  49%|████▉     | 2004/4096 [00:34<00:30, 68.11it/s]
resnet50 [stderr]  49%|████▉     | 2012/4096 [00:34<00:29, 70.32it/s]
resnet50 [stderr]  49%|████▉     | 2020/4096 [00:34<00:33, 62.19it/s]
resnet50 [stderr]  50%|████▉     | 2029/4096 [00:34<00:30, 68.68it/s]
resnet50 [stderr]  50%|████▉     | 2037/4096 [00:34<00:32, 62.69it/s]
resnet50 [stderr]  50%|████▉     | 2047/4096 [00:34<00:29, 68.54it/s]
resnet50 [stderr]  50%|█████     | 2055/4096 [00:34<00:30, 68.00it/s]
resnet50 [stderr]  50%|█████     | 2062/4096 [00:34<00:32, 62.22it/s]
resnet50 [stderr]  51%|█████     | 2071/4096 [00:35<00:29, 69.06it/s]
resnet50 [stderr]  51%|█████     | 2079/4096 [00:35<00:31, 64.46it/s]
resnet50 [stderr]  51%|█████     | 2087/4096 [00:35<00:30, 65.85it/s]
resnet50 [stderr]  51%|█████     | 2094/4096 [00:35<00:30, 66.57it/s]
resnet50 [stderr]  51%|█████▏    | 2101/4096 [00:35<00:30, 64.50it/s]
resnet50 [stderr]  51%|█████▏    | 2108/4096 [00:35<00:30, 65.06it/s]
resnet50 [stderr]  52%|█████▏    | 2116/4096 [00:35<00:28, 68.65it/s]
resnet50 [stderr]  52%|█████▏    | 2123/4096 [00:35<00:29, 66.55it/s]
resnet50 [stderr]  52%|█████▏    | 2130/4096 [00:35<00:29, 66.99it/s]
resnet50 [stderr]  52%|█████▏    | 2137/4096 [00:36<00:30, 64.89it/s]
resnet50 [stderr]  52%|█████▏    | 2144/4096 [00:36<00:30, 63.43it/s]
resnet50 [stderr]  53%|█████▎    | 2151/4096 [00:36<00:32, 59.94it/s]
resnet50 [stderr]  53%|█████▎    | 2160/4096 [00:36<00:31, 61.64it/s]
resnet50 [stderr]  53%|█████▎    | 2167/4096 [00:36<00:30, 62.76it/s]
resnet50 [stderr]  53%|█████▎    | 2174/4096 [00:36<00:32, 58.48it/s]
resnet50 [stderr]  53%|█████▎    | 2183/4096 [00:36<00:28, 66.04it/s]
resnet50 [stderr]  53%|█████▎    | 2191/4096 [00:36<00:28, 66.87it/s]
resnet50 [stderr]  54%|█████▎    | 2198/4096 [00:37<00:28, 66.32it/s]
resnet50 [stderr]  54%|█████▍    | 2207/4096 [00:37<00:27, 68.10it/s]
resnet50 [stderr]  54%|█████▍    | 2214/4096 [00:37<00:29, 64.22it/s]
resnet50 [stderr]  54%|█████▍    | 2223/4096 [00:37<00:28, 65.11it/s]
resnet50 [stderr]  54%|█████▍    | 2230/4096 [00:37<00:29, 62.52it/s]
resnet50 [stderr]  55%|█████▍    | 2238/4096 [00:37<00:27, 66.36it/s]
resnet50 [stderr]  55%|█████▍    | 2245/4096 [00:37<00:27, 67.29it/s]
resnet50 [stderr]  55%|█████▍    | 2252/4096 [00:37<00:29, 63.26it/s]
resnet50 [stderr]  55%|█████▌    | 2260/4096 [00:38<00:27, 66.92it/s]
resnet50 [stderr]  55%|█████▌    | 2268/4096 [00:38<00:28, 65.00it/s]
resnet50 [stderr]  56%|█████▌    | 2276/4096 [00:38<00:27, 65.99it/s]
resnet50 [stderr]  56%|█████▌    | 2284/4096 [00:38<00:27, 66.87it/s]
resnet50 [stderr]  56%|█████▌    | 2291/4096 [00:38<00:28, 64.27it/s]
resnet50 [stderr]  56%|█████▌    | 2299/4096 [00:38<00:27, 66.34it/s]
resnet50 [stderr]  56%|█████▋    | 2306/4096 [00:38<00:28, 62.61it/s]
resnet50 [stderr]  57%|█████▋    | 2315/4096 [00:38<00:26, 67.91it/s]
resnet50 [stderr]  57%|█████▋    | 2322/4096 [00:38<00:27, 64.30it/s]
resnet50 [stderr]  57%|█████▋    | 2329/4096 [00:39<00:28, 61.72it/s]
resnet50 [stderr]  57%|█████▋    | 2336/4096 [00:39<00:28, 62.79it/s]
resnet50 [stderr]  57%|█████▋    | 2343/4096 [00:39<00:27, 64.03it/s]
resnet50 [stderr]  57%|█████▋    | 2351/4096 [00:39<00:26, 66.24it/s]
resnet50 [stderr]  58%|█████▊    | 2358/4096 [00:39<00:28, 61.20it/s]
resnet50 [stderr]  58%|█████▊    | 2365/4096 [00:39<00:27, 63.16it/s]
resnet50 [stderr]  58%|█████▊    | 2373/4096 [00:39<00:26, 64.36it/s]
resnet50 [stderr]  58%|█████▊    | 2381/4096 [00:39<00:26, 63.79it/s]
resnet50 [stderr]  58%|█████▊    | 2389/4096 [00:40<00:27, 62.33it/s]
resnet50 [stderr]  59%|█████▊    | 2397/4096 [00:40<00:26, 64.05it/s]
resnet50 [stderr]  59%|█████▊    | 2405/4096 [00:40<00:25, 65.83it/s]
resnet50 [stderr]  59%|█████▉    | 2413/4096 [00:40<00:24, 69.43it/s]
resnet50 [stderr]  59%|█████▉    | 2421/4096 [00:40<00:23, 70.51it/s]
resnet50 [stderr]  59%|█████▉    | 2429/4096 [00:40<00:24, 69.29it/s]
resnet50 [stderr]  59%|█████▉    | 2437/4096 [00:40<00:23, 70.93it/s]
resnet50 [stderr]  60%|█████▉    | 2445/4096 [00:40<00:24, 66.80it/s]
resnet50 [stderr]  60%|█████▉    | 2452/4096 [00:40<00:25, 64.21it/s]
resnet50 [stderr]  60%|██████    | 2460/4096 [00:41<00:24, 65.62it/s]
resnet50 [stderr]  60%|██████    | 2467/4096 [00:41<00:24, 65.38it/s]
resnet50 [stderr]  60%|██████    | 2474/4096 [00:41<00:24, 65.27it/s]
resnet50 [stderr]  61%|██████    | 2482/4096 [00:41<00:23, 67.82it/s]
resnet50 [stderr]  61%|██████    | 2490/4096 [00:41<00:23, 68.89it/s]
resnet50 [stderr]  61%|██████    | 2497/4096 [00:41<00:23, 68.58it/s]
resnet50 [stderr]  61%|██████    | 2505/4096 [00:41<00:24, 66.19it/s]
resnet50 [stderr]  61%|██████▏   | 2513/4096 [00:41<00:24, 65.67it/s]
resnet50 [stderr]  62%|██████▏   | 2520/4096 [00:41<00:24, 65.42it/s]
resnet50 [stderr]  62%|██████▏   | 2528/4096 [00:42<00:23, 67.20it/s]
resnet50 [stderr]  62%|██████▏   | 2537/4096 [00:42<00:24, 64.45it/s]
resnet50 [stderr]  62%|██████▏   | 2544/4096 [00:42<00:23, 65.62it/s]
resnet50 [stderr]  62%|██████▏   | 2552/4096 [00:42<00:23, 66.69it/s]
resnet50 [stderr]  63%|██████▎   | 2561/4096 [00:42<00:22, 69.46it/s]
resnet50 [stderr]  63%|██████▎   | 2568/4096 [00:42<00:23, 65.41it/s]
resnet50 [stderr]  63%|██████▎   | 2575/4096 [00:42<00:22, 66.38it/s]
resnet50 [stderr]  63%|██████▎   | 2583/4096 [00:42<00:22, 65.80it/s]
resnet50 [stderr]  63%|██████▎   | 2590/4096 [00:43<00:22, 65.96it/s]
resnet50 [stderr]  63%|██████▎   | 2597/4096 [00:43<00:23, 63.16it/s]
resnet50 [stderr]  64%|██████▎   | 2605/4096 [00:43<00:22, 66.96it/s]
resnet50 [stderr]  64%|██████▍   | 2613/4096 [00:43<00:21, 68.32it/s]
resnet50 [stderr]  64%|██████▍   | 2620/4096 [00:43<00:23, 63.78it/s]
resnet50 [stderr]  64%|██████▍   | 2628/4096 [00:43<00:22, 66.07it/s]
resnet50 [stderr]  64%|██████▍   | 2636/4096 [00:43<00:21, 67.86it/s]
resnet50 [stderr]  65%|██████▍   | 2643/4096 [00:43<00:22, 64.26it/s]
resnet50 [stderr]  65%|██████▍   | 2650/4096 [00:43<00:22, 65.56it/s]
resnet50 [stderr]  65%|██████▍   | 2658/4096 [00:44<00:20, 69.05it/s]
resnet50 [stderr]  65%|██████▌   | 2665/4096 [00:44<00:22, 64.90it/s]
resnet50 [stderr]  65%|██████▌   | 2673/4096 [00:44<00:20, 68.30it/s]
resnet50 [stderr]  65%|██████▌   | 2681/4096 [00:44<00:19, 70.79it/s]
resnet50 [stderr]  66%|██████▌   | 2689/4096 [00:44<00:21, 64.81it/s]
resnet50 [stderr]  66%|██████▌   | 2696/4096 [00:44<00:21, 65.48it/s]
resnet50 [stderr]  66%|██████▌   | 2704/4096 [00:44<00:20, 66.32it/s]
resnet50 [stderr]  66%|██████▌   | 2711/4096 [00:44<00:20, 66.37it/s]
resnet50 [stderr]  66%|██████▋   | 2718/4096 [00:44<00:21, 65.21it/s]
resnet50 [stderr]  67%|██████▋   | 2726/4096 [00:45<00:20, 65.90it/s]
resnet50 [stderr]  67%|██████▋   | 2734/4096 [00:45<00:20, 65.46it/s]
resnet50 [stderr]  67%|██████▋   | 2742/4096 [00:45<00:20, 66.28it/s]
resnet50 [stderr]  67%|██████▋   | 2750/4096 [00:45<00:20, 66.39it/s]
resnet50 [stderr]  67%|██████▋   | 2757/4096 [00:45<00:20, 65.95it/s]
resnet50 [stderr]  68%|██████▊   | 2766/4096 [00:45<00:19, 67.31it/s]
resnet50 [stderr]  68%|██████▊   | 2773/4096 [00:45<00:19, 67.36it/s]
resnet50 [stderr]  68%|██████▊   | 2780/4096 [00:45<00:20, 63.43it/s]
resnet50 [stderr]  68%|██████▊   | 2787/4096 [00:46<00:20, 64.98it/s]
resnet50 [stderr]  68%|██████▊   | 2794/4096 [00:46<00:19, 65.88it/s]
resnet50 [stderr]  68%|██████▊   | 2802/4096 [00:46<00:19, 65.62it/s]
resnet50 [stderr]  69%|██████▊   | 2809/4096 [00:46<00:19, 66.37it/s]
resnet50 [stderr]  69%|██████▉   | 2818/4096 [00:46<00:19, 66.53it/s]
resnet50 [stderr]  69%|██████▉   | 2825/4096 [00:46<00:19, 63.84it/s]
resnet50 [stderr]  69%|██████▉   | 2832/4096 [00:46<00:19, 65.45it/s]
resnet50 [stderr]  69%|██████▉   | 2841/4096 [00:46<00:19, 65.95it/s]
resnet50 [stderr]  70%|██████▉   | 2848/4096 [00:46<00:19, 64.94it/s]
resnet50 [stderr]  70%|██████▉   | 2856/4096 [00:47<00:18, 66.60it/s]
resnet50 [stderr]  70%|██████▉   | 2863/4096 [00:47<00:19, 64.73it/s]
resnet50 [stderr]  70%|███████   | 2870/4096 [00:47<00:19, 64.08it/s]
resnet50 [stderr]  70%|███████   | 2878/4096 [00:47<00:18, 66.24it/s]
resnet50 [stderr]  70%|███████   | 2885/4096 [00:47<00:18, 66.04it/s]
resnet50 [stderr]  71%|███████   | 2892/4096 [00:47<00:18, 66.43it/s]
resnet50 [stderr]  71%|███████   | 2899/4096 [00:47<00:18, 64.99it/s]
resnet50 [stderr]  71%|███████   | 2906/4096 [00:47<00:18, 63.43it/s]
resnet50 [stderr]  71%|███████   | 2913/4096 [00:47<00:18, 64.17it/s]
resnet50 [stderr]  71%|███████▏  | 2920/4096 [00:48<00:18, 64.69it/s]
resnet50 [stderr]  71%|███████▏  | 2928/4096 [00:48<00:17, 66.02it/s]
resnet50 [stderr]  72%|███████▏  | 2935/4096 [00:48<00:17, 66.29it/s]
resnet50 [stderr]  72%|███████▏  | 2942/4096 [00:48<00:17, 67.31it/s]
resnet50 [stderr]  72%|███████▏  | 2949/4096 [00:48<00:18, 62.77it/s]
resnet50 [stderr]  72%|███████▏  | 2956/4096 [00:48<00:19, 59.19it/s]
resnet50 [stderr]  72%|███████▏  | 2964/4096 [00:48<00:18, 62.49it/s]
resnet50 [stderr]  73%|███████▎  | 2971/4096 [00:48<00:18, 60.44it/s]
resnet50 [stderr]  73%|███████▎  | 2978/4096 [00:48<00:18, 59.18it/s]
resnet50 [stderr]  73%|███████▎  | 2986/4096 [00:49<00:18, 61.62it/s]
resnet50 [stderr]  73%|███████▎  | 2993/4096 [00:49<00:18, 60.64it/s]
resnet50 [stderr]  73%|███████▎  | 3001/4096 [00:49<00:17, 64.38it/s]
resnet50 [stderr]  73%|███████▎  | 3008/4096 [00:49<00:17, 63.61it/s]
resnet50 [stderr]  74%|███████▎  | 3015/4096 [00:49<00:16, 64.14it/s]
resnet50 [stderr]  74%|███████▍  | 3022/4096 [00:49<00:17, 62.80it/s]
resnet50 [stderr]  74%|███████▍  | 3029/4096 [00:49<00:17, 62.22it/s]
resnet50 [stderr]  74%|███████▍  | 3036/4096 [00:49<00:17, 62.32it/s]
resnet50 [stderr]  74%|███████▍  | 3044/4096 [00:50<00:15, 65.77it/s]
resnet50 [stderr]  74%|███████▍  | 3051/4096 [00:50<00:16, 62.54it/s]
resnet50 [stderr]  75%|███████▍  | 3058/4096 [00:50<00:16, 61.88it/s]
resnet50 [stderr]  75%|███████▍  | 3066/4096 [00:50<00:15, 66.20it/s]
resnet50 [stderr]  75%|███████▌  | 3074/4096 [00:50<00:15, 65.03it/s]
resnet50 [stderr]  75%|███████▌  | 3082/4096 [00:50<00:15, 64.45it/s]
resnet50 [stderr]  75%|███████▌  | 3090/4096 [00:50<00:15, 64.91it/s]
resnet50 [stderr]  76%|███████▌  | 3097/4096 [00:50<00:15, 64.75it/s]
resnet50 [stderr]  76%|███████▌  | 3106/4096 [00:50<00:15, 65.80it/s]
resnet50 [stderr]  76%|███████▌  | 3113/4096 [00:51<00:15, 64.95it/s]
resnet50 [stderr]  76%|███████▌  | 3120/4096 [00:51<00:15, 62.35it/s]
resnet50 [stderr]  76%|███████▋  | 3129/4096 [00:51<00:14, 65.22it/s]
resnet50 [stderr]  77%|███████▋  | 3137/4096 [00:51<00:14, 66.00it/s]
resnet50 [stderr]  77%|███████▋  | 3145/4096 [00:51<00:14, 67.58it/s]
resnet50 [stderr]  77%|███████▋  | 3153/4096 [00:51<00:14, 65.97it/s]
resnet50 [stderr]  77%|███████▋  | 3160/4096 [00:51<00:15, 61.83it/s]
resnet50 [stderr]  77%|███████▋  | 3168/4096 [00:51<00:14, 66.28it/s]
resnet50 [stderr]  78%|███████▊  | 3175/4096 [00:52<00:14, 64.06it/s]
resnet50 [stderr]  78%|███████▊  | 3182/4096 [00:52<00:13, 65.42it/s]
resnet50 [stderr]  78%|███████▊  | 3189/4096 [00:52<00:13, 65.59it/s]
resnet50 [stderr]  78%|███████▊  | 3196/4096 [00:52<00:13, 65.85it/s]
resnet50 [stderr]  78%|███████▊  | 3204/4096 [00:52<00:12, 69.19it/s]
resnet50 [stderr]  78%|███████▊  | 3211/4096 [00:52<00:13, 65.92it/s]
resnet50 [stderr]  79%|███████▊  | 3218/4096 [00:52<00:13, 65.83it/s]
resnet50 [stderr]  79%|███████▊  | 3225/4096 [00:52<00:13, 62.33it/s]
resnet50 [stderr]  79%|███████▉  | 3232/4096 [00:52<00:13, 63.70it/s]
resnet50 [stderr]  79%|███████▉  | 3240/4096 [00:53<00:13, 65.33it/s]
resnet50 [stderr]  79%|███████▉  | 3248/4096 [00:53<00:12, 66.41it/s]
resnet50 [stderr]  79%|███████▉  | 3255/4096 [00:53<00:13, 63.95it/s]
resnet50 [stderr]  80%|███████▉  | 3263/4096 [00:53<00:12, 64.55it/s]
resnet50 [stderr]  80%|███████▉  | 3271/4096 [00:53<00:12, 65.87it/s]
resnet50 [stderr]  80%|████████  | 3278/4096 [00:53<00:12, 65.57it/s]
resnet50 [stderr]  80%|████████  | 3287/4096 [00:53<00:12, 66.86it/s]
resnet50 [stderr]  80%|████████  | 3295/4096 [00:53<00:12, 63.22it/s]
resnet50 [stderr]  81%|████████  | 3304/4096 [00:54<00:12, 64.36it/s]
resnet50 [stderr]  81%|████████  | 3312/4096 [00:54<00:11, 66.03it/s]
resnet50 [stderr]  81%|████████  | 3319/4096 [00:54<00:12, 64.44it/s]
resnet50 [stderr]  81%|████████  | 3326/4096 [00:54<00:11, 64.54it/s]
resnet50 [stderr]  81%|████████▏ | 3334/4096 [00:54<00:11, 67.24it/s]
resnet50 [stderr]  82%|████████▏ | 3341/4096 [00:54<00:11, 65.38it/s]
resnet50 [stderr]  82%|████████▏ | 3349/4096 [00:54<00:11, 67.38it/s]
resnet50 [stderr]  82%|████████▏ | 3356/4096 [00:54<00:11, 63.90it/s]
resnet50 [stderr]  82%|████████▏ | 3364/4096 [00:54<00:11, 64.43it/s]
resnet50 [stderr]  82%|████████▏ | 3372/4096 [00:55<00:11, 65.00it/s]
resnet50 [stderr]  82%|████████▏ | 3379/4096 [00:55<00:11, 64.95it/s]
resnet50 [stderr]  83%|████████▎ | 3386/4096 [00:55<00:10, 64.64it/s]
resnet50 [stderr]  83%|████████▎ | 3395/4096 [00:55<00:10, 69.12it/s]
resnet50 [stderr]  83%|████████▎ | 3402/4096 [00:55<00:10, 66.44it/s]
resnet50 [stderr]  83%|████████▎ | 3409/4096 [00:55<00:10, 64.13it/s]
resnet50 [stderr]  83%|████████▎ | 3418/4096 [00:55<00:09, 70.89it/s]
resnet50 [stderr]  84%|████████▎ | 3426/4096 [00:55<00:10, 66.73it/s]
resnet50 [stderr]  84%|████████▍ | 3433/4096 [00:55<00:10, 65.50it/s]
resnet50 [stderr]  84%|████████▍ | 3440/4096 [00:56<00:10, 62.48it/s]
resnet50 [stderr]  84%|████████▍ | 3448/4096 [00:56<00:09, 67.11it/s]
resnet50 [stderr]  84%|████████▍ | 3455/4096 [00:56<00:09, 67.16it/s]
resnet50 [stderr]  85%|████████▍ | 3462/4096 [00:56<00:09, 64.55it/s]
resnet50 [stderr]  85%|████████▍ | 3469/4096 [00:56<00:09, 65.13it/s]
resnet50 [stderr]  85%|████████▍ | 3476/4096 [00:56<00:09, 64.33it/s]
resnet50 [stderr]  85%|████████▌ | 3483/4096 [00:56<00:09, 64.48it/s]
resnet50 [stderr]  85%|████████▌ | 3490/4096 [00:56<00:09, 61.63it/s]
resnet50 [stderr]  85%|████████▌ | 3497/4096 [00:57<00:10, 59.19it/s]
resnet50 [stderr]  86%|████████▌ | 3505/4096 [00:57<00:09, 64.49it/s]
resnet50 [stderr]  86%|████████▌ | 3512/4096 [00:57<00:09, 60.87it/s]
resnet50 [stderr]  86%|████████▌ | 3519/4096 [00:57<00:09, 62.64it/s]
resnet50 [stderr]  86%|████████▌ | 3527/4096 [00:57<00:09, 62.74it/s]
resnet50 [stderr]  86%|████████▋ | 3535/4096 [00:57<00:08, 65.98it/s]
resnet50 [stderr]  86%|████████▋ | 3542/4096 [00:57<00:08, 66.62it/s]
resnet50 [stderr]  87%|████████▋ | 3549/4096 [00:57<00:08, 67.07it/s]
resnet50 [stderr]  87%|████████▋ | 3556/4096 [00:57<00:08, 63.35it/s]
resnet50 [stderr]  87%|████████▋ | 3564/4096 [00:58<00:08, 66.00it/s]
resnet50 [stderr]  87%|████████▋ | 3571/4096 [00:58<00:08, 63.36it/s]
resnet50 [stderr]  87%|████████▋ | 3579/4096 [00:58<00:08, 64.60it/s]
resnet50 [stderr]  88%|████████▊ | 3586/4096 [00:58<00:07, 65.33it/s]
resnet50 [stderr]  88%|████████▊ | 3593/4096 [00:58<00:07, 66.11it/s]
resnet50 [stderr]  88%|████████▊ | 3600/4096 [00:58<00:07, 64.82it/s]
resnet50 [stderr]  88%|████████▊ | 3607/4096 [00:58<00:07, 64.58it/s]
resnet50 [stderr]  88%|████████▊ | 3615/4096 [00:58<00:07, 68.31it/s]
resnet50 [stderr]  88%|████████▊ | 3622/4096 [00:58<00:07, 63.57it/s]
resnet50 [stderr]  89%|████████▊ | 3629/4096 [00:59<00:07, 61.94it/s]
resnet50 [stderr]  89%|████████▉ | 3638/4096 [00:59<00:07, 64.63it/s]
resnet50 [stderr]  89%|████████▉ | 3646/4096 [00:59<00:06, 67.99it/s]
resnet50 [stderr]  89%|████████▉ | 3653/4096 [00:59<00:07, 62.82it/s]
resnet50 [stderr]  89%|████████▉ | 3660/4096 [00:59<00:07, 61.87it/s]
resnet50 [stderr]  90%|████████▉ | 3669/4096 [00:59<00:06, 63.31it/s]
resnet50 [stderr]  90%|████████▉ | 3678/4096 [00:59<00:06, 66.73it/s]
resnet50 [stderr]  90%|████████▉ | 3685/4096 [00:59<00:06, 65.55it/s]
resnet50 [stderr]  90%|█████████ | 3694/4096 [01:00<00:05, 67.71it/s]
resnet50 [stderr]  90%|█████████ | 3702/4096 [01:00<00:05, 70.18it/s]
resnet50 [stderr]  91%|█████████ | 3710/4096 [01:00<00:05, 66.36it/s]
resnet50 [stderr]  91%|█████████ | 3717/4096 [01:00<00:05, 63.47it/s]
resnet50 [stderr]  91%|█████████ | 3724/4096 [01:00<00:05, 62.47it/s]
resnet50 [stderr]  91%|█████████ | 3732/4096 [01:00<00:05, 66.83it/s]
resnet50 [stderr]  91%|█████████▏| 3739/4096 [01:00<00:05, 60.46it/s]
resnet50 [stderr]  92%|█████████▏| 3748/4096 [01:00<00:05, 65.60it/s]
resnet50 [stderr]  92%|█████████▏| 3755/4096 [01:00<00:05, 62.59it/s]
resnet50 [stderr]  92%|█████████▏| 3764/4096 [01:01<00:05, 64.98it/s]
resnet50 [stderr]  92%|█████████▏| 3771/4096 [01:01<00:04, 65.35it/s]
resnet50 [stderr]  92%|█████████▏| 3779/4096 [01:01<00:04, 64.05it/s]
resnet50 [stderr]  92%|█████████▏| 3787/4096 [01:01<00:04, 62.16it/s]
resnet50 [stderr]  93%|█████████▎| 3796/4096 [01:01<00:04, 67.56it/s]
resnet50 [stderr]  93%|█████████▎| 3803/4096 [01:01<00:04, 64.43it/s]
resnet50 [stderr]  93%|█████████▎| 3811/4096 [01:01<00:04, 63.64it/s]
resnet50 [stderr]  93%|█████████▎| 3819/4096 [01:01<00:04, 65.82it/s]
resnet50 [stderr]  93%|█████████▎| 3827/4096 [01:02<00:04, 65.19it/s]
resnet50 [stderr]  94%|█████████▎| 3835/4096 [01:02<00:04, 65.20it/s]
resnet50 [stderr]  94%|█████████▍| 3843/4096 [01:02<00:03, 66.96it/s]
resnet50 [stderr]  94%|█████████▍| 3852/4096 [01:02<00:03, 66.07it/s]
resnet50 [stderr]  94%|█████████▍| 3859/4096 [01:02<00:03, 65.51it/s]
resnet50 [stderr]  94%|█████████▍| 3866/4096 [01:02<00:03, 65.32it/s]
resnet50 [stderr]  95%|█████████▍| 3875/4096 [01:02<00:03, 69.22it/s]
resnet50 [stderr]  95%|█████████▍| 3882/4096 [01:02<00:03, 67.97it/s]
resnet50 [stderr]  95%|█████████▍| 3890/4096 [01:03<00:03, 67.55it/s]
resnet50 [stderr]  95%|█████████▌| 3897/4096 [01:03<00:03, 65.37it/s]
resnet50 [stderr]  95%|█████████▌| 3904/4096 [01:03<00:03, 63.20it/s]
resnet50 [stderr]  96%|█████████▌| 3912/4096 [01:03<00:02, 64.95it/s]
resnet50 [stderr]  96%|█████████▌| 3919/4096 [01:03<00:02, 63.58it/s]
resnet50 [stderr]  96%|█████████▌| 3926/4096 [01:03<00:02, 65.27it/s]
resnet50 [stderr]  96%|█████████▌| 3933/4096 [01:03<00:02, 65.88it/s]
resnet50 [stderr]  96%|█████████▌| 3940/4096 [01:03<00:02, 64.86it/s]
resnet50 [stderr]  96%|█████████▋| 3947/4096 [01:03<00:02, 63.44it/s]
resnet50 [stderr]  97%|█████████▋| 3955/4096 [01:04<00:02, 62.28it/s]
resnet50 [stderr]  97%|█████████▋| 3963/4096 [01:04<00:02, 63.61it/s]
resnet50 [stderr]  97%|█████████▋| 3971/4096 [01:04<00:01, 63.25it/s]
resnet50 [stderr]  97%|█████████▋| 3979/4096 [01:04<00:01, 66.24it/s]
resnet50 [stderr]  97%|█████████▋| 3987/4096 [01:04<00:01, 69.91it/s]
resnet50 [stderr]  98%|█████████▊| 3995/4096 [01:04<00:01, 65.24it/s]
resnet50 [stderr]  98%|█████████▊| 4003/4096 [01:04<00:01, 68.21it/s]
resnet50 [stderr]  98%|█████████▊| 4011/4096 [01:04<00:01, 69.10it/s]
resnet50 [stderr]  98%|█████████▊| 4018/4096 [01:04<00:01, 67.15it/s]
resnet50 [stderr]  98%|█████████▊| 4025/4096 [01:05<00:01, 66.96it/s]
resnet50 [stderr]  98%|█████████▊| 4032/4096 [01:05<00:00, 66.33it/s]
resnet50 [stderr]  99%|█████████▊| 4039/4096 [01:05<00:00, 65.41it/s]
resnet50 [stderr]  99%|█████████▉| 4047/4096 [01:05<00:00, 69.23it/s]
resnet50 [stderr]  99%|█████████▉| 4054/4096 [01:05<00:00, 67.91it/s]
resnet50 [stderr]  99%|█████████▉| 4061/4096 [01:05<00:00, 62.21it/s]
resnet50 [stderr]  99%|█████████▉| 4070/4096 [01:05<00:00, 68.40it/s]
resnet50 [stderr] 100%|█████████▉| 4077/4096 [01:05<00:00, 65.26it/s]
resnet50 [stderr] 100%|█████████▉| 4084/4096 [01:05<00:00, 63.98it/s]
resnet50 [stderr] 100%|█████████▉| 4092/4096 [01:06<00:00, 64.38it/s]
resnet50 [stderr] 100%|██████████| 4096/4096 [01:06<00:00, 61.96it/s]
resnet50 [stdout] Generating val
resnet50 [stderr]   0%|          | 0/16 [00:00<?, ?it/s]
resnet50 [stderr]   6%|▋         | 1/16 [00:03<00:45,  3.01s/it]
resnet50 [stderr]  19%|█▉        | 3/16 [00:03<00:10,  1.20it/s]
resnet50 [stderr]  44%|████▍     | 7/16 [00:03<00:02,  3.46it/s]
resnet50 [stderr]  75%|███████▌  | 12/16 [00:03<00:00,  6.93it/s]
resnet50 [stderr]  94%|█████████▍| 15/16 [00:03<00:00,  7.77it/s]
resnet50 [stderr] 100%|██████████| 16/16 [00:03<00:00,  4.33it/s]
resnet50 [stdout] Generating test
resnet50 [stderr]   0%|          | 0/16 [00:00<?, ?it/s]
resnet50 [stderr]   6%|▋         | 1/16 [00:03<00:47,  3.18s/it]
resnet50 [stderr]  25%|██▌       | 4/16 [00:03<00:07,  1.57it/s]
resnet50 [stderr]  44%|████▍     | 7/16 [00:03<00:02,  3.18it/s]
resnet50 [stderr]  81%|████████▏ | 13/16 [00:03<00:00,  7.19it/s]
resnet50 [stderr] 100%|██████████| 16/16 [00:03<00:00,  4.38it/s]
resnet50 [stderr] 
resnet50 [stdout] Done!
resnet50 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model resnet50 --batch-size 64 [at 2024-02-06 11:58:42.211994]
convnext_large-fp32 [config.system.arch] cuda
convnext_large-fp32 [config.system.sshkey] None
convnext_large-fp32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
convnext_large-fp32 [config.system.gpu.capacity] 81920 MiB
convnext_large-fp32 [config.system.self.name] local
convnext_large-fp32 [config.system.self.ip] 127.0.0.1
convnext_large-fp32 [config.system.self.port] 8123
convnext_large-fp32 [config.system.self.user] root
convnext_large-fp32 [config.system.self.main] True
convnext_large-fp32 [config.system.self.hostname] localhost
convnext_large-fp32 [config.system.self.aliaslist] []
convnext_large-fp32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
convnext_large-fp32 [config.system.self.local] True
convnext_large-fp32 [config.dirs.base] /Tmp/slurm.4115007.0/base
convnext_large-fp32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
convnext_large-fp32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
convnext_large-fp32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
convnext_large-fp32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
convnext_large-fp32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
convnext_large-fp32 [config.group] torchvision
convnext_large-fp32 [config.install_group] torch
convnext_large-fp32 [config.install_variant] cuda
convnext_large-fp32 [config.run_name] prepare.2024-02-06_11:57:01.236402
convnext_large-fp32 [config.enabled] True
convnext_large-fp32 [config.capabilities.nodes] 1
convnext_large-fp32 [config.max_duration] 600
convnext_large-fp32 [config.voir.options.stop] 30
convnext_large-fp32 [config.voir.options.interval] 1s
convnext_large-fp32 [config.validation.usage.gpu_load_threshold] 0.5
convnext_large-fp32 [config.validation.usage.gpu_mem_threshold] 0.5
convnext_large-fp32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
convnext_large-fp32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
convnext_large-fp32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
convnext_large-fp32 [config.plan.method] per_gpu
convnext_large-fp32 [config.argv.--precision] fp32
convnext_large-fp32 [config.argv.--lr] 0.01
convnext_large-fp32 [config.argv.--no-stdout] True
convnext_large-fp32 [config.argv.--epochs] 50
convnext_large-fp32 [config.argv.--model] convnext_large
convnext_large-fp32 [config.argv.--batch-size] 128
convnext_large-fp32 [config.tags] ['classification', 'convnet', 'precision-showcase', 'vision']
convnext_large-fp32 [config.weight] 0.0
convnext_large-fp32 [config.name] convnext_large-fp32
convnext_large-fp32 [config.tag] ['convnext_large-fp32']
convnext_large-fp32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.827,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256724.61334,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
convnext_large-fp32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision fp32 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:44.633712]
convnext_large-fp32 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
convnext_large-fp32 [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
convnext_large-fp32 [stdout] Done!
convnext_large-fp32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision fp32 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:44.667601]
convnext_large-fp16 [config.system.arch] cuda
convnext_large-fp16 [config.system.sshkey] None
convnext_large-fp16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
convnext_large-fp16 [config.system.gpu.capacity] 81920 MiB
convnext_large-fp16 [config.system.self.name] local
convnext_large-fp16 [config.system.self.ip] 127.0.0.1
convnext_large-fp16 [config.system.self.port] 8123
convnext_large-fp16 [config.system.self.user] root
convnext_large-fp16 [config.system.self.main] True
convnext_large-fp16 [config.system.self.hostname] localhost
convnext_large-fp16 [config.system.self.aliaslist] []
convnext_large-fp16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
convnext_large-fp16 [config.system.self.local] True
convnext_large-fp16 [config.dirs.base] /Tmp/slurm.4115007.0/base
convnext_large-fp16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
convnext_large-fp16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
convnext_large-fp16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
convnext_large-fp16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
convnext_large-fp16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
convnext_large-fp16 [config.group] torchvision
convnext_large-fp16 [config.install_group] torch
convnext_large-fp16 [config.install_variant] cuda
convnext_large-fp16 [config.run_name] prepare.2024-02-06_11:57:01.236402
convnext_large-fp16 [config.enabled] True
convnext_large-fp16 [config.capabilities.nodes] 1
convnext_large-fp16 [config.max_duration] 600
convnext_large-fp16 [config.voir.options.stop] 30
convnext_large-fp16 [config.voir.options.interval] 1s
convnext_large-fp16 [config.validation.usage.gpu_load_threshold] 0.5
convnext_large-fp16 [config.validation.usage.gpu_mem_threshold] 0.5
convnext_large-fp16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
convnext_large-fp16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
convnext_large-fp16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
convnext_large-fp16 [config.plan.method] per_gpu
convnext_large-fp16 [config.argv.--precision] fp16
convnext_large-fp16 [config.argv.--lr] 0.01
convnext_large-fp16 [config.argv.--no-stdout] True
convnext_large-fp16 [config.argv.--epochs] 50
convnext_large-fp16 [config.argv.--model] convnext_large
convnext_large-fp16 [config.argv.--batch-size] 128
convnext_large-fp16 [config.tags] ['classification', 'convnet', 'precision-showcase', 'vision']
convnext_large-fp16 [config.weight] 0.0
convnext_large-fp16 [config.name] convnext_large-fp16
convnext_large-fp16 [config.tag] ['convnext_large-fp16']
convnext_large-fp16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.827,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256727.106197,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
convnext_large-fp16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision fp16 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:47.124932]
convnext_large-fp16 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
convnext_large-fp16 [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
convnext_large-fp16 [stdout] Done!
convnext_large-fp16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision fp16 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:47.159392]
convnext_large-tf32 [config.system.arch] cuda
convnext_large-tf32 [config.system.sshkey] None
convnext_large-tf32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
convnext_large-tf32 [config.system.gpu.capacity] 81920 MiB
convnext_large-tf32 [config.system.self.name] local
convnext_large-tf32 [config.system.self.ip] 127.0.0.1
convnext_large-tf32 [config.system.self.port] 8123
convnext_large-tf32 [config.system.self.user] root
convnext_large-tf32 [config.system.self.main] True
convnext_large-tf32 [config.system.self.hostname] localhost
convnext_large-tf32 [config.system.self.aliaslist] []
convnext_large-tf32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
convnext_large-tf32 [config.system.self.local] True
convnext_large-tf32 [config.dirs.base] /Tmp/slurm.4115007.0/base
convnext_large-tf32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
convnext_large-tf32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
convnext_large-tf32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
convnext_large-tf32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
convnext_large-tf32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
convnext_large-tf32 [config.group] torchvision
convnext_large-tf32 [config.install_group] torch
convnext_large-tf32 [config.install_variant] cuda
convnext_large-tf32 [config.run_name] prepare.2024-02-06_11:57:01.236402
convnext_large-tf32 [config.enabled] True
convnext_large-tf32 [config.capabilities.nodes] 1
convnext_large-tf32 [config.max_duration] 600
convnext_large-tf32 [config.voir.options.stop] 30
convnext_large-tf32 [config.voir.options.interval] 1s
convnext_large-tf32 [config.validation.usage.gpu_load_threshold] 0.5
convnext_large-tf32 [config.validation.usage.gpu_mem_threshold] 0.5
convnext_large-tf32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
convnext_large-tf32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
convnext_large-tf32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
convnext_large-tf32 [config.plan.method] per_gpu
convnext_large-tf32 [config.argv.--precision] tf32
convnext_large-tf32 [config.argv.--lr] 0.01
convnext_large-tf32 [config.argv.--no-stdout] True
convnext_large-tf32 [config.argv.--epochs] 50
convnext_large-tf32 [config.argv.--model] convnext_large
convnext_large-tf32 [config.argv.--batch-size] 128
convnext_large-tf32 [config.tags] ['classification', 'convnet', 'precision-showcase', 'vision']
convnext_large-tf32 [config.weight] 0.0
convnext_large-tf32 [config.name] convnext_large-tf32
convnext_large-tf32 [config.tag] ['convnext_large-tf32']
convnext_large-tf32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.827,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256729.55149,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
convnext_large-tf32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:49.569763]
convnext_large-tf32 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
convnext_large-tf32 [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
convnext_large-tf32 [stdout] Done!
convnext_large-tf32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:49.604119]
convnext_large-tf32-fp16 [config.system.arch] cuda
convnext_large-tf32-fp16 [config.system.sshkey] None
convnext_large-tf32-fp16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
convnext_large-tf32-fp16 [config.system.gpu.capacity] 81920 MiB
convnext_large-tf32-fp16 [config.system.self.name] local
convnext_large-tf32-fp16 [config.system.self.ip] 127.0.0.1
convnext_large-tf32-fp16 [config.system.self.port] 8123
convnext_large-tf32-fp16 [config.system.self.user] root
convnext_large-tf32-fp16 [config.system.self.main] True
convnext_large-tf32-fp16 [config.system.self.hostname] localhost
convnext_large-tf32-fp16 [config.system.self.aliaslist] []
convnext_large-tf32-fp16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
convnext_large-tf32-fp16 [config.system.self.local] True
convnext_large-tf32-fp16 [config.dirs.base] /Tmp/slurm.4115007.0/base
convnext_large-tf32-fp16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
convnext_large-tf32-fp16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
convnext_large-tf32-fp16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
convnext_large-tf32-fp16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
convnext_large-tf32-fp16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
convnext_large-tf32-fp16 [config.group] torchvision
convnext_large-tf32-fp16 [config.install_group] torch
convnext_large-tf32-fp16 [config.install_variant] cuda
convnext_large-tf32-fp16 [config.run_name] prepare.2024-02-06_11:57:01.236402
convnext_large-tf32-fp16 [config.enabled] True
convnext_large-tf32-fp16 [config.capabilities.nodes] 1
convnext_large-tf32-fp16 [config.max_duration] 600
convnext_large-tf32-fp16 [config.voir.options.stop] 30
convnext_large-tf32-fp16 [config.voir.options.interval] 1s
convnext_large-tf32-fp16 [config.validation.usage.gpu_load_threshold] 0.5
convnext_large-tf32-fp16 [config.validation.usage.gpu_mem_threshold] 0.5
convnext_large-tf32-fp16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
convnext_large-tf32-fp16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
convnext_large-tf32-fp16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
convnext_large-tf32-fp16 [config.plan.method] per_gpu
convnext_large-tf32-fp16 [config.argv.--precision] tf32-fp16
convnext_large-tf32-fp16 [config.argv.--lr] 0.01
convnext_large-tf32-fp16 [config.argv.--no-stdout] True
convnext_large-tf32-fp16 [config.argv.--epochs] 50
convnext_large-tf32-fp16 [config.argv.--model] convnext_large
convnext_large-tf32-fp16 [config.argv.--batch-size] 128
convnext_large-tf32-fp16 [config.tags] ['classification', 'convnet', 'precision-showcase', 'vision']
convnext_large-tf32-fp16 [config.weight] 3.0
convnext_large-tf32-fp16 [config.name] convnext_large-tf32-fp16
convnext_large-tf32-fp16 [config.tag] ['convnext_large-tf32-fp16']
convnext_large-tf32-fp16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.827,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256731.988304,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
convnext_large-tf32-fp16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:52.006887]
convnext_large-tf32-fp16 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
convnext_large-tf32-fp16 [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
convnext_large-tf32-fp16 [stdout] Done!
convnext_large-tf32-fp16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model convnext_large --batch-size 128 [at 2024-02-06 11:58:52.041427]
regnet_y_128gf [config.system.arch] cuda
regnet_y_128gf [config.system.sshkey] None
regnet_y_128gf [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
regnet_y_128gf [config.system.gpu.capacity] 81920 MiB
regnet_y_128gf [config.system.self.name] local
regnet_y_128gf [config.system.self.ip] 127.0.0.1
regnet_y_128gf [config.system.self.port] 8123
regnet_y_128gf [config.system.self.user] root
regnet_y_128gf [config.system.self.main] True
regnet_y_128gf [config.system.self.hostname] localhost
regnet_y_128gf [config.system.self.aliaslist] []
regnet_y_128gf [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
regnet_y_128gf [config.system.self.local] True
regnet_y_128gf [config.dirs.base] /Tmp/slurm.4115007.0/base
regnet_y_128gf [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
regnet_y_128gf [config.dirs.data] /Tmp/slurm.4115007.0/base/data
regnet_y_128gf [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
regnet_y_128gf [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/torchvision
regnet_y_128gf [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
regnet_y_128gf [config.group] torchvision
regnet_y_128gf [config.install_group] torch
regnet_y_128gf [config.install_variant] cuda
regnet_y_128gf [config.run_name] prepare.2024-02-06_11:57:01.236402
regnet_y_128gf [config.enabled] True
regnet_y_128gf [config.capabilities.nodes] 1
regnet_y_128gf [config.max_duration] 600
regnet_y_128gf [config.voir.options.stop] 60
regnet_y_128gf [config.voir.options.interval] 1s
regnet_y_128gf [config.validation.usage.gpu_load_threshold] 0.5
regnet_y_128gf [config.validation.usage.gpu_mem_threshold] 0.5
regnet_y_128gf [config.config_base] /Tmp/slurm.4115007.0/milabench/config
regnet_y_128gf [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
regnet_y_128gf [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision
regnet_y_128gf [config.plan.method] per_gpu
regnet_y_128gf [config.argv.--precision] tf32-fp16
regnet_y_128gf [config.argv.--lr] 0.01
regnet_y_128gf [config.argv.--no-stdout] True
regnet_y_128gf [config.argv.--epochs] 50
regnet_y_128gf [config.argv.--model] regnet_y_128gf
regnet_y_128gf [config.argv.--batch-size] 64
regnet_y_128gf [config.tags] ['classification', 'convnet', 'lstm', 'resnet', 'vision']
regnet_y_128gf [config.weight] 2.0
regnet_y_128gf [config.name] regnet_y_128gf
regnet_y_128gf [config.tag] ['regnet_y_128gf']
regnet_y_128gf [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.068,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 26,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256734.405609,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
regnet_y_128gf [start] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model regnet_y_128gf --batch-size 64 [at 2024-02-06 11:58:54.424188]
regnet_y_128gf [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
regnet_y_128gf [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
regnet_y_128gf [stdout] Done!
regnet_y_128gf [end] /Tmp/slurm.4115007.0/milabench/benchmarks/torchvision/prepare.py --precision tf32-fp16 --lr 0.01 --no-stdout --epochs 50 --model regnet_y_128gf --batch-size 64 [at 2024-02-06 11:58:54.458453]
bert-fp32 [config.system.arch] cuda
bert-fp32 [config.system.sshkey] None
bert-fp32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
bert-fp32 [config.system.gpu.capacity] 81920 MiB
bert-fp32 [config.system.self.name] local
bert-fp32 [config.system.self.ip] 127.0.0.1
bert-fp32 [config.system.self.port] 8123
bert-fp32 [config.system.self.user] root
bert-fp32 [config.system.self.main] True
bert-fp32 [config.system.self.hostname] localhost
bert-fp32 [config.system.self.aliaslist] []
bert-fp32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
bert-fp32 [config.system.self.local] True
bert-fp32 [config.dirs.base] /Tmp/slurm.4115007.0/base
bert-fp32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
bert-fp32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
bert-fp32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
bert-fp32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
bert-fp32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
bert-fp32 [config.group] hf
bert-fp32 [config.install_group] torch
bert-fp32 [config.install_variant] cuda
bert-fp32 [config.run_name] prepare.2024-02-06_11:57:01.236402
bert-fp32 [config.enabled] True
bert-fp32 [config.capabilities.nodes] 1
bert-fp32 [config.max_duration] 600
bert-fp32 [config.voir.options.stop] 30
bert-fp32 [config.voir.options.interval] 1s
bert-fp32 [config.validation.usage.gpu_load_threshold] 0.5
bert-fp32 [config.validation.usage.gpu_mem_threshold] 0.5
bert-fp32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
bert-fp32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
bert-fp32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
bert-fp32 [config.argv.--precision] fp32
bert-fp32 [config.argv.--num-workers] 8
bert-fp32 [config.argv.--model] Bert
bert-fp32 [config.argv.--batch-size] 32
bert-fp32 [config.plan.method] per_gpu
bert-fp32 [config.tags] ['huggingface', 'language-modeling', 'nlp', 'precision-showcase', 'transformer']
bert-fp32 [config.weight] 0.0
bert-fp32 [config.name] bert-fp32
bert-fp32 [config.tag] ['bert-fp32']
bert-fp32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 61.996,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 27,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256736.854086,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
bert-fp32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision fp32 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:58:56.872441]
bert-fp32 [stdout] Preparing Bert
bert-fp32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision fp32 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:58:59.836639]
bert-fp16 [config.system.arch] cuda
bert-fp16 [config.system.sshkey] None
bert-fp16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
bert-fp16 [config.system.gpu.capacity] 81920 MiB
bert-fp16 [config.system.self.name] local
bert-fp16 [config.system.self.ip] 127.0.0.1
bert-fp16 [config.system.self.port] 8123
bert-fp16 [config.system.self.user] root
bert-fp16 [config.system.self.main] True
bert-fp16 [config.system.self.hostname] localhost
bert-fp16 [config.system.self.aliaslist] []
bert-fp16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
bert-fp16 [config.system.self.local] True
bert-fp16 [config.dirs.base] /Tmp/slurm.4115007.0/base
bert-fp16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
bert-fp16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
bert-fp16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
bert-fp16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
bert-fp16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
bert-fp16 [config.group] hf
bert-fp16 [config.install_group] torch
bert-fp16 [config.install_variant] cuda
bert-fp16 [config.run_name] prepare.2024-02-06_11:57:01.236402
bert-fp16 [config.enabled] True
bert-fp16 [config.capabilities.nodes] 1
bert-fp16 [config.max_duration] 600
bert-fp16 [config.voir.options.stop] 30
bert-fp16 [config.voir.options.interval] 1s
bert-fp16 [config.validation.usage.gpu_load_threshold] 0.5
bert-fp16 [config.validation.usage.gpu_mem_threshold] 0.5
bert-fp16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
bert-fp16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
bert-fp16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
bert-fp16 [config.argv.--precision] fp16
bert-fp16 [config.argv.--num-workers] 8
bert-fp16 [config.argv.--model] Bert
bert-fp16 [config.argv.--batch-size] 32
bert-fp16 [config.plan.method] per_gpu
bert-fp16 [config.tags] ['huggingface', 'language-modeling', 'nlp', 'precision-showcase', 'transformer']
bert-fp16 [config.weight] 0.0
bert-fp16 [config.name] bert-fp16
bert-fp16 [config.tag] ['bert-fp16']
bert-fp16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.068,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 27,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256742.225864,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
bert-fp16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision fp16 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:02.244352]
bert-fp16 [stdout] Preparing Bert
bert-fp16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision fp16 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:05.184879]
bert-tf32 [config.system.arch] cuda
bert-tf32 [config.system.sshkey] None
bert-tf32 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
bert-tf32 [config.system.gpu.capacity] 81920 MiB
bert-tf32 [config.system.self.name] local
bert-tf32 [config.system.self.ip] 127.0.0.1
bert-tf32 [config.system.self.port] 8123
bert-tf32 [config.system.self.user] root
bert-tf32 [config.system.self.main] True
bert-tf32 [config.system.self.hostname] localhost
bert-tf32 [config.system.self.aliaslist] []
bert-tf32 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
bert-tf32 [config.system.self.local] True
bert-tf32 [config.dirs.base] /Tmp/slurm.4115007.0/base
bert-tf32 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
bert-tf32 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
bert-tf32 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
bert-tf32 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
bert-tf32 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
bert-tf32 [config.group] hf
bert-tf32 [config.install_group] torch
bert-tf32 [config.install_variant] cuda
bert-tf32 [config.run_name] prepare.2024-02-06_11:57:01.236402
bert-tf32 [config.enabled] True
bert-tf32 [config.capabilities.nodes] 1
bert-tf32 [config.max_duration] 600
bert-tf32 [config.voir.options.stop] 30
bert-tf32 [config.voir.options.interval] 1s
bert-tf32 [config.validation.usage.gpu_load_threshold] 0.5
bert-tf32 [config.validation.usage.gpu_mem_threshold] 0.5
bert-tf32 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
bert-tf32 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
bert-tf32 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
bert-tf32 [config.argv.--precision] tf32
bert-tf32 [config.argv.--num-workers] 8
bert-tf32 [config.argv.--model] Bert
bert-tf32 [config.argv.--batch-size] 32
bert-tf32 [config.plan.method] per_gpu
bert-tf32 [config.tags] ['huggingface', 'language-modeling', 'nlp', 'precision-showcase', 'transformer']
bert-tf32 [config.weight] 0.0
bert-tf32 [config.name] bert-tf32
bert-tf32 [config.tag] ['bert-tf32']
bert-tf32 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.285,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 28,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256747.579552,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
bert-tf32 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:07.598271]
bert-tf32 [stdout] Preparing Bert
bert-tf32 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:10.571605]
bert-tf32-fp16 [config.system.arch] cuda
bert-tf32-fp16 [config.system.sshkey] None
bert-tf32-fp16 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
bert-tf32-fp16 [config.system.gpu.capacity] 81920 MiB
bert-tf32-fp16 [config.system.self.name] local
bert-tf32-fp16 [config.system.self.ip] 127.0.0.1
bert-tf32-fp16 [config.system.self.port] 8123
bert-tf32-fp16 [config.system.self.user] root
bert-tf32-fp16 [config.system.self.main] True
bert-tf32-fp16 [config.system.self.hostname] localhost
bert-tf32-fp16 [config.system.self.aliaslist] []
bert-tf32-fp16 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
bert-tf32-fp16 [config.system.self.local] True
bert-tf32-fp16 [config.dirs.base] /Tmp/slurm.4115007.0/base
bert-tf32-fp16 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
bert-tf32-fp16 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
bert-tf32-fp16 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
bert-tf32-fp16 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
bert-tf32-fp16 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
bert-tf32-fp16 [config.group] hf
bert-tf32-fp16 [config.install_group] torch
bert-tf32-fp16 [config.install_variant] cuda
bert-tf32-fp16 [config.run_name] prepare.2024-02-06_11:57:01.236402
bert-tf32-fp16 [config.enabled] True
bert-tf32-fp16 [config.capabilities.nodes] 1
bert-tf32-fp16 [config.max_duration] 600
bert-tf32-fp16 [config.voir.options.stop] 30
bert-tf32-fp16 [config.voir.options.interval] 1s
bert-tf32-fp16 [config.validation.usage.gpu_load_threshold] 0.5
bert-tf32-fp16 [config.validation.usage.gpu_mem_threshold] 0.5
bert-tf32-fp16 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
bert-tf32-fp16 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
bert-tf32-fp16 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
bert-tf32-fp16 [config.argv.--precision] tf32-fp16
bert-tf32-fp16 [config.argv.--num-workers] 8
bert-tf32-fp16 [config.argv.--model] Bert
bert-tf32-fp16 [config.argv.--batch-size] 32
bert-tf32-fp16 [config.plan.method] per_gpu
bert-tf32-fp16 [config.tags] ['huggingface', 'language-modeling', 'nlp', 'precision-showcase', 'transformer']
bert-tf32-fp16 [config.weight] 3.0
bert-tf32-fp16 [config.name] bert-tf32-fp16
bert-tf32-fp16 [config.tag] ['bert-tf32-fp16']
bert-tf32-fp16 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.605,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 28,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256752.997973,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
bert-tf32-fp16 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:13.016733]
bert-tf32-fp16 [stdout] Preparing Bert
bert-tf32-fp16 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Bert --batch-size 32 [at 2024-02-06 11:59:15.947285]
t5 [config.system.arch] cuda
t5 [config.system.sshkey] None
t5 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
t5 [config.system.gpu.capacity] 81920 MiB
t5 [config.system.self.name] local
t5 [config.system.self.ip] 127.0.0.1
t5 [config.system.self.port] 8123
t5 [config.system.self.user] root
t5 [config.system.self.main] True
t5 [config.system.self.hostname] localhost
t5 [config.system.self.aliaslist] []
t5 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
t5 [config.system.self.local] True
t5 [config.dirs.base] /Tmp/slurm.4115007.0/base
t5 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
t5 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
t5 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
t5 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
t5 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
t5 [config.group] hf
t5 [config.install_group] torch
t5 [config.install_variant] cuda
t5 [config.run_name] prepare.2024-02-06_11:57:01.236402
t5 [config.enabled] True
t5 [config.capabilities.nodes] 1
t5 [config.max_duration] 600
t5 [config.voir.options.stop] 60
t5 [config.voir.options.interval] 1s
t5 [config.validation.usage.gpu_load_threshold] 0.5
t5 [config.validation.usage.gpu_mem_threshold] 0.5
t5 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
t5 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
t5 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
t5 [config.argv.--precision] tf32-fp16
t5 [config.argv.--num-workers] 8
t5 [config.argv.--model] T5
t5 [config.argv.--batch-size] 16
t5 [config.plan.method] per_gpu
t5 [config.tags] ['huggingface', 'language-modeling', 'nlp', 'transformer']
t5 [config.weight] 2.0
t5 [config.name] t5
t5 [config.tag] ['t5']
t5 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.605,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 29,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256758.378135,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
t5 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model T5 --batch-size 16 [at 2024-02-06 11:59:18.396944]
t5 [stdout] Preparing T5
t5 [stderr] Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]
t5 [stderr] Downloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 351kB/s]
t5 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model T5 --batch-size 16 [at 2024-02-06 11:59:20.965967]
reformer [config.system.arch] cuda
reformer [config.system.sshkey] None
reformer [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
reformer [config.system.gpu.capacity] 81920 MiB
reformer [config.system.self.name] local
reformer [config.system.self.ip] 127.0.0.1
reformer [config.system.self.port] 8123
reformer [config.system.self.user] root
reformer [config.system.self.main] True
reformer [config.system.self.hostname] localhost
reformer [config.system.self.aliaslist] []
reformer [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
reformer [config.system.self.local] True
reformer [config.dirs.base] /Tmp/slurm.4115007.0/base
reformer [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
reformer [config.dirs.data] /Tmp/slurm.4115007.0/base/data
reformer [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
reformer [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
reformer [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
reformer [config.group] hf
reformer [config.install_group] torch
reformer [config.install_variant] cuda
reformer [config.run_name] prepare.2024-02-06_11:57:01.236402
reformer [config.enabled] True
reformer [config.capabilities.nodes] 1
reformer [config.max_duration] 600
reformer [config.voir.options.stop] 60
reformer [config.voir.options.interval] 1s
reformer [config.validation.usage.gpu_load_threshold] 0.5
reformer [config.validation.usage.gpu_mem_threshold] 0.5
reformer [config.config_base] /Tmp/slurm.4115007.0/milabench/config
reformer [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
reformer [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
reformer [config.argv.--precision] tf32-fp16
reformer [config.argv.--num-workers] 8
reformer [config.argv.--model] Reformer
reformer [config.argv.--batch-size] 64
reformer [config.plan.method] per_gpu
reformer [config.tags] ['huggingface', 'language-modeling', 'nlp', 'transformer']
reformer [config.weight] 1.0
reformer [config.name] reformer
reformer [config.tag] ['reformer']
reformer [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.894,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 29,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256763.386218,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
reformer [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Reformer --batch-size 64 [at 2024-02-06 11:59:23.404946]
reformer [stdout] Preparing Reformer
reformer [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Reformer --batch-size 64 [at 2024-02-06 11:59:25.250427]
whisper [config.system.arch] cuda
whisper [config.system.sshkey] None
whisper [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
whisper [config.system.gpu.capacity] 81920 MiB
whisper [config.system.self.name] local
whisper [config.system.self.ip] 127.0.0.1
whisper [config.system.self.port] 8123
whisper [config.system.self.user] root
whisper [config.system.self.main] True
whisper [config.system.self.hostname] localhost
whisper [config.system.self.aliaslist] []
whisper [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
whisper [config.system.self.local] True
whisper [config.dirs.base] /Tmp/slurm.4115007.0/base
whisper [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
whisper [config.dirs.data] /Tmp/slurm.4115007.0/base/data
whisper [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
whisper [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/hf
whisper [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
whisper [config.group] hf
whisper [config.install_group] torch
whisper [config.install_variant] cuda
whisper [config.run_name] prepare.2024-02-06_11:57:01.236402
whisper [config.enabled] True
whisper [config.capabilities.nodes] 1
whisper [config.max_duration] 600
whisper [config.voir.options.stop] 60
whisper [config.voir.options.interval] 1s
whisper [config.validation.usage.gpu_load_threshold] 0.5
whisper [config.validation.usage.gpu_mem_threshold] 0.5
whisper [config.config_base] /Tmp/slurm.4115007.0/milabench/config
whisper [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
whisper [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface
whisper [config.argv.--precision] tf32-fp16
whisper [config.argv.--num-workers] 8
whisper [config.argv.--model] Whisper
whisper [config.argv.--batch-size] 64
whisper [config.plan.method] per_gpu
whisper [config.tags] ['audio', 'huggingface']
whisper [config.weight] 1.0
whisper [config.name] whisper
whisper [config.tag] ['whisper']
whisper [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 62.894,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 29,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.439,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256767.617855,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
whisper [start] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Whisper --batch-size 64 [at 2024-02-06 11:59:27.636436]
whisper [stdout] Preparing Whisper
whisper [stderr] Downloading config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]
whisper [stderr] Downloading config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 584kB/s]
whisper [end] /Tmp/slurm.4115007.0/milabench/benchmarks/huggingface/prepare.py --precision tf32-fp16 --num-workers 8 --model Whisper --batch-size 64 [at 2024-02-06 11:59:29.721219]
resnet152 [config.system.arch] cuda
resnet152 [config.system.sshkey] None
resnet152 [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
resnet152 [config.system.gpu.capacity] 81920 MiB
resnet152 [config.system.self.name] local
resnet152 [config.system.self.ip] 127.0.0.1
resnet152 [config.system.self.port] 8123
resnet152 [config.system.self.user] root
resnet152 [config.system.self.main] True
resnet152 [config.system.self.hostname] localhost
resnet152 [config.system.self.aliaslist] []
resnet152 [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
resnet152 [config.system.self.local] True
resnet152 [config.dirs.base] /Tmp/slurm.4115007.0/base
resnet152 [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
resnet152 [config.dirs.data] /Tmp/slurm.4115007.0/base/data
resnet152 [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
resnet152 [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/timm
resnet152 [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
resnet152 [config.group] timm
resnet152 [config.install_group] torch
resnet152 [config.install_variant] cuda
resnet152 [config.run_name] prepare.2024-02-06_11:57:01.236402
resnet152 [config.enabled] True
resnet152 [config.capabilities.nodes] 1
resnet152 [config.max_duration] 600
resnet152 [config.voir.options.stop] 60
resnet152 [config.voir.options.interval] 1s
resnet152 [config.validation.usage.gpu_load_threshold] 0.5
resnet152 [config.validation.usage.gpu_mem_threshold] 0.5
resnet152 [config.config_base] /Tmp/slurm.4115007.0/milabench/config
resnet152 [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
resnet152 [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/timm
resnet152 [config.plan.method] per_gpu
resnet152 [config.argv.--amp] True
resnet152 [config.argv.--model] resnet152
resnet152 [config.argv.--batch-size] 256
resnet152 [config.tags] ['classification', 'convnet', 'resnet', 'vision']
resnet152 [config.weight] 1.0
resnet152 [config.name] resnet152
resnet152 [config.tag] ['resnet152']
resnet152 [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.135,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256772.089362,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
resnet152 [start] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model resnet152 --batch-size 256 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/resnet152 --checkpoint-hist 1 [at 2024-02-06 11:59:32.108301]
resnet152 [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
resnet152 [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
resnet152 [stdout] Done!
resnet152 [end] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model resnet152 --batch-size 256 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/resnet152 --checkpoint-hist 1 [at 2024-02-06 11:59:32.142713]
resnet152-multi [config.system.arch] cuda
resnet152-multi [config.system.sshkey] None
resnet152-multi [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
resnet152-multi [config.system.gpu.capacity] 81920 MiB
resnet152-multi [config.system.self.name] local
resnet152-multi [config.system.self.ip] 127.0.0.1
resnet152-multi [config.system.self.port] 8123
resnet152-multi [config.system.self.user] root
resnet152-multi [config.system.self.main] True
resnet152-multi [config.system.self.hostname] localhost
resnet152-multi [config.system.self.aliaslist] []
resnet152-multi [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
resnet152-multi [config.system.self.local] True
resnet152-multi [config.dirs.base] /Tmp/slurm.4115007.0/base
resnet152-multi [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
resnet152-multi [config.dirs.data] /Tmp/slurm.4115007.0/base/data
resnet152-multi [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
resnet152-multi [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/timm
resnet152-multi [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
resnet152-multi [config.group] timm
resnet152-multi [config.install_group] torch
resnet152-multi [config.install_variant] cuda
resnet152-multi [config.run_name] prepare.2024-02-06_11:57:01.236402
resnet152-multi [config.enabled] True
resnet152-multi [config.capabilities.nodes] 1
resnet152-multi [config.max_duration] 600
resnet152-multi [config.voir.options.stop] 60
resnet152-multi [config.voir.options.interval] 1s
resnet152-multi [config.validation.usage.gpu_load_threshold] 0.5
resnet152-multi [config.validation.usage.gpu_mem_threshold] 0.5
resnet152-multi [config.config_base] /Tmp/slurm.4115007.0/milabench/config
resnet152-multi [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
resnet152-multi [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/timm
resnet152-multi [config.plan.method] njobs
resnet152-multi [config.plan.n] 1
resnet152-multi [config.argv.--amp] True
resnet152-multi [config.argv.--model] resnet152
resnet152-multi [config.argv.--batch-size] 256
resnet152-multi [config.tags] ['classification', 'convnet', 'multigpu', 'resnet', 'vision']
resnet152-multi [config.weight] 5.0
resnet152-multi [config.name] resnet152-multi
resnet152-multi [config.tag] ['resnet152-multi']
resnet152-multi [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.135,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256774.585363,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
resnet152-multi [start] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model resnet152 --batch-size 256 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/resnet152-multi --checkpoint-hist 1 [at 2024-02-06 11:59:34.604282]
resnet152-multi [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
resnet152-multi [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
resnet152-multi [stdout] Done!
resnet152-multi [end] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model resnet152 --batch-size 256 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/resnet152-multi --checkpoint-hist 1 [at 2024-02-06 11:59:34.638684]
davit_large [config.system.arch] cuda
davit_large [config.system.sshkey] None
davit_large [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
davit_large [config.system.gpu.capacity] 81920 MiB
davit_large [config.system.self.name] local
davit_large [config.system.self.ip] 127.0.0.1
davit_large [config.system.self.port] 8123
davit_large [config.system.self.user] root
davit_large [config.system.self.main] True
davit_large [config.system.self.hostname] localhost
davit_large [config.system.self.aliaslist] []
davit_large [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
davit_large [config.system.self.local] True
davit_large [config.dirs.base] /Tmp/slurm.4115007.0/base
davit_large [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
davit_large [config.dirs.data] /Tmp/slurm.4115007.0/base/data
davit_large [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
davit_large [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/timm
davit_large [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
davit_large [config.group] timm
davit_large [config.install_group] torch
davit_large [config.install_variant] cuda
davit_large [config.run_name] prepare.2024-02-06_11:57:01.236402
davit_large [config.enabled] True
davit_large [config.capabilities.nodes] 1
davit_large [config.max_duration] 600
davit_large [config.voir.options.stop] 60
davit_large [config.voir.options.interval] 1s
davit_large [config.validation.usage.gpu_load_threshold] 0.5
davit_large [config.validation.usage.gpu_mem_threshold] 0.5
davit_large [config.config_base] /Tmp/slurm.4115007.0/milabench/config
davit_large [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
davit_large [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/timm
davit_large [config.plan.method] per_gpu
davit_large [config.argv.--amp] True
davit_large [config.argv.--model] davit_large
davit_large [config.argv.--batch-size] 128
davit_large [config.argv.--lr-base] 0.01
davit_large [config.tags] ['classification', 'transformer', 'vision']
davit_large [config.weight] 1.0
davit_large [config.name] davit_large
davit_large [config.tag] ['davit_large']
davit_large [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.135,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256776.999299,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
davit_large [start] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model davit_large --batch-size 128 --lr-base 0.01 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/davit_large --checkpoint-hist 1 [at 2024-02-06 11:59:37.021438]
davit_large [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
davit_large [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
davit_large [stdout] Done!
davit_large [end] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model davit_large --batch-size 128 --lr-base 0.01 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/davit_large --checkpoint-hist 1 [at 2024-02-06 11:59:37.055802]
davit_large-multi [config.system.arch] cuda
davit_large-multi [config.system.sshkey] None
davit_large-multi [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
davit_large-multi [config.system.gpu.capacity] 81920 MiB
davit_large-multi [config.system.self.name] local
davit_large-multi [config.system.self.ip] 127.0.0.1
davit_large-multi [config.system.self.port] 8123
davit_large-multi [config.system.self.user] root
davit_large-multi [config.system.self.main] True
davit_large-multi [config.system.self.hostname] localhost
davit_large-multi [config.system.self.aliaslist] []
davit_large-multi [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
davit_large-multi [config.system.self.local] True
davit_large-multi [config.dirs.base] /Tmp/slurm.4115007.0/base
davit_large-multi [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
davit_large-multi [config.dirs.data] /Tmp/slurm.4115007.0/base/data
davit_large-multi [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
davit_large-multi [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/timm
davit_large-multi [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
davit_large-multi [config.group] timm
davit_large-multi [config.install_group] torch
davit_large-multi [config.install_variant] cuda
davit_large-multi [config.run_name] prepare.2024-02-06_11:57:01.236402
davit_large-multi [config.enabled] True
davit_large-multi [config.capabilities.nodes] 1
davit_large-multi [config.max_duration] 600
davit_large-multi [config.voir.options.stop] 60
davit_large-multi [config.voir.options.interval] 1s
davit_large-multi [config.validation.usage.gpu_load_threshold] 0.5
davit_large-multi [config.validation.usage.gpu_mem_threshold] 0.5
davit_large-multi [config.config_base] /Tmp/slurm.4115007.0/milabench/config
davit_large-multi [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
davit_large-multi [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/timm
davit_large-multi [config.plan.method] njobs
davit_large-multi [config.plan.n] 1
davit_large-multi [config.argv.--amp] True
davit_large-multi [config.argv.--model] davit_large
davit_large-multi [config.argv.--batch-size] 128
davit_large-multi [config.argv.--lr-base] 0.01
davit_large-multi [config.tags] ['classification', 'multigpu', 'transformer', 'vision']
davit_large-multi [config.weight] 5.0
davit_large-multi [config.name] davit_large-multi
davit_large-multi [config.tag] ['davit_large-multi']
davit_large-multi [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.351,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256779.403725,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
davit_large-multi [start] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model davit_large --batch-size 128 --lr-base 0.01 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/davit_large-multi --checkpoint-hist 1 [at 2024-02-06 11:59:39.422439]
davit_large-multi [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
davit_large-multi [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
davit_large-multi [stdout] Done!
davit_large-multi [end] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model davit_large --batch-size 128 --lr-base 0.01 --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/davit_large-multi --checkpoint-hist 1 [at 2024-02-06 11:59:39.456710]
focalnet [config.system.arch] cuda
focalnet [config.system.sshkey] None
focalnet [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
focalnet [config.system.gpu.capacity] 81920 MiB
focalnet [config.system.self.name] local
focalnet [config.system.self.ip] 127.0.0.1
focalnet [config.system.self.port] 8123
focalnet [config.system.self.user] root
focalnet [config.system.self.main] True
focalnet [config.system.self.hostname] localhost
focalnet [config.system.self.aliaslist] []
focalnet [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
focalnet [config.system.self.local] True
focalnet [config.dirs.base] /Tmp/slurm.4115007.0/base
focalnet [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
focalnet [config.dirs.data] /Tmp/slurm.4115007.0/base/data
focalnet [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
focalnet [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/timm
focalnet [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
focalnet [config.group] timm
focalnet [config.install_group] torch
focalnet [config.install_variant] cuda
focalnet [config.run_name] prepare.2024-02-06_11:57:01.236402
focalnet [config.enabled] True
focalnet [config.capabilities.nodes] 1
focalnet [config.max_duration] 600
focalnet [config.voir.options.stop] 60
focalnet [config.voir.options.interval] 1s
focalnet [config.validation.usage.gpu_load_threshold] 0.5
focalnet [config.validation.usage.gpu_mem_threshold] 0.5
focalnet [config.config_base] /Tmp/slurm.4115007.0/milabench/config
focalnet [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
focalnet [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/timm
focalnet [config.plan.method] per_gpu
focalnet [config.argv.--amp] True
focalnet [config.argv.--model] focalnet_base_lrf
focalnet [config.tags] ['classification', 'convnet', 'vision']
focalnet [config.weight] 2.0
focalnet [config.name] focalnet
focalnet [config.tag] ['focalnet']
focalnet [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.425,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256781.892571,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
focalnet [start] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model focalnet_base_lrf --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/focalnet --checkpoint-hist 1 [at 2024-02-06 11:59:41.911608]
focalnet [stdout] Generating fake data into /Tmp/slurm.4115007.0/base/data/FakeImageNet...
focalnet [stdout] /Tmp/slurm.4115007.0/base/data/FakeImageNet was already generated
focalnet [stdout] Done!
focalnet [end] /Tmp/slurm.4115007.0/milabench/benchmarks/timm/prepare.py --amp --model focalnet_base_lrf --data-dir /Tmp/slurm.4115007.0/base/data --dataset FakeImageNet --output /Tmp/slurm.4115007.0/base/extra/timm/prepare.2024-02-06_11:57:01.236402/focalnet --checkpoint-hist 1 [at 2024-02-06 11:59:41.945506]
opt-1_3b [config.system.arch] cuda
opt-1_3b [config.system.sshkey] None
opt-1_3b [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
opt-1_3b [config.system.gpu.capacity] 81920 MiB
opt-1_3b [config.system.self.name] local
opt-1_3b [config.system.self.ip] 127.0.0.1
opt-1_3b [config.system.self.port] 8123
opt-1_3b [config.system.self.user] root
opt-1_3b [config.system.self.main] True
opt-1_3b [config.system.self.hostname] localhost
opt-1_3b [config.system.self.aliaslist] []
opt-1_3b [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
opt-1_3b [config.system.self.local] True
opt-1_3b [config.dirs.base] /Tmp/slurm.4115007.0/base
opt-1_3b [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
opt-1_3b [config.dirs.data] /Tmp/slurm.4115007.0/base/data
opt-1_3b [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
opt-1_3b [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/opt
opt-1_3b [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
opt-1_3b [config.group] opt
opt-1_3b [config.install_group] torch
opt-1_3b [config.install_variant] cuda
opt-1_3b [config.run_name] prepare.2024-02-06_11:57:01.236402
opt-1_3b [config.enabled] True
opt-1_3b [config.capabilities.nodes] 1
opt-1_3b [config.max_duration] 600
opt-1_3b [config.voir.options.stop] 60
opt-1_3b [config.voir.options.interval] 1s
opt-1_3b [config.validation.usage.gpu_load_threshold] 0.5
opt-1_3b [config.validation.usage.gpu_mem_threshold] 0.5
opt-1_3b [config.config_base] /Tmp/slurm.4115007.0/milabench/config
opt-1_3b [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
opt-1_3b [config.tags] ['huggingface', 'language-modeling', 'llm', 'multigpu', 'nlp', 'transformer']
opt-1_3b [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt
opt-1_3b [config.plan.method] njobs
opt-1_3b [config.plan.n] 1
opt-1_3b [config.argv.--max_train_steps] 100
opt-1_3b [config.argv.--dataset_name] wikitext
opt-1_3b [config.argv.--dataset_config_name] wikitext-103-v1
opt-1_3b [config.argv.--dataset_rev] b08601e
opt-1_3b [config.argv.--validation_split_percentage] 5
opt-1_3b [config.argv.--per_gpu_batch_size] 1
opt-1_3b [config.argv.--cpus_per_gpu] 8
opt-1_3b [config.argv.--model_name] facebook/opt-1.3b
opt-1_3b [config.gradient_accumulation_steps] 1
opt-1_3b [config.use_deepspeed] False
opt-1_3b [config.num_machines] 1
opt-1_3b [config.weight] 5.0
opt-1_3b [config.name] opt-1_3b
opt-1_3b [config.tag] ['opt-1_3b']
opt-1_3b [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 63.425,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 30,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256784.286321,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
opt-1_3b [start] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-1.3b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 11:59:44.305675]
opt-1_3b [stderr] The following values were not passed to `accelerate launch` and had defaults used instead:
opt-1_3b [stderr] 		More than one GPU was found, enabling multi-GPU training.
opt-1_3b [stderr] 		If this was unintended please pass in `--num_processes=1`.
opt-1_3b [stderr] To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
opt-1_3b [stderr] Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
opt-1_3b [stdout] [02/06/24 11:59:48] INFO     [0/1] __main__ - Distributed          logging.py:60
opt-1_3b [stdout]                              environment: MULTI_GPU  Backend: nccl
opt-1_3b [stdout]                              Num processes: 1
opt-1_3b [stdout]                              Process index: 0
opt-1_3b [stdout]                              Local process index: 0
opt-1_3b [stdout]                              Device: cuda:0
opt-1_3b [stdout] 
opt-1_3b [stdout]                              Mixed precision type: fp16
opt-1_3b [stdout] 
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   table = cls._concat_blocks(blocks, axis=0)
opt-1_3b [stderr] Downloading config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]
opt-1_3b [stderr] Downloading config.json: 100%|██████████| 653/653 [00:00<00:00, 288kB/s]
opt-1_3b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b [stderr] Model config OPTConfig {
opt-1_3b [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b [stderr]   "_remove_final_layer_norm": false,
opt-1_3b [stderr]   "activation_dropout": 0.0,
opt-1_3b [stderr]   "activation_function": "relu",
opt-1_3b [stderr]   "architectures": [
opt-1_3b [stderr]     "OPTForCausalLM"
opt-1_3b [stderr]   ],
opt-1_3b [stderr]   "attention_dropout": 0.0,
opt-1_3b [stderr]   "bos_token_id": 2,
opt-1_3b [stderr]   "do_layer_norm_before": true,
opt-1_3b [stderr]   "dropout": 0.1,
opt-1_3b [stderr]   "enable_bias": true,
opt-1_3b [stderr]   "eos_token_id": 2,
opt-1_3b [stderr]   "ffn_dim": 8192,
opt-1_3b [stderr]   "hidden_size": 2048,
opt-1_3b [stderr]   "init_std": 0.02,
opt-1_3b [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b [stderr]   "layerdrop": 0.0,
opt-1_3b [stderr]   "max_position_embeddings": 2048,
opt-1_3b [stderr]   "model_type": "opt",
opt-1_3b [stderr]   "num_attention_heads": 32,
opt-1_3b [stderr]   "num_hidden_layers": 24,
opt-1_3b [stderr]   "pad_token_id": 1,
opt-1_3b [stderr]   "prefix": "</s>",
opt-1_3b [stderr]   "torch_dtype": "float16",
opt-1_3b [stderr]   "transformers_version": "4.35.0",
opt-1_3b [stderr]   "use_cache": true,
opt-1_3b [stderr]   "vocab_size": 50272,
opt-1_3b [stderr]   "word_embed_proj_dim": 2048
opt-1_3b [stderr] }
opt-1_3b [stderr] 
opt-1_3b [stderr] Downloading tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]
opt-1_3b [stderr] Downloading tokenizer_config.json: 100%|██████████| 685/685 [00:00<00:00, 393kB/s]
opt-1_3b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b [stderr] Model config OPTConfig {
opt-1_3b [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b [stderr]   "_remove_final_layer_norm": false,
opt-1_3b [stderr]   "activation_dropout": 0.0,
opt-1_3b [stderr]   "activation_function": "relu",
opt-1_3b [stderr]   "architectures": [
opt-1_3b [stderr]     "OPTForCausalLM"
opt-1_3b [stderr]   ],
opt-1_3b [stderr]   "attention_dropout": 0.0,
opt-1_3b [stderr]   "bos_token_id": 2,
opt-1_3b [stderr]   "do_layer_norm_before": true,
opt-1_3b [stderr]   "dropout": 0.1,
opt-1_3b [stderr]   "enable_bias": true,
opt-1_3b [stderr]   "eos_token_id": 2,
opt-1_3b [stderr]   "ffn_dim": 8192,
opt-1_3b [stderr]   "hidden_size": 2048,
opt-1_3b [stderr]   "init_std": 0.02,
opt-1_3b [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b [stderr]   "layerdrop": 0.0,
opt-1_3b [stderr]   "max_position_embeddings": 2048,
opt-1_3b [stderr]   "model_type": "opt",
opt-1_3b [stderr]   "num_attention_heads": 32,
opt-1_3b [stderr]   "num_hidden_layers": 24,
opt-1_3b [stderr]   "pad_token_id": 1,
opt-1_3b [stderr]   "prefix": "</s>",
opt-1_3b [stderr]   "torch_dtype": "float16",
opt-1_3b [stderr]   "transformers_version": "4.35.0",
opt-1_3b [stderr]   "use_cache": true,
opt-1_3b [stderr]   "vocab_size": 50272,
opt-1_3b [stderr]   "word_embed_proj_dim": 2048
opt-1_3b [stderr] }
opt-1_3b [stderr] 
opt-1_3b [stderr] Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]
opt-1_3b [stderr] Downloading vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 10.3MB/s]
opt-1_3b [stderr] Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
opt-1_3b [stderr] Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 7.47MB/s]
opt-1_3b [stderr] Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]
opt-1_3b [stderr] Downloading (…)cial_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 1.48MB/s]
opt-1_3b [stderr] loading file vocab.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/vocab.json
opt-1_3b [stderr] loading file merges.txt from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/merges.txt
opt-1_3b [stderr] loading file tokenizer.json from cache at None
opt-1_3b [stderr] loading file added_tokens.json from cache at None
opt-1_3b [stderr] loading file special_tokens_map.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/special_tokens_map.json
opt-1_3b [stderr] loading file tokenizer_config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/tokenizer_config.json
opt-1_3b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b [stderr] Model config OPTConfig {
opt-1_3b [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b [stderr]   "_remove_final_layer_norm": false,
opt-1_3b [stderr]   "activation_dropout": 0.0,
opt-1_3b [stderr]   "activation_function": "relu",
opt-1_3b [stderr]   "architectures": [
opt-1_3b [stderr]     "OPTForCausalLM"
opt-1_3b [stderr]   ],
opt-1_3b [stderr]   "attention_dropout": 0.0,
opt-1_3b [stderr]   "bos_token_id": 2,
opt-1_3b [stderr]   "do_layer_norm_before": true,
opt-1_3b [stderr]   "dropout": 0.1,
opt-1_3b [stderr]   "enable_bias": true,
opt-1_3b [stderr]   "eos_token_id": 2,
opt-1_3b [stderr]   "ffn_dim": 8192,
opt-1_3b [stderr]   "hidden_size": 2048,
opt-1_3b [stderr]   "init_std": 0.02,
opt-1_3b [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b [stderr]   "layerdrop": 0.0,
opt-1_3b [stderr]   "max_position_embeddings": 2048,
opt-1_3b [stderr]   "model_type": "opt",
opt-1_3b [stderr]   "num_attention_heads": 32,
opt-1_3b [stderr]   "num_hidden_layers": 24,
opt-1_3b [stderr]   "pad_token_id": 1,
opt-1_3b [stderr]   "prefix": "</s>",
opt-1_3b [stderr]   "torch_dtype": "float16",
opt-1_3b [stderr]   "transformers_version": "4.35.0",
opt-1_3b [stderr]   "use_cache": true,
opt-1_3b [stderr]   "vocab_size": 50272,
opt-1_3b [stderr]   "word_embed_proj_dim": 2048
opt-1_3b [stderr] }
opt-1_3b [stderr] 
opt-1_3b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b [stderr] Model config OPTConfig {
opt-1_3b [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b [stderr]   "_remove_final_layer_norm": false,
opt-1_3b [stderr]   "activation_dropout": 0.0,
opt-1_3b [stderr]   "activation_function": "relu",
opt-1_3b [stderr]   "architectures": [
opt-1_3b [stderr]     "OPTForCausalLM"
opt-1_3b [stderr]   ],
opt-1_3b [stderr]   "attention_dropout": 0.0,
opt-1_3b [stderr]   "bos_token_id": 2,
opt-1_3b [stderr]   "do_layer_norm_before": true,
opt-1_3b [stderr]   "dropout": 0.1,
opt-1_3b [stderr]   "enable_bias": true,
opt-1_3b [stderr]   "eos_token_id": 2,
opt-1_3b [stderr]   "ffn_dim": 8192,
opt-1_3b [stderr]   "hidden_size": 2048,
opt-1_3b [stderr]   "init_std": 0.02,
opt-1_3b [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b [stderr]   "layerdrop": 0.0,
opt-1_3b [stderr]   "max_position_embeddings": 2048,
opt-1_3b [stderr]   "model_type": "opt",
opt-1_3b [stderr]   "num_attention_heads": 32,
opt-1_3b [stderr]   "num_hidden_layers": 24,
opt-1_3b [stderr]   "pad_token_id": 1,
opt-1_3b [stderr]   "prefix": "</s>",
opt-1_3b [stderr]   "torch_dtype": "float16",
opt-1_3b [stderr]   "transformers_version": "4.35.0",
opt-1_3b [stderr]   "use_cache": true,
opt-1_3b [stderr]   "vocab_size": 50272,
opt-1_3b [stderr]   "word_embed_proj_dim": 2048
opt-1_3b [stderr] }
opt-1_3b [stderr] 
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/4358 [00:00<?, ? examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 545/4358 [00:00<00:01, 3188.48 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 1635/4358 [00:00<00:00, 6248.54 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 3270/4358 [00:00<00:00, 9967.24 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 4358/4358 [00:00<00:00, 9008.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/1801350 [00:00<?, ? examples/s]
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 1000/1801350 [00:00<09:20, 3213.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 3000/1801350 [00:00<04:09, 7221.44 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 6000/1801350 [00:00<02:20, 12790.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 8000/1801350 [00:00<02:07, 14119.67 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 12000/1801350 [00:00<01:34, 18987.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 17000/1801350 [00:00<01:09, 25796.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 20000/1801350 [00:01<01:14, 24019.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   1%|▏         | 23000/1801350 [00:01<01:15, 23612.17 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   1%|▏         | 26000/1801350 [00:01<01:15, 23649.18 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 31000/1801350 [00:01<01:13, 23983.83 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 35000/1801350 [00:01<01:07, 26160.14 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 39000/1801350 [00:01<01:06, 26352.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 42000/1801350 [00:01<01:08, 25826.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 46000/1801350 [00:02<01:03, 27657.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 49000/1801350 [00:02<01:12, 24104.16 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 53000/1801350 [00:02<01:03, 27402.44 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 56000/1801350 [00:02<01:04, 27132.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 59000/1801350 [00:02<01:06, 26129.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 62000/1801350 [00:02<01:05, 26500.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▎         | 65000/1801350 [00:02<01:04, 27118.48 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 69000/1801350 [00:02<01:01, 28284.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 72000/1801350 [00:03<01:09, 24951.38 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 77000/1801350 [00:03<01:04, 26564.78 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 80000/1801350 [00:03<01:08, 25290.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▍         | 84000/1801350 [00:03<01:06, 25643.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▍         | 87000/1801350 [00:03<01:08, 24959.56 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▌         | 93000/1801350 [00:03<01:03, 26911.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▌         | 96000/1801350 [00:04<01:05, 25932.24 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 100000/1801350 [00:04<01:04, 26200.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 103000/1801350 [00:04<01:07, 25216.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 106000/1801350 [00:04<01:06, 25557.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 111000/1801350 [00:04<01:00, 27739.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▋         | 114000/1801350 [00:04<01:06, 25284.66 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▋         | 117000/1801350 [00:04<01:08, 24614.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 121000/1801350 [00:04<01:05, 25785.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 124000/1801350 [00:05<01:07, 24966.40 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 128000/1801350 [00:05<00:59, 28068.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 131000/1801350 [00:05<01:07, 24565.98 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 135000/1801350 [00:05<01:11, 23446.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 138000/1801350 [00:05<01:08, 24198.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 141000/1801350 [00:05<01:14, 22436.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 144000/1801350 [00:05<01:11, 23238.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 148000/1801350 [00:06<01:22, 19931.66 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 151000/1801350 [00:06<01:16, 21621.09 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▊         | 154000/1801350 [00:06<01:17, 21276.11 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▊         | 157000/1801350 [00:06<01:15, 21712.66 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 160000/1801350 [00:06<01:23, 19719.60 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 166000/1801350 [00:06<01:04, 25346.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 169000/1801350 [00:07<01:10, 23084.68 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  10%|▉         | 174000/1801350 [00:07<01:00, 26759.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  10%|▉         | 177000/1801350 [00:07<01:07, 24185.86 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 181000/1801350 [00:07<00:59, 27021.47 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 184000/1801350 [00:07<00:58, 27648.93 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 187000/1801350 [00:07<01:04, 24962.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 190000/1801350 [00:07<01:02, 25613.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 193000/1801350 [00:08<01:01, 26112.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 196000/1801350 [00:08<01:01, 26247.04 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 199000/1801350 [00:08<01:07, 23875.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█▏        | 204000/1801350 [00:08<00:54, 29332.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 208000/1801350 [00:08<01:01, 25762.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 213000/1801350 [00:08<00:52, 30200.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 217000/1801350 [00:08<00:53, 29855.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 221000/1801350 [00:09<01:01, 25831.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 225000/1801350 [00:09<00:57, 27628.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 228000/1801350 [00:09<00:58, 26947.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 231000/1801350 [00:09<01:09, 22655.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 235000/1801350 [00:09<01:03, 24816.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 238000/1801350 [00:09<01:03, 24796.28 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 242000/1801350 [00:09<00:57, 27307.33 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▎        | 245000/1801350 [00:09<01:01, 25203.44 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 249000/1801350 [00:10<01:01, 25075.46 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 252000/1801350 [00:10<01:05, 23541.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 257000/1801350 [00:10<01:00, 25359.77 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 261000/1801350 [00:10<00:56, 27172.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▍        | 265000/1801350 [00:10<01:02, 24610.98 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▍        | 270000/1801350 [00:10<00:52, 29192.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▌        | 274000/1801350 [00:11<00:57, 26433.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▌        | 277000/1801350 [00:11<00:57, 26447.12 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 280000/1801350 [00:11<01:06, 22856.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 286000/1801350 [00:11<00:50, 30036.04 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 290000/1801350 [00:11<00:59, 25225.28 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▋        | 293000/1801350 [00:11<00:57, 26059.92 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▋        | 297000/1801350 [00:11<00:54, 27790.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 300000/1801350 [00:12<01:05, 22927.67 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 304000/1801350 [00:12<01:02, 23875.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 307000/1801350 [00:12<01:03, 23588.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 311000/1801350 [00:12<00:55, 27035.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 314000/1801350 [00:12<01:01, 24084.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 318000/1801350 [00:12<00:59, 25118.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 321000/1801350 [00:13<01:08, 21714.74 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 327000/1801350 [00:13<00:50, 29024.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 331000/1801350 [00:13<01:00, 24456.12 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▊        | 334000/1801350 [00:13<00:57, 25495.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▊        | 337000/1801350 [00:13<00:57, 25392.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 340000/1801350 [00:13<01:00, 24155.33 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 343000/1801350 [00:13<00:58, 24864.25 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 346000/1801350 [00:13<00:58, 24867.18 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 349000/1801350 [00:14<01:04, 22538.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 352000/1801350 [00:14<01:05, 21977.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 357000/1801350 [00:14<01:02, 22938.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 360000/1801350 [00:14<01:02, 23033.93 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  20%|██        | 364000/1801350 [00:14<00:59, 24303.89 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  20%|██        | 367000/1801350 [00:14<01:05, 21756.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 370000/1801350 [00:15<01:02, 22801.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 373000/1801350 [00:15<01:03, 22494.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 377000/1801350 [00:15<00:55, 25802.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 381000/1801350 [00:15<00:51, 27630.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██▏       | 384000/1801350 [00:15<00:50, 27978.92 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██▏       | 387000/1801350 [00:15<00:52, 26732.60 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 390000/1801350 [00:15<00:52, 27011.11 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 393000/1801350 [00:15<00:57, 24673.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 397000/1801350 [00:16<00:51, 27443.79 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 400000/1801350 [00:16<00:51, 26990.45 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 403000/1801350 [00:16<00:56, 24846.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 406000/1801350 [00:16<00:56, 24849.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 409000/1801350 [00:16<00:53, 25878.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 412000/1801350 [00:16<00:52, 26670.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 415000/1801350 [00:16<00:54, 25365.66 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 418000/1801350 [00:16<00:52, 26358.17 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 421000/1801350 [00:16<00:54, 25181.40 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▎       | 425000/1801350 [00:17<00:48, 28608.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 428000/1801350 [00:17<00:56, 24181.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 433000/1801350 [00:17<00:51, 26497.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 436000/1801350 [00:17<00:56, 24122.32 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 441000/1801350 [00:17<00:50, 26740.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▍       | 444000/1801350 [00:17<00:53, 25348.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▍       | 448000/1801350 [00:17<00:49, 27112.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 451000/1801350 [00:18<00:55, 24276.46 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 456000/1801350 [00:18<00:51, 25971.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 459000/1801350 [00:18<00:52, 25684.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 463000/1801350 [00:18<00:50, 26719.06 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 466000/1801350 [00:18<00:52, 25326.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 470000/1801350 [00:18<00:48, 27700.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▋       | 473000/1801350 [00:18<00:53, 24940.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▋       | 477000/1801350 [00:19<00:48, 27587.13 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 481000/1801350 [00:19<00:50, 26251.43 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 486000/1801350 [00:19<00:51, 25379.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 491000/1801350 [00:19<00:45, 28826.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 494000/1801350 [00:19<00:48, 27038.89 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 497000/1801350 [00:19<00:48, 26889.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 500000/1801350 [00:19<00:47, 27344.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 503000/1801350 [00:20<00:55, 23420.03 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 508000/1801350 [00:20<00:44, 29128.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 512000/1801350 [00:20<00:49, 26027.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▊       | 515000/1801350 [00:20<00:52, 24358.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 518000/1801350 [00:20<00:53, 24160.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 522000/1801350 [00:20<00:48, 26605.11 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 525000/1801350 [00:21<00:56, 22742.79 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 530000/1801350 [00:21<00:45, 27935.48 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  30%|██▉       | 534000/1801350 [00:21<00:51, 24559.84 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  30%|██▉       | 538000/1801350 [00:21<01:00, 20854.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  30%|███       | 543000/1801350 [00:21<00:51, 24260.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  30%|███       | 546000/1801350 [00:21<00:57, 21676.44 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 550000/1801350 [00:22<00:53, 23417.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 553000/1801350 [00:22<00:54, 22780.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 557000/1801350 [00:22<00:49, 25125.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 560000/1801350 [00:22<00:50, 24643.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███▏      | 563000/1801350 [00:22<00:59, 20710.90 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███▏      | 567000/1801350 [00:22<00:55, 22320.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 571000/1801350 [00:22<00:52, 23551.51 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 574000/1801350 [00:23<00:49, 24547.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 577000/1801350 [00:23<00:51, 24005.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 582000/1801350 [00:23<00:43, 28260.77 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 585000/1801350 [00:23<00:47, 25589.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 589000/1801350 [00:23<00:47, 25630.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 592000/1801350 [00:23<00:47, 25593.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 597000/1801350 [00:23<00:46, 25959.91 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 602000/1801350 [00:24<00:41, 28863.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▎      | 605000/1801350 [00:24<00:41, 28765.22 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 608000/1801350 [00:24<00:48, 24567.73 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 611000/1801350 [00:24<00:48, 24788.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 615000/1801350 [00:24<00:44, 26410.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 619000/1801350 [00:24<00:43, 27220.24 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▍      | 622000/1801350 [00:24<00:49, 23762.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▍      | 627000/1801350 [00:25<00:39, 29391.39 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 631000/1801350 [00:25<00:45, 25819.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 634000/1801350 [00:25<00:43, 26637.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 637000/1801350 [00:25<00:44, 26267.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 640000/1801350 [00:25<00:47, 24557.50 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 645000/1801350 [00:25<00:37, 30536.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 649000/1801350 [00:25<00:43, 26529.95 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 652000/1801350 [00:26<00:46, 24911.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▋      | 655000/1801350 [00:26<00:47, 24106.14 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 658000/1801350 [00:26<00:50, 22548.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 663000/1801350 [00:26<00:42, 26479.12 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 666000/1801350 [00:26<00:42, 26616.34 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 670000/1801350 [00:26<00:41, 27291.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 673000/1801350 [00:26<00:44, 25528.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 677000/1801350 [00:26<00:42, 26431.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 681000/1801350 [00:27<00:45, 24369.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 686000/1801350 [00:27<00:41, 27189.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 689000/1801350 [00:27<00:41, 26737.62 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 692000/1801350 [00:27<00:43, 25769.32 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▊      | 695000/1801350 [00:27<00:48, 22984.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 700000/1801350 [00:27<00:41, 26475.85 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 703000/1801350 [00:28<00:43, 25117.46 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 707000/1801350 [00:28<00:38, 28482.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 710000/1801350 [00:28<00:48, 22412.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  40%|███▉      | 716000/1801350 [00:28<00:41, 26362.86 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  40%|███▉      | 719000/1801350 [00:28<00:46, 23192.61 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  40%|████      | 725000/1801350 [00:28<00:37, 28968.54 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  40%|████      | 729000/1801350 [00:29<00:41, 25924.75 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 733000/1801350 [00:29<00:41, 25639.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 736000/1801350 [00:29<00:44, 24049.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 739000/1801350 [00:29<00:50, 21116.33 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 743000/1801350 [00:29<00:46, 22712.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████▏     | 746000/1801350 [00:29<00:45, 23243.68 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 749000/1801350 [00:29<00:43, 24270.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 753000/1801350 [00:30<00:43, 24068.43 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 756000/1801350 [00:30<00:48, 21649.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 760000/1801350 [00:30<00:44, 23325.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 763000/1801350 [00:30<00:42, 24173.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 766000/1801350 [00:30<00:44, 23419.52 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 771000/1801350 [00:30<00:40, 25641.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 774000/1801350 [00:30<00:41, 24480.23 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 778000/1801350 [00:31<00:37, 27098.06 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 781000/1801350 [00:31<00:38, 26339.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▎     | 784000/1801350 [00:31<00:38, 26573.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▎     | 787000/1801350 [00:31<00:40, 25075.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 790000/1801350 [00:31<00:40, 25217.27 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 794000/1801350 [00:31<00:38, 26090.76 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 797000/1801350 [00:31<00:43, 23034.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 800000/1801350 [00:31<00:40, 24445.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▍     | 804000/1801350 [00:32<00:36, 27521.50 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▍     | 807000/1801350 [00:32<00:37, 26858.14 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▍     | 810000/1801350 [00:32<00:39, 24973.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▌     | 814000/1801350 [00:32<00:38, 25918.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▌     | 818000/1801350 [00:32<00:37, 26391.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 822000/1801350 [00:32<00:36, 26927.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 826000/1801350 [00:32<00:33, 28817.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 829000/1801350 [00:33<00:35, 27677.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 832000/1801350 [00:33<00:40, 24074.50 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▋     | 836000/1801350 [00:33<00:37, 25909.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 839000/1801350 [00:33<00:36, 26696.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 842000/1801350 [00:33<00:38, 25017.16 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 845000/1801350 [00:33<00:38, 24631.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 848000/1801350 [00:33<00:39, 24382.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 852000/1801350 [00:33<00:38, 24468.10 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 855000/1801350 [00:34<00:38, 24736.04 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 859000/1801350 [00:34<00:38, 24421.40 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 862000/1801350 [00:34<00:36, 25389.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 867000/1801350 [00:34<00:31, 29234.00 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 870000/1801350 [00:34<00:35, 26336.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▊     | 875000/1801350 [00:34<00:33, 27781.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▊     | 878000/1801350 [00:34<00:33, 27942.74 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 881000/1801350 [00:35<00:33, 27295.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 884000/1801350 [00:35<00:37, 24552.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 887000/1801350 [00:35<00:36, 24856.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 891000/1801350 [00:35<00:32, 27886.28 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  50%|████▉     | 894000/1801350 [00:35<00:33, 26692.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  50%|████▉     | 897000/1801350 [00:35<00:35, 25536.03 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  50%|████▉     | 900000/1801350 [00:35<00:39, 22585.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  50%|█████     | 903000/1801350 [00:35<00:37, 23669.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  50%|█████     | 907000/1801350 [00:36<00:34, 25888.93 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 911000/1801350 [00:36<00:34, 25871.12 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 914000/1801350 [00:36<00:35, 24792.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 918000/1801350 [00:36<00:40, 21816.24 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 922000/1801350 [00:36<00:35, 24936.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████▏    | 925000/1801350 [00:36<00:42, 20562.22 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 932000/1801350 [00:37<00:33, 26281.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 935000/1801350 [00:37<00:35, 24322.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 938000/1801350 [00:37<00:35, 24382.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 941000/1801350 [00:37<00:35, 23910.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 944000/1801350 [00:37<00:37, 22944.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 947000/1801350 [00:37<00:36, 23545.48 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 950000/1801350 [00:37<00:34, 24534.60 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 953000/1801350 [00:38<00:37, 22907.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 956000/1801350 [00:38<00:36, 23220.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 960000/1801350 [00:38<00:36, 23197.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▎    | 964000/1801350 [00:38<00:37, 22438.30 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 970000/1801350 [00:38<00:32, 25806.60 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 973000/1801350 [00:38<00:32, 25468.54 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 977000/1801350 [00:38<00:30, 26672.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 980000/1801350 [00:39<00:34, 23632.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▍    | 985000/1801350 [00:39<00:30, 26506.14 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▍    | 988000/1801350 [00:39<00:32, 25337.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 991000/1801350 [00:39<00:30, 26238.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 994000/1801350 [00:39<00:32, 24775.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 998000/1801350 [00:39<00:28, 28147.13 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1001000/1801350 [00:39<00:30, 26598.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1004000/1801350 [00:40<00:33, 24125.40 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1008000/1801350 [00:40<00:31, 25329.19 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1011000/1801350 [00:40<00:30, 26178.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▋    | 1014000/1801350 [00:40<00:31, 25102.90 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▋    | 1017000/1801350 [00:40<00:32, 24454.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1020000/1801350 [00:40<00:30, 25390.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1023000/1801350 [00:40<00:29, 26369.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1026000/1801350 [00:40<00:28, 27152.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1029000/1801350 [00:41<00:30, 25233.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1032000/1801350 [00:41<00:30, 25022.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1036000/1801350 [00:41<00:27, 27784.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1039000/1801350 [00:41<00:29, 25905.03 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1043000/1801350 [00:41<00:31, 24206.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1048000/1801350 [00:41<00:25, 29306.42 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1052000/1801350 [00:41<00:29, 25496.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▊    | 1055000/1801350 [00:42<00:28, 26341.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1059000/1801350 [00:42<00:25, 29293.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1063000/1801350 [00:42<00:29, 24834.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1068000/1801350 [00:42<00:27, 26944.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1071000/1801350 [00:42<00:28, 25973.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  60%|█████▉    | 1074000/1801350 [00:42<00:27, 26295.14 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  60%|█████▉    | 1077000/1801350 [00:42<00:29, 24202.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  60%|██████    | 1082000/1801350 [00:43<00:26, 26657.52 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  60%|██████    | 1085000/1801350 [00:43<00:27, 26464.23 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  60%|██████    | 1089000/1801350 [00:43<00:24, 28793.31 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1092000/1801350 [00:43<00:25, 28062.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1095000/1801350 [00:43<00:29, 24238.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1100000/1801350 [00:43<00:24, 29079.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████▏   | 1104000/1801350 [00:43<00:27, 25510.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████▏   | 1107000/1801350 [00:43<00:26, 26357.46 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1110000/1801350 [00:44<00:29, 23544.76 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1114000/1801350 [00:44<00:25, 27211.47 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1117000/1801350 [00:44<00:27, 24792.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1120000/1801350 [00:44<00:30, 22591.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1124000/1801350 [00:44<00:30, 22092.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1128000/1801350 [00:44<00:26, 25586.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1131000/1801350 [00:44<00:26, 24948.34 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1134000/1801350 [00:45<00:26, 24909.52 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1137000/1801350 [00:45<00:32, 20425.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▎   | 1144000/1801350 [00:45<00:23, 28275.89 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▎   | 1148000/1801350 [00:45<00:28, 23280.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1153000/1801350 [00:45<00:25, 25711.28 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1156000/1801350 [00:46<00:27, 23534.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1162000/1801350 [00:46<00:23, 27709.76 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1165000/1801350 [00:46<00:25, 24596.92 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1170000/1801350 [00:46<00:25, 24408.54 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▌   | 1176000/1801350 [00:46<00:22, 27761.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▌   | 1179000/1801350 [00:46<00:26, 23915.03 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▌   | 1185000/1801350 [00:47<00:20, 30404.74 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▌   | 1189000/1801350 [00:47<00:22, 27187.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▋   | 1194000/1801350 [00:47<00:22, 26859.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▋   | 1197000/1801350 [00:47<00:24, 25090.61 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1201000/1801350 [00:47<00:22, 26191.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1205000/1801350 [00:47<00:21, 27361.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1209000/1801350 [00:47<00:22, 26040.79 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1213000/1801350 [00:48<00:23, 25270.83 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1216000/1801350 [00:48<00:23, 25431.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1221000/1801350 [00:48<00:20, 28062.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1224000/1801350 [00:48<00:20, 27744.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1227000/1801350 [00:48<00:20, 28166.60 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1230000/1801350 [00:48<00:21, 26896.82 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1233000/1801350 [00:48<00:21, 26975.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▊   | 1236000/1801350 [00:49<00:22, 25194.68 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1239000/1801350 [00:49<00:22, 24487.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1243000/1801350 [00:49<00:20, 26925.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1246000/1801350 [00:49<00:23, 23831.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|██████▉   | 1252000/1801350 [00:49<00:19, 27701.50 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|██████▉   | 1255000/1801350 [00:49<00:21, 25042.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|██████▉   | 1258000/1801350 [00:49<00:21, 25556.17 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1261000/1801350 [00:49<00:20, 26469.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1265000/1801350 [00:50<00:19, 26823.79 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1268000/1801350 [00:50<00:21, 25277.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1273000/1801350 [00:50<00:17, 30011.10 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1277000/1801350 [00:50<00:20, 25113.17 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1282000/1801350 [00:50<00:18, 27808.78 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████▏  | 1285000/1801350 [00:50<00:20, 24691.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1289000/1801350 [00:51<00:18, 27284.75 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1292000/1801350 [00:51<00:18, 26891.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1295000/1801350 [00:51<00:19, 25358.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1299000/1801350 [00:51<00:17, 28672.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1302000/1801350 [00:51<00:20, 23821.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1306000/1801350 [00:51<00:19, 25466.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1309000/1801350 [00:51<00:20, 23549.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1312000/1801350 [00:51<00:22, 21862.51 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1315000/1801350 [00:52<00:21, 22785.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1318000/1801350 [00:52<00:23, 20754.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1321000/1801350 [00:52<00:21, 21838.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▎  | 1324000/1801350 [00:52<00:21, 22370.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▎  | 1328000/1801350 [00:52<00:19, 23906.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1331000/1801350 [00:52<00:20, 23145.45 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1336000/1801350 [00:52<00:17, 25898.33 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1340000/1801350 [00:53<00:17, 25683.86 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 1343000/1801350 [00:53<00:18, 24856.25 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 1346000/1801350 [00:53<00:19, 23555.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 1350000/1801350 [00:53<00:16, 27220.36 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 1353000/1801350 [00:53<00:18, 24455.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 1356000/1801350 [00:53<00:18, 24563.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1361000/1801350 [00:53<00:14, 29952.84 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1365000/1801350 [00:54<00:17, 25127.33 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1368000/1801350 [00:54<00:17, 24125.67 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1372000/1801350 [00:54<00:15, 27467.34 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▋  | 1375000/1801350 [00:54<00:16, 25415.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▋  | 1378000/1801350 [00:54<00:17, 24646.42 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1382000/1801350 [00:54<00:15, 26525.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1385000/1801350 [00:54<00:16, 25226.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1390000/1801350 [00:55<00:15, 26343.18 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1393000/1801350 [00:55<00:17, 23481.34 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1398000/1801350 [00:55<00:14, 27909.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1401000/1801350 [00:55<00:16, 24218.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1405000/1801350 [00:55<00:14, 27142.54 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1409000/1801350 [00:55<00:15, 25067.10 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1412000/1801350 [00:55<00:15, 25865.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▊  | 1416000/1801350 [00:56<00:15, 25637.98 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1419000/1801350 [00:56<00:16, 23779.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1425000/1801350 [00:56<00:13, 27005.40 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1428000/1801350 [00:56<00:13, 26834.67 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1431000/1801350 [00:56<00:14, 25214.93 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  80%|███████▉  | 1434000/1801350 [00:56<00:14, 25913.23 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  80%|███████▉  | 1437000/1801350 [00:56<00:15, 24024.83 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  80%|███████▉  | 1440000/1801350 [00:57<00:14, 24921.69 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  80%|████████  | 1445000/1801350 [00:57<00:12, 29174.15 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  80%|████████  | 1448000/1801350 [00:57<00:14, 24324.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1452000/1801350 [00:57<00:13, 26863.25 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1455000/1801350 [00:57<00:13, 26136.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1458000/1801350 [00:57<00:12, 26703.95 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1461000/1801350 [00:57<00:12, 26880.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████▏ | 1464000/1801350 [00:57<00:14, 23570.52 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1469000/1801350 [00:58<00:11, 29681.65 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1473000/1801350 [00:58<00:12, 27351.72 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1476000/1801350 [00:58<00:12, 26677.20 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1480000/1801350 [00:58<00:11, 28281.23 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1483000/1801350 [00:58<00:12, 24663.42 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1487000/1801350 [00:58<00:13, 22990.85 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1490000/1801350 [00:59<00:13, 23279.87 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1495000/1801350 [00:59<00:11, 27359.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1498000/1801350 [00:59<00:13, 22649.58 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1503000/1801350 [00:59<00:14, 21110.56 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▎ | 1506000/1801350 [00:59<00:13, 21729.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1511000/1801350 [00:59<00:11, 24697.29 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1514000/1801350 [01:00<00:11, 24109.45 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1518000/1801350 [01:00<00:11, 25626.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1522000/1801350 [01:00<00:10, 26515.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1525000/1801350 [01:00<00:10, 25473.62 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1528000/1801350 [01:00<00:10, 24862.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1531000/1801350 [01:00<00:10, 24872.62 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▌ | 1534000/1801350 [01:00<00:11, 22647.63 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▌ | 1538000/1801350 [01:01<00:11, 23136.09 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1541000/1801350 [01:01<00:10, 24639.71 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1544000/1801350 [01:01<00:10, 25020.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1548000/1801350 [01:01<00:09, 27497.78 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1551000/1801350 [01:01<00:10, 24002.32 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▋ | 1557000/1801350 [01:01<00:09, 24943.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1560000/1801350 [01:01<00:09, 24963.64 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1564000/1801350 [01:02<00:09, 23810.11 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1569000/1801350 [01:02<00:08, 28561.97 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1573000/1801350 [01:02<00:08, 26756.21 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1576000/1801350 [01:02<00:08, 27448.76 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1579000/1801350 [01:02<00:08, 24973.53 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1583000/1801350 [01:02<00:07, 27937.05 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1586000/1801350 [01:02<00:09, 23620.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1590000/1801350 [01:02<00:07, 27254.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1593000/1801350 [01:03<00:08, 24199.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▊ | 1597000/1801350 [01:03<00:08, 22940.67 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1602000/1801350 [01:03<00:07, 27228.83 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1605000/1801350 [01:03<00:07, 26207.45 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1608000/1801350 [01:03<00:07, 24857.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1612000/1801350 [01:03<00:08, 23302.81 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  90%|████████▉ | 1617000/1801350 [01:04<00:06, 26876.86 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  90%|████████▉ | 1620000/1801350 [01:04<00:07, 25631.37 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  90%|█████████ | 1623000/1801350 [01:04<00:06, 25898.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  90%|█████████ | 1627000/1801350 [01:04<00:06, 28530.38 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1631000/1801350 [01:04<00:06, 26836.34 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1634000/1801350 [01:04<00:06, 26239.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1639000/1801350 [01:04<00:05, 31243.16 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1643000/1801350 [01:05<00:06, 25810.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████▏| 1646000/1801350 [01:05<00:06, 25436.49 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1650000/1801350 [01:05<00:05, 27636.16 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1653000/1801350 [01:05<00:05, 27940.26 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1656000/1801350 [01:05<00:05, 26341.41 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1659000/1801350 [01:05<00:05, 25186.07 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1662000/1801350 [01:05<00:05, 24169.59 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1665000/1801350 [01:05<00:06, 22033.78 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1671000/1801350 [01:06<00:04, 30505.62 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1675000/1801350 [01:06<00:05, 23864.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1680000/1801350 [01:06<00:04, 26647.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1683000/1801350 [01:06<00:04, 24997.22 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▎| 1687000/1801350 [01:06<00:04, 25089.46 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1690000/1801350 [01:06<00:04, 22633.08 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1693000/1801350 [01:07<00:04, 22732.51 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1696000/1801350 [01:07<00:05, 19297.35 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1700000/1801350 [01:07<00:04, 23421.73 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▍| 1704000/1801350 [01:07<00:04, 22625.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▍| 1707000/1801350 [01:07<00:03, 23780.01 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▌| 1712000/1801350 [01:07<00:03, 28395.51 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▌| 1716000/1801350 [01:08<00:03, 23813.61 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▌| 1720000/1801350 [01:08<00:03, 25880.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1724000/1801350 [01:08<00:02, 26320.39 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1727000/1801350 [01:08<00:03, 23281.99 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1731000/1801350 [01:08<00:02, 26414.74 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▋| 1734000/1801350 [01:08<00:02, 23499.50 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▋| 1737000/1801350 [01:08<00:02, 24505.96 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1742000/1801350 [01:08<00:02, 29565.80 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1746000/1801350 [01:09<00:02, 24850.88 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1750169/1801350 [01:09<00:01, 26199.30 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1755169/1801350 [01:09<00:01, 25477.73 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1760169/1801350 [01:09<00:01, 30163.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1764169/1801350 [01:09<00:01, 26051.44 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1768169/1801350 [01:09<00:01, 25610.00 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1771169/1801350 [01:10<00:01, 25891.62 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▊| 1775338/1801350 [01:10<00:01, 24980.86 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1779338/1801350 [01:10<00:00, 27821.95 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1783507/1801350 [01:10<00:00, 25575.02 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1787507/1801350 [01:10<00:00, 27763.57 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1790676/1801350 [01:10<00:00, 26403.18 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1794013/1801350 [01:10<00:00, 27336.76 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1797013/1801350 [01:11<00:00, 22448.52 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1800181/1801350 [01:11<00:00, 21337.55 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 1801350/1801350 [01:11<00:00, 25184.70 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/3760 [00:00<?, ? examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▎        | 470/3760 [00:00<00:01, 2546.94 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 1410/3760 [00:00<00:00, 5219.56 examples/s]
opt-1_3b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 3760/3760 [00:00<00:00, 8711.44 examples/s]
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stderr] libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b [stdout] [02/06/24 12:01:03] WARNING  [0/1] __main__ - The tokenizer picked logging.py:60
opt-1_3b [stdout]                              seems to have a very large
opt-1_3b [stdout]                              `model_max_length`
opt-1_3b [stdout]                              (1000000000000000019884624838656).
opt-1_3b [stdout]                              Picking 1024 instead. You can change
opt-1_3b [stdout]                              that default value by passing
opt-1_3b [stdout]                              --block_size xxx.
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/4358 [00:00<?, ? examples/s]
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  25%|██▌       | 1090/4358 [00:00<00:00, 10705.34 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 4358/4358 [00:00<00:00, 22243.72 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/1801350 [00:00<?, ? examples/s]
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 1000/1801350 [00:00<03:57, 7585.53 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 8000/1801350 [00:00<00:46, 38449.87 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   1%|          | 16000/1801350 [00:00<00:32, 55658.62 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   1%|          | 22000/1801350 [00:00<00:31, 56948.18 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   2%|▏         | 30000/1801350 [00:00<00:28, 62886.49 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   2%|▏         | 39000/1801350 [00:00<00:24, 71370.42 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   3%|▎         | 47000/1801350 [00:00<00:25, 68284.72 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   3%|▎         | 54000/1801350 [00:00<00:26, 65549.13 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   3%|▎         | 62000/1801350 [00:01<00:26, 66398.02 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   4%|▍         | 70000/1801350 [00:01<00:24, 69854.67 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   4%|▍         | 81000/1801350 [00:01<00:23, 72818.30 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   5%|▍         | 89000/1801350 [00:01<00:23, 73711.85 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   5%|▌         | 97000/1801350 [00:01<00:24, 69220.87 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   6%|▌         | 105000/1801350 [00:01<00:24, 69160.10 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   6%|▋         | 114000/1801350 [00:01<00:23, 70790.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   7%|▋         | 123000/1801350 [00:01<00:23, 72898.73 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   7%|▋         | 131000/1801350 [00:01<00:23, 71931.61 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   8%|▊         | 139000/1801350 [00:02<00:24, 68483.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   8%|▊         | 147000/1801350 [00:02<00:23, 69361.10 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   9%|▊         | 156000/1801350 [00:02<00:23, 70625.43 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   9%|▉         | 165000/1801350 [00:02<00:22, 73357.70 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  10%|▉         | 173000/1801350 [00:02<00:22, 72063.87 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  10%|█         | 181000/1801350 [00:02<00:24, 64924.79 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  11%|█         | 190000/1801350 [00:02<00:23, 67581.84 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  11%|█         | 198000/1801350 [00:02<00:23, 67033.11 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  11%|█▏        | 207000/1801350 [00:03<00:22, 70904.14 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  12%|█▏        | 215000/1801350 [00:03<00:23, 68628.68 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  12%|█▏        | 224000/1801350 [00:03<00:22, 71170.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  13%|█▎        | 234000/1801350 [00:03<00:22, 70094.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  13%|█▎        | 242000/1801350 [00:03<00:21, 71883.03 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  14%|█▍        | 250000/1801350 [00:03<00:21, 72736.68 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  14%|█▍        | 258000/1801350 [00:03<00:21, 70867.82 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  15%|█▍        | 267000/1801350 [00:03<00:22, 68991.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  15%|█▌        | 275000/1801350 [00:04<00:21, 70493.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  16%|█▌        | 283000/1801350 [00:04<00:21, 71076.57 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  16%|█▌        | 291000/1801350 [00:04<00:20, 72232.32 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  17%|█▋        | 299000/1801350 [00:04<00:21, 69817.74 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  17%|█▋        | 309000/1801350 [00:04<00:19, 74989.08 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  18%|█▊        | 317000/1801350 [00:04<00:20, 72333.21 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  18%|█▊        | 326000/1801350 [00:04<00:19, 76818.43 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  19%|█▊        | 334000/1801350 [00:04<00:20, 71552.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  19%|█▉        | 342000/1801350 [00:04<00:20, 70201.94 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  20%|█▉        | 352000/1801350 [00:05<00:19, 72956.72 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  20%|██        | 363000/1801350 [00:05<00:19, 73887.46 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  21%|██        | 372000/1801350 [00:05<00:19, 74879.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  21%|██        | 380000/1801350 [00:05<00:19, 72462.82 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  22%|██▏       | 388000/1801350 [00:05<00:19, 71339.65 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  22%|██▏       | 397000/1801350 [00:05<00:19, 73693.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  23%|██▎       | 406000/1801350 [00:05<00:18, 74528.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  23%|██▎       | 414000/1801350 [00:05<00:18, 74076.33 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  23%|██▎       | 422000/1801350 [00:06<00:18, 73334.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  24%|██▍       | 431000/1801350 [00:06<00:19, 72114.01 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  24%|██▍       | 439000/1801350 [00:06<00:18, 72125.38 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  25%|██▍       | 449000/1801350 [00:06<00:18, 73752.77 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  26%|██▌       | 460000/1801350 [00:06<00:18, 73503.18 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  26%|██▌       | 471000/1801350 [00:06<00:17, 76613.27 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  27%|██▋       | 479000/1801350 [00:06<00:17, 76749.85 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  27%|██▋       | 488000/1801350 [00:06<00:17, 74889.06 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  28%|██▊       | 497000/1801350 [00:07<00:17, 75509.56 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  28%|██▊       | 505000/1801350 [00:07<00:17, 76111.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  29%|██▊       | 514000/1801350 [00:07<00:17, 75491.28 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  29%|██▉       | 522000/1801350 [00:07<00:16, 75885.66 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  29%|██▉       | 530000/1801350 [00:07<00:17, 73954.74 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  30%|██▉       | 538000/1801350 [00:07<00:17, 71205.95 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  30%|███       | 546000/1801350 [00:07<00:17, 73493.77 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  31%|███       | 554000/1801350 [00:07<00:17, 70938.07 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  31%|███▏      | 563000/1801350 [00:07<00:16, 74922.16 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  32%|███▏      | 571000/1801350 [00:08<00:16, 76033.13 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  32%|███▏      | 579000/1801350 [00:08<00:16, 74243.26 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  33%|███▎      | 588000/1801350 [00:08<00:16, 75292.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  33%|███▎      | 596000/1801350 [00:08<00:17, 69599.20 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  34%|███▎      | 606000/1801350 [00:08<00:15, 75392.73 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  34%|███▍      | 616000/1801350 [00:08<00:15, 75051.55 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  35%|███▍      | 625000/1801350 [00:08<00:15, 77184.91 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  35%|███▌      | 633000/1801350 [00:08<00:15, 74692.78 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  36%|███▌      | 642000/1801350 [00:08<00:15, 74231.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  36%|███▌      | 650000/1801350 [00:09<00:15, 73178.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  37%|███▋      | 660000/1801350 [00:09<00:14, 78976.94 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  37%|███▋      | 668000/1801350 [00:09<00:14, 78942.16 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  38%|███▊      | 678000/1801350 [00:09<00:13, 80890.19 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  38%|███▊      | 687000/1801350 [00:09<00:13, 79953.80 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  39%|███▊      | 696000/1801350 [00:09<00:14, 76766.20 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  39%|███▉      | 704000/1801350 [00:09<00:15, 71568.60 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  40%|███▉      | 712000/1801350 [00:09<00:15, 71611.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  40%|████      | 721000/1801350 [00:10<00:15, 71326.71 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  41%|████      | 730000/1801350 [00:10<00:14, 72938.20 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  41%|████      | 738000/1801350 [00:10<00:14, 72560.34 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  42%|████▏     | 749000/1801350 [00:10<00:13, 79142.32 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  42%|████▏     | 757000/1801350 [00:10<00:13, 75234.94 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  43%|████▎     | 766000/1801350 [00:10<00:13, 76311.03 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  43%|████▎     | 774000/1801350 [00:10<00:13, 74319.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  43%|████▎     | 783000/1801350 [00:10<00:13, 74592.20 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  44%|████▍     | 791000/1801350 [00:10<00:13, 72874.16 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  44%|████▍     | 801000/1801350 [00:11<00:13, 76148.55 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  45%|████▍     | 810000/1801350 [00:11<00:12, 78594.24 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  45%|████▌     | 818000/1801350 [00:11<00:12, 75768.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  46%|████▌     | 826000/1801350 [00:11<00:13, 72878.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  46%|████▋     | 834000/1801350 [00:11<00:13, 71196.49 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  47%|████▋     | 844000/1801350 [00:11<00:12, 77375.95 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  47%|████▋     | 852000/1801350 [00:11<00:12, 75536.73 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  48%|████▊     | 861000/1801350 [00:11<00:12, 72713.55 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  48%|████▊     | 869000/1801350 [00:12<00:12, 73422.25 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  49%|████▊     | 877000/1801350 [00:12<00:12, 73382.75 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  49%|████▉     | 885000/1801350 [00:12<00:12, 71440.57 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  50%|████▉     | 894000/1801350 [00:12<00:12, 73721.71 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  50%|█████     | 903000/1801350 [00:12<00:12, 73298.56 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  51%|█████     | 911000/1801350 [00:12<00:12, 73087.98 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  51%|█████     | 919000/1801350 [00:12<00:12, 72667.26 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  51%|█████▏    | 927000/1801350 [00:12<00:12, 69244.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  52%|█████▏    | 935000/1801350 [00:12<00:12, 71042.72 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  52%|█████▏    | 943000/1801350 [00:13<00:12, 70597.91 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  53%|█████▎    | 951000/1801350 [00:13<00:12, 68532.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  53%|█████▎    | 959000/1801350 [00:13<00:11, 70359.25 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▎    | 967000/1801350 [00:13<00:12, 68146.33 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▍    | 974000/1801350 [00:13<00:12, 67015.76 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▍    | 981000/1801350 [00:13<00:12, 66051.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  55%|█████▍    | 990000/1801350 [00:13<00:11, 70334.87 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  55%|█████▌    | 998000/1801350 [00:13<00:11, 71417.01 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  56%|█████▌    | 1006000/1801350 [00:13<00:11, 68224.24 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  56%|█████▋    | 1014000/1801350 [00:14<00:11, 70165.66 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  57%|█████▋    | 1022000/1801350 [00:14<00:10, 71173.14 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  57%|█████▋    | 1031000/1801350 [00:14<00:10, 75673.50 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  58%|█████▊    | 1040000/1801350 [00:14<00:10, 72163.71 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  58%|█████▊    | 1048000/1801350 [00:14<00:10, 72313.60 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  59%|█████▊    | 1057000/1801350 [00:14<00:10, 68536.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  59%|█████▉    | 1066000/1801350 [00:14<00:11, 65852.45 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  60%|█████▉    | 1078000/1801350 [00:14<00:09, 78457.36 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  60%|██████    | 1087000/1801350 [00:15<00:09, 77668.57 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  61%|██████    | 1095000/1801350 [00:15<00:09, 71186.17 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  61%|██████    | 1103000/1801350 [00:15<00:09, 71539.44 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  62%|██████▏   | 1111000/1801350 [00:15<00:09, 72871.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  62%|██████▏   | 1119000/1801350 [00:15<00:09, 72824.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  63%|██████▎   | 1127000/1801350 [00:15<00:09, 69726.36 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  63%|██████▎   | 1135000/1801350 [00:15<00:09, 70472.85 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  64%|██████▎   | 1144000/1801350 [00:15<00:09, 69216.37 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  64%|██████▍   | 1155000/1801350 [00:16<00:08, 76540.89 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▍   | 1163000/1801350 [00:16<00:08, 74227.61 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▌   | 1171000/1801350 [00:16<00:08, 70643.61 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▌   | 1179000/1801350 [00:16<00:08, 70330.61 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  66%|██████▌   | 1187000/1801350 [00:16<00:08, 71729.70 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  66%|██████▋   | 1197000/1801350 [00:16<00:08, 72018.57 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  67%|██████▋   | 1208000/1801350 [00:16<00:07, 80267.32 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1217000/1801350 [00:16<00:07, 74986.65 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1225000/1801350 [00:16<00:08, 71141.67 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1233000/1801350 [00:17<00:08, 69751.05 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  69%|██████▉   | 1242000/1801350 [00:17<00:07, 73335.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  69%|██████▉   | 1250000/1801350 [00:17<00:07, 74268.47 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  70%|██████▉   | 1258000/1801350 [00:17<00:07, 72234.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  70%|███████   | 1266000/1801350 [00:17<00:07, 70296.48 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  71%|███████   | 1274000/1801350 [00:17<00:07, 68608.10 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  71%|███████   | 1283000/1801350 [00:17<00:07, 68836.04 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  72%|███████▏  | 1292000/1801350 [00:17<00:06, 73789.02 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  72%|███████▏  | 1300000/1801350 [00:18<00:07, 71570.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  73%|███████▎  | 1309000/1801350 [00:18<00:06, 71855.32 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  73%|███████▎  | 1317000/1801350 [00:18<00:06, 72039.50 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  74%|███████▎  | 1325000/1801350 [00:18<00:06, 73688.81 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  74%|███████▍  | 1333000/1801350 [00:18<00:06, 73259.32 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  74%|███████▍  | 1341000/1801350 [00:18<00:06, 72712.07 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  75%|███████▍  | 1350000/1801350 [00:18<00:05, 76171.65 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  75%|███████▌  | 1358000/1801350 [00:18<00:06, 70979.45 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  76%|███████▌  | 1366000/1801350 [00:18<00:06, 67248.98 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  76%|███████▋  | 1375000/1801350 [00:19<00:05, 71700.28 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  77%|███████▋  | 1383000/1801350 [00:19<00:06, 69470.29 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  77%|███████▋  | 1391000/1801350 [00:19<00:05, 68542.44 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  78%|███████▊  | 1399000/1801350 [00:19<00:06, 66990.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  78%|███████▊  | 1408000/1801350 [00:19<00:05, 72359.05 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  79%|███████▊  | 1416000/1801350 [00:19<00:05, 73146.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  79%|███████▉  | 1424000/1801350 [00:19<00:05, 73186.46 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  80%|███████▉  | 1433000/1801350 [00:19<00:04, 73870.61 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  80%|███████▉  | 1441000/1801350 [00:19<00:04, 72721.93 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  80%|████████  | 1449000/1801350 [00:20<00:05, 65837.10 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  81%|████████  | 1459000/1801350 [00:20<00:04, 72300.92 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  81%|████████▏ | 1468000/1801350 [00:20<00:04, 68502.27 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  82%|████████▏ | 1479000/1801350 [00:20<00:04, 75636.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  83%|████████▎ | 1487000/1801350 [00:20<00:04, 70027.48 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  83%|████████▎ | 1495000/1801350 [00:20<00:04, 69785.60 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  83%|████████▎ | 1503000/1801350 [00:20<00:04, 70027.27 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  84%|████████▍ | 1511000/1801350 [00:20<00:04, 71807.36 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  84%|████████▍ | 1519000/1801350 [00:21<00:03, 72387.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  85%|████████▍ | 1527000/1801350 [00:21<00:03, 70896.47 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  85%|████████▌ | 1535000/1801350 [00:21<00:03, 66713.79 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  86%|████████▌ | 1543000/1801350 [00:21<00:03, 66739.59 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  86%|████████▌ | 1552000/1801350 [00:21<00:03, 72525.17 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  87%|████████▋ | 1560000/1801350 [00:21<00:03, 72147.50 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  87%|████████▋ | 1569000/1801350 [00:21<00:03, 71736.93 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  88%|████████▊ | 1577000/1801350 [00:21<00:03, 73312.21 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  88%|████████▊ | 1585000/1801350 [00:22<00:03, 70436.13 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  88%|████████▊ | 1593000/1801350 [00:22<00:02, 70084.06 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  89%|████████▉ | 1602000/1801350 [00:22<00:02, 71129.40 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  89%|████████▉ | 1610000/1801350 [00:22<00:02, 71879.26 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  90%|████████▉ | 1618000/1801350 [00:22<00:02, 72848.45 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  90%|█████████ | 1626000/1801350 [00:22<00:02, 64620.44 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  91%|█████████ | 1636000/1801350 [00:22<00:02, 70954.51 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  91%|█████████▏| 1645000/1801350 [00:22<00:02, 72099.54 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  92%|█████████▏| 1655000/1801350 [00:23<00:01, 75565.45 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  92%|█████████▏| 1663000/1801350 [00:23<00:01, 74403.79 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  93%|█████████▎| 1672000/1801350 [00:23<00:01, 75483.62 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  93%|█████████▎| 1681169/1801350 [00:23<00:01, 79426.67 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  94%|█████████▍| 1690169/1801350 [00:23<00:01, 72602.00 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  94%|█████████▍| 1698337/1801350 [00:23<00:01, 67993.11 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  95%|█████████▍| 1706337/1801350 [00:23<00:01, 69100.66 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  95%|█████████▌| 1714337/1801350 [00:23<00:01, 68629.83 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▌| 1721337/1801350 [00:23<00:01, 66409.81 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▌| 1728337/1801350 [00:24<00:01, 62874.23 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▋| 1735675/1801350 [00:24<00:01, 65432.31 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  97%|█████████▋| 1742675/1801350 [00:24<00:00, 60899.09 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  97%|█████████▋| 1750675/1801350 [00:24<00:00, 63167.70 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  98%|█████████▊| 1759675/1801350 [00:24<00:00, 62976.90 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  98%|█████████▊| 1766844/1801350 [00:24<00:00, 60719.65 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  98%|█████████▊| 1773844/1801350 [00:24<00:00, 56513.15 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  99%|█████████▉| 1782013/1801350 [00:25<00:00, 53120.27 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  99%|█████████▉| 1788013/1801350 [00:25<00:00, 52909.86 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|█████████▉| 1795182/1801350 [00:25<00:00, 43940.63 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|█████████▉| 1801182/1801350 [00:25<00:00, 33896.44 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 1801350/1801350 [00:25<00:00, 69931.03 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/3760 [00:00<?, ? examples/s]
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  38%|███▊      | 1410/3760 [00:00<00:00, 12664.29 examples/s]
opt-1_3b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 3760/3760 [00:00<00:00, 19534.61 examples/s]
opt-1_3b [end] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-1.3b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:01:31.186258]
opt-1_3b-multinode [config.system.arch] cuda
opt-1_3b-multinode [config.system.sshkey] None
opt-1_3b-multinode [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
opt-1_3b-multinode [config.system.gpu.capacity] 81920 MiB
opt-1_3b-multinode [config.system.self.name] local
opt-1_3b-multinode [config.system.self.ip] 127.0.0.1
opt-1_3b-multinode [config.system.self.port] 8123
opt-1_3b-multinode [config.system.self.user] root
opt-1_3b-multinode [config.system.self.main] True
opt-1_3b-multinode [config.system.self.hostname] localhost
opt-1_3b-multinode [config.system.self.aliaslist] []
opt-1_3b-multinode [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
opt-1_3b-multinode [config.system.self.local] True
opt-1_3b-multinode [config.dirs.base] /Tmp/slurm.4115007.0/base
opt-1_3b-multinode [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
opt-1_3b-multinode [config.dirs.data] /Tmp/slurm.4115007.0/base/data
opt-1_3b-multinode [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
opt-1_3b-multinode [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/opt
opt-1_3b-multinode [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
opt-1_3b-multinode [config.group] opt
opt-1_3b-multinode [config.install_group] torch
opt-1_3b-multinode [config.install_variant] cuda
opt-1_3b-multinode [config.run_name] prepare.2024-02-06_11:57:01.236402
opt-1_3b-multinode [config.enabled] True
opt-1_3b-multinode [config.capabilities.nodes] 1
opt-1_3b-multinode [config.max_duration] 600
opt-1_3b-multinode [config.voir.options.stop] 60
opt-1_3b-multinode [config.voir.options.interval] 1s
opt-1_3b-multinode [config.validation.usage.gpu_load_threshold] 0.5
opt-1_3b-multinode [config.validation.usage.gpu_mem_threshold] 0.5
opt-1_3b-multinode [config.config_base] /Tmp/slurm.4115007.0/milabench/config
opt-1_3b-multinode [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
opt-1_3b-multinode [config.tags] ['huggingface', 'language-modeling', 'llm', 'multigpu', 'multinode', 'nlp', 'transformer']
opt-1_3b-multinode [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt
opt-1_3b-multinode [config.plan.method] njobs
opt-1_3b-multinode [config.plan.n] 1
opt-1_3b-multinode [config.argv.--max_train_steps] 100
opt-1_3b-multinode [config.argv.--dataset_name] wikitext
opt-1_3b-multinode [config.argv.--dataset_config_name] wikitext-103-v1
opt-1_3b-multinode [config.argv.--dataset_rev] b08601e
opt-1_3b-multinode [config.argv.--validation_split_percentage] 5
opt-1_3b-multinode [config.argv.--per_gpu_batch_size] 1
opt-1_3b-multinode [config.argv.--cpus_per_gpu] 8
opt-1_3b-multinode [config.argv.--model_name] facebook/opt-1.3b
opt-1_3b-multinode [config.gradient_accumulation_steps] 1
opt-1_3b-multinode [config.use_deepspeed] False
opt-1_3b-multinode [config.num_machines] 2
opt-1_3b-multinode [config.weight] 10.0
opt-1_3b-multinode [config.requires_capabilities] ['len(nodes) >= 2']
opt-1_3b-multinode [config.docker_image] ghcr.io/mila-iqia/milabench:cuda-nightly
opt-1_3b-multinode [config.name] opt-1_3b-multinode
opt-1_3b-multinode [config.tag] ['opt-1_3b-multinode']
opt-1_3b-multinode [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 82.05,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 35,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.371,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256893.611088,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
opt-1_3b-multinode [start] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-1.3b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:01:33.630289]
opt-1_3b-multinode [stderr] The following values were not passed to `accelerate launch` and had defaults used instead:
opt-1_3b-multinode [stderr] 		More than one GPU was found, enabling multi-GPU training.
opt-1_3b-multinode [stderr] 		If this was unintended please pass in `--num_processes=1`.
opt-1_3b-multinode [stderr] To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
opt-1_3b-multinode [stderr] Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
opt-1_3b-multinode [stdout] [02/06/24 12:01:37] INFO     [0/1] __main__ - Distributed          logging.py:60
opt-1_3b-multinode [stdout]                              environment: MULTI_GPU  Backend: nccl
opt-1_3b-multinode [stdout]                              Num processes: 1
opt-1_3b-multinode [stdout]                              Process index: 0
opt-1_3b-multinode [stdout]                              Local process index: 0
opt-1_3b-multinode [stdout]                              Device: cuda:0
opt-1_3b-multinode [stdout] 
opt-1_3b-multinode [stdout]                              Mixed precision type: fp16
opt-1_3b-multinode [stdout] 
opt-1_3b-multinode [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
opt-1_3b-multinode [stderr]   table = cls._concat_blocks(blocks, axis=0)
opt-1_3b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b-multinode [stderr] Model config OPTConfig {
opt-1_3b-multinode [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-1_3b-multinode [stderr]   "activation_dropout": 0.0,
opt-1_3b-multinode [stderr]   "activation_function": "relu",
opt-1_3b-multinode [stderr]   "architectures": [
opt-1_3b-multinode [stderr]     "OPTForCausalLM"
opt-1_3b-multinode [stderr]   ],
opt-1_3b-multinode [stderr]   "attention_dropout": 0.0,
opt-1_3b-multinode [stderr]   "bos_token_id": 2,
opt-1_3b-multinode [stderr]   "do_layer_norm_before": true,
opt-1_3b-multinode [stderr]   "dropout": 0.1,
opt-1_3b-multinode [stderr]   "enable_bias": true,
opt-1_3b-multinode [stderr]   "eos_token_id": 2,
opt-1_3b-multinode [stderr]   "ffn_dim": 8192,
opt-1_3b-multinode [stderr]   "hidden_size": 2048,
opt-1_3b-multinode [stderr]   "init_std": 0.02,
opt-1_3b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b-multinode [stderr]   "layerdrop": 0.0,
opt-1_3b-multinode [stderr]   "max_position_embeddings": 2048,
opt-1_3b-multinode [stderr]   "model_type": "opt",
opt-1_3b-multinode [stderr]   "num_attention_heads": 32,
opt-1_3b-multinode [stderr]   "num_hidden_layers": 24,
opt-1_3b-multinode [stderr]   "pad_token_id": 1,
opt-1_3b-multinode [stderr]   "prefix": "</s>",
opt-1_3b-multinode [stderr]   "torch_dtype": "float16",
opt-1_3b-multinode [stderr]   "transformers_version": "4.35.0",
opt-1_3b-multinode [stderr]   "use_cache": true,
opt-1_3b-multinode [stderr]   "vocab_size": 50272,
opt-1_3b-multinode [stderr]   "word_embed_proj_dim": 2048
opt-1_3b-multinode [stderr] }
opt-1_3b-multinode [stderr] 
opt-1_3b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b-multinode [stderr] Model config OPTConfig {
opt-1_3b-multinode [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-1_3b-multinode [stderr]   "activation_dropout": 0.0,
opt-1_3b-multinode [stderr]   "activation_function": "relu",
opt-1_3b-multinode [stderr]   "architectures": [
opt-1_3b-multinode [stderr]     "OPTForCausalLM"
opt-1_3b-multinode [stderr]   ],
opt-1_3b-multinode [stderr]   "attention_dropout": 0.0,
opt-1_3b-multinode [stderr]   "bos_token_id": 2,
opt-1_3b-multinode [stderr]   "do_layer_norm_before": true,
opt-1_3b-multinode [stderr]   "dropout": 0.1,
opt-1_3b-multinode [stderr]   "enable_bias": true,
opt-1_3b-multinode [stderr]   "eos_token_id": 2,
opt-1_3b-multinode [stderr]   "ffn_dim": 8192,
opt-1_3b-multinode [stderr]   "hidden_size": 2048,
opt-1_3b-multinode [stderr]   "init_std": 0.02,
opt-1_3b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b-multinode [stderr]   "layerdrop": 0.0,
opt-1_3b-multinode [stderr]   "max_position_embeddings": 2048,
opt-1_3b-multinode [stderr]   "model_type": "opt",
opt-1_3b-multinode [stderr]   "num_attention_heads": 32,
opt-1_3b-multinode [stderr]   "num_hidden_layers": 24,
opt-1_3b-multinode [stderr]   "pad_token_id": 1,
opt-1_3b-multinode [stderr]   "prefix": "</s>",
opt-1_3b-multinode [stderr]   "torch_dtype": "float16",
opt-1_3b-multinode [stderr]   "transformers_version": "4.35.0",
opt-1_3b-multinode [stderr]   "use_cache": true,
opt-1_3b-multinode [stderr]   "vocab_size": 50272,
opt-1_3b-multinode [stderr]   "word_embed_proj_dim": 2048
opt-1_3b-multinode [stderr] }
opt-1_3b-multinode [stderr] 
opt-1_3b-multinode [stderr] loading file vocab.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/vocab.json
opt-1_3b-multinode [stderr] loading file merges.txt from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/merges.txt
opt-1_3b-multinode [stderr] loading file tokenizer.json from cache at None
opt-1_3b-multinode [stderr] loading file added_tokens.json from cache at None
opt-1_3b-multinode [stderr] loading file special_tokens_map.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/special_tokens_map.json
opt-1_3b-multinode [stderr] loading file tokenizer_config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/tokenizer_config.json
opt-1_3b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b-multinode [stderr] Model config OPTConfig {
opt-1_3b-multinode [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-1_3b-multinode [stderr]   "activation_dropout": 0.0,
opt-1_3b-multinode [stderr]   "activation_function": "relu",
opt-1_3b-multinode [stderr]   "architectures": [
opt-1_3b-multinode [stderr]     "OPTForCausalLM"
opt-1_3b-multinode [stderr]   ],
opt-1_3b-multinode [stderr]   "attention_dropout": 0.0,
opt-1_3b-multinode [stderr]   "bos_token_id": 2,
opt-1_3b-multinode [stderr]   "do_layer_norm_before": true,
opt-1_3b-multinode [stderr]   "dropout": 0.1,
opt-1_3b-multinode [stderr]   "enable_bias": true,
opt-1_3b-multinode [stderr]   "eos_token_id": 2,
opt-1_3b-multinode [stderr]   "ffn_dim": 8192,
opt-1_3b-multinode [stderr]   "hidden_size": 2048,
opt-1_3b-multinode [stderr]   "init_std": 0.02,
opt-1_3b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b-multinode [stderr]   "layerdrop": 0.0,
opt-1_3b-multinode [stderr]   "max_position_embeddings": 2048,
opt-1_3b-multinode [stderr]   "model_type": "opt",
opt-1_3b-multinode [stderr]   "num_attention_heads": 32,
opt-1_3b-multinode [stderr]   "num_hidden_layers": 24,
opt-1_3b-multinode [stderr]   "pad_token_id": 1,
opt-1_3b-multinode [stderr]   "prefix": "</s>",
opt-1_3b-multinode [stderr]   "torch_dtype": "float16",
opt-1_3b-multinode [stderr]   "transformers_version": "4.35.0",
opt-1_3b-multinode [stderr]   "use_cache": true,
opt-1_3b-multinode [stderr]   "vocab_size": 50272,
opt-1_3b-multinode [stderr]   "word_embed_proj_dim": 2048
opt-1_3b-multinode [stderr] }
opt-1_3b-multinode [stderr] 
opt-1_3b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/config.json
opt-1_3b-multinode [stderr] Model config OPTConfig {
opt-1_3b-multinode [stderr]   "_name_or_path": "facebook/opt-1.3b",
opt-1_3b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-1_3b-multinode [stderr]   "activation_dropout": 0.0,
opt-1_3b-multinode [stderr]   "activation_function": "relu",
opt-1_3b-multinode [stderr]   "architectures": [
opt-1_3b-multinode [stderr]     "OPTForCausalLM"
opt-1_3b-multinode [stderr]   ],
opt-1_3b-multinode [stderr]   "attention_dropout": 0.0,
opt-1_3b-multinode [stderr]   "bos_token_id": 2,
opt-1_3b-multinode [stderr]   "do_layer_norm_before": true,
opt-1_3b-multinode [stderr]   "dropout": 0.1,
opt-1_3b-multinode [stderr]   "enable_bias": true,
opt-1_3b-multinode [stderr]   "eos_token_id": 2,
opt-1_3b-multinode [stderr]   "ffn_dim": 8192,
opt-1_3b-multinode [stderr]   "hidden_size": 2048,
opt-1_3b-multinode [stderr]   "init_std": 0.02,
opt-1_3b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-1_3b-multinode [stderr]   "layerdrop": 0.0,
opt-1_3b-multinode [stderr]   "max_position_embeddings": 2048,
opt-1_3b-multinode [stderr]   "model_type": "opt",
opt-1_3b-multinode [stderr]   "num_attention_heads": 32,
opt-1_3b-multinode [stderr]   "num_hidden_layers": 24,
opt-1_3b-multinode [stderr]   "pad_token_id": 1,
opt-1_3b-multinode [stderr]   "prefix": "</s>",
opt-1_3b-multinode [stderr]   "torch_dtype": "float16",
opt-1_3b-multinode [stderr]   "transformers_version": "4.35.0",
opt-1_3b-multinode [stderr]   "use_cache": true,
opt-1_3b-multinode [stderr]   "vocab_size": 50272,
opt-1_3b-multinode [stderr]   "word_embed_proj_dim": 2048
opt-1_3b-multinode [stderr] }
opt-1_3b-multinode [stderr] 
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
opt-1_3b-multinode [stdout] [02/06/24 12:01:38] WARNING  [0/1] __main__ - The tokenizer picked logging.py:60
opt-1_3b-multinode [stdout]                              seems to have a very large
opt-1_3b-multinode [stdout]                              `model_max_length`
opt-1_3b-multinode [stdout]                              (1000000000000000019884624838656).
opt-1_3b-multinode [stdout]                              Picking 1024 instead. You can change
opt-1_3b-multinode [stdout]                              that default value by passing
opt-1_3b-multinode [stdout]                              --block_size xxx.
opt-1_3b-multinode [end] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-1.3b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:01:40.406792]
opt-6_7b [config.system.arch] cuda
opt-6_7b [config.system.sshkey] None
opt-6_7b [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
opt-6_7b [config.system.gpu.capacity] 81920 MiB
opt-6_7b [config.system.self.name] local
opt-6_7b [config.system.self.ip] 127.0.0.1
opt-6_7b [config.system.self.port] 8123
opt-6_7b [config.system.self.user] root
opt-6_7b [config.system.self.main] True
opt-6_7b [config.system.self.hostname] localhost
opt-6_7b [config.system.self.aliaslist] []
opt-6_7b [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
opt-6_7b [config.system.self.local] True
opt-6_7b [config.dirs.base] /Tmp/slurm.4115007.0/base
opt-6_7b [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
opt-6_7b [config.dirs.data] /Tmp/slurm.4115007.0/base/data
opt-6_7b [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
opt-6_7b [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/opt
opt-6_7b [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
opt-6_7b [config.group] opt
opt-6_7b [config.install_group] torch
opt-6_7b [config.install_variant] cuda
opt-6_7b [config.run_name] prepare.2024-02-06_11:57:01.236402
opt-6_7b [config.enabled] True
opt-6_7b [config.capabilities.nodes] 1
opt-6_7b [config.max_duration] 600
opt-6_7b [config.voir.options.stop] 60
opt-6_7b [config.voir.options.interval] 1s
opt-6_7b [config.validation.usage.gpu_load_threshold] 0.5
opt-6_7b [config.validation.usage.gpu_mem_threshold] 0.5
opt-6_7b [config.config_base] /Tmp/slurm.4115007.0/milabench/config
opt-6_7b [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
opt-6_7b [config.tags] ['huggingface', 'language-modeling', 'llm', 'multigpu', 'nlp', 'transformer']
opt-6_7b [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt
opt-6_7b [config.plan.method] njobs
opt-6_7b [config.plan.n] 1
opt-6_7b [config.argv.--max_train_steps] 100
opt-6_7b [config.argv.--dataset_name] wikitext
opt-6_7b [config.argv.--dataset_config_name] wikitext-103-v1
opt-6_7b [config.argv.--dataset_rev] b08601e
opt-6_7b [config.argv.--validation_split_percentage] 5
opt-6_7b [config.argv.--per_gpu_batch_size] 1
opt-6_7b [config.argv.--cpus_per_gpu] 8
opt-6_7b [config.argv.--model_name] facebook/opt-6.7b
opt-6_7b [config.gradient_accumulation_steps] 1
opt-6_7b [config.use_deepspeed] True
opt-6_7b [config.num_machines] 1
opt-6_7b [config.weight] 5.0
opt-6_7b [config.name] opt-6_7b
opt-6_7b [config.tag] ['opt-6_7b']
opt-6_7b [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 81.177,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 34,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707256902.78669,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
opt-6_7b [start] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-6.7b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:01:42.805103]
opt-6_7b [stderr] The following values were not passed to `accelerate launch` and had defaults used instead:
opt-6_7b [stderr] 		More than one GPU was found, enabling multi-GPU training.
opt-6_7b [stderr] 		If this was unintended please pass in `--num_processes=1`.
opt-6_7b [stderr] To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
opt-6_7b [stderr] Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
opt-6_7b [stdout] [02/06/24 12:01:46] INFO     [0/1] __main__ - Distributed          logging.py:60
opt-6_7b [stdout]                              environment: MULTI_GPU  Backend: nccl
opt-6_7b [stdout]                              Num processes: 1
opt-6_7b [stdout]                              Process index: 0
opt-6_7b [stdout]                              Local process index: 0
opt-6_7b [stdout]                              Device: cuda:0
opt-6_7b [stdout] 
opt-6_7b [stdout]                              Mixed precision type: fp16
opt-6_7b [stdout] 
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   table = cls._concat_blocks(blocks, axis=0)
opt-6_7b [stderr] Downloading config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]
opt-6_7b [stderr] Downloading config.json: 100%|██████████| 651/651 [00:00<00:00, 267kB/s]
opt-6_7b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b [stderr] Model config OPTConfig {
opt-6_7b [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b [stderr]   "_remove_final_layer_norm": false,
opt-6_7b [stderr]   "activation_dropout": 0.0,
opt-6_7b [stderr]   "activation_function": "relu",
opt-6_7b [stderr]   "architectures": [
opt-6_7b [stderr]     "OPTForCausalLM"
opt-6_7b [stderr]   ],
opt-6_7b [stderr]   "attention_dropout": 0.0,
opt-6_7b [stderr]   "bos_token_id": 2,
opt-6_7b [stderr]   "do_layer_norm_before": true,
opt-6_7b [stderr]   "dropout": 0.1,
opt-6_7b [stderr]   "enable_bias": true,
opt-6_7b [stderr]   "eos_token_id": 2,
opt-6_7b [stderr]   "ffn_dim": 16384,
opt-6_7b [stderr]   "hidden_size": 4096,
opt-6_7b [stderr]   "init_std": 0.02,
opt-6_7b [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b [stderr]   "layerdrop": 0.0,
opt-6_7b [stderr]   "max_position_embeddings": 2048,
opt-6_7b [stderr]   "model_type": "opt",
opt-6_7b [stderr]   "num_attention_heads": 32,
opt-6_7b [stderr]   "num_hidden_layers": 32,
opt-6_7b [stderr]   "pad_token_id": 1,
opt-6_7b [stderr]   "prefix": "</s>",
opt-6_7b [stderr]   "torch_dtype": "float16",
opt-6_7b [stderr]   "transformers_version": "4.35.0",
opt-6_7b [stderr]   "use_cache": true,
opt-6_7b [stderr]   "vocab_size": 50272,
opt-6_7b [stderr]   "word_embed_proj_dim": 4096
opt-6_7b [stderr] }
opt-6_7b [stderr] 
opt-6_7b [stderr] Downloading tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]
opt-6_7b [stderr] Downloading tokenizer_config.json: 100%|██████████| 685/685 [00:00<00:00, 362kB/s]
opt-6_7b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b [stderr] Model config OPTConfig {
opt-6_7b [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b [stderr]   "_remove_final_layer_norm": false,
opt-6_7b [stderr]   "activation_dropout": 0.0,
opt-6_7b [stderr]   "activation_function": "relu",
opt-6_7b [stderr]   "architectures": [
opt-6_7b [stderr]     "OPTForCausalLM"
opt-6_7b [stderr]   ],
opt-6_7b [stderr]   "attention_dropout": 0.0,
opt-6_7b [stderr]   "bos_token_id": 2,
opt-6_7b [stderr]   "do_layer_norm_before": true,
opt-6_7b [stderr]   "dropout": 0.1,
opt-6_7b [stderr]   "enable_bias": true,
opt-6_7b [stderr]   "eos_token_id": 2,
opt-6_7b [stderr]   "ffn_dim": 16384,
opt-6_7b [stderr]   "hidden_size": 4096,
opt-6_7b [stderr]   "init_std": 0.02,
opt-6_7b [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b [stderr]   "layerdrop": 0.0,
opt-6_7b [stderr]   "max_position_embeddings": 2048,
opt-6_7b [stderr]   "model_type": "opt",
opt-6_7b [stderr]   "num_attention_heads": 32,
opt-6_7b [stderr]   "num_hidden_layers": 32,
opt-6_7b [stderr]   "pad_token_id": 1,
opt-6_7b [stderr]   "prefix": "</s>",
opt-6_7b [stderr]   "torch_dtype": "float16",
opt-6_7b [stderr]   "transformers_version": "4.35.0",
opt-6_7b [stderr]   "use_cache": true,
opt-6_7b [stderr]   "vocab_size": 50272,
opt-6_7b [stderr]   "word_embed_proj_dim": 4096
opt-6_7b [stderr] }
opt-6_7b [stderr] 
opt-6_7b [stderr] Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]
opt-6_7b [stderr] Downloading vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 11.2MB/s]
opt-6_7b [stderr] Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
opt-6_7b [stderr] Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 10.9MB/s]
opt-6_7b [stderr] Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]
opt-6_7b [stderr] Downloading (…)cial_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 1.48MB/s]
opt-6_7b [stderr] loading file vocab.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/vocab.json
opt-6_7b [stderr] loading file merges.txt from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/merges.txt
opt-6_7b [stderr] loading file tokenizer.json from cache at None
opt-6_7b [stderr] loading file added_tokens.json from cache at None
opt-6_7b [stderr] loading file special_tokens_map.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/special_tokens_map.json
opt-6_7b [stderr] loading file tokenizer_config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/tokenizer_config.json
opt-6_7b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b [stderr] Model config OPTConfig {
opt-6_7b [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b [stderr]   "_remove_final_layer_norm": false,
opt-6_7b [stderr]   "activation_dropout": 0.0,
opt-6_7b [stderr]   "activation_function": "relu",
opt-6_7b [stderr]   "architectures": [
opt-6_7b [stderr]     "OPTForCausalLM"
opt-6_7b [stderr]   ],
opt-6_7b [stderr]   "attention_dropout": 0.0,
opt-6_7b [stderr]   "bos_token_id": 2,
opt-6_7b [stderr]   "do_layer_norm_before": true,
opt-6_7b [stderr]   "dropout": 0.1,
opt-6_7b [stderr]   "enable_bias": true,
opt-6_7b [stderr]   "eos_token_id": 2,
opt-6_7b [stderr]   "ffn_dim": 16384,
opt-6_7b [stderr]   "hidden_size": 4096,
opt-6_7b [stderr]   "init_std": 0.02,
opt-6_7b [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b [stderr]   "layerdrop": 0.0,
opt-6_7b [stderr]   "max_position_embeddings": 2048,
opt-6_7b [stderr]   "model_type": "opt",
opt-6_7b [stderr]   "num_attention_heads": 32,
opt-6_7b [stderr]   "num_hidden_layers": 32,
opt-6_7b [stderr]   "pad_token_id": 1,
opt-6_7b [stderr]   "prefix": "</s>",
opt-6_7b [stderr]   "torch_dtype": "float16",
opt-6_7b [stderr]   "transformers_version": "4.35.0",
opt-6_7b [stderr]   "use_cache": true,
opt-6_7b [stderr]   "vocab_size": 50272,
opt-6_7b [stderr]   "word_embed_proj_dim": 4096
opt-6_7b [stderr] }
opt-6_7b [stderr] 
opt-6_7b [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b [stderr] Model config OPTConfig {
opt-6_7b [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b [stderr]   "_remove_final_layer_norm": false,
opt-6_7b [stderr]   "activation_dropout": 0.0,
opt-6_7b [stderr]   "activation_function": "relu",
opt-6_7b [stderr]   "architectures": [
opt-6_7b [stderr]     "OPTForCausalLM"
opt-6_7b [stderr]   ],
opt-6_7b [stderr]   "attention_dropout": 0.0,
opt-6_7b [stderr]   "bos_token_id": 2,
opt-6_7b [stderr]   "do_layer_norm_before": true,
opt-6_7b [stderr]   "dropout": 0.1,
opt-6_7b [stderr]   "enable_bias": true,
opt-6_7b [stderr]   "eos_token_id": 2,
opt-6_7b [stderr]   "ffn_dim": 16384,
opt-6_7b [stderr]   "hidden_size": 4096,
opt-6_7b [stderr]   "init_std": 0.02,
opt-6_7b [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b [stderr]   "layerdrop": 0.0,
opt-6_7b [stderr]   "max_position_embeddings": 2048,
opt-6_7b [stderr]   "model_type": "opt",
opt-6_7b [stderr]   "num_attention_heads": 32,
opt-6_7b [stderr]   "num_hidden_layers": 32,
opt-6_7b [stderr]   "pad_token_id": 1,
opt-6_7b [stderr]   "prefix": "</s>",
opt-6_7b [stderr]   "torch_dtype": "float16",
opt-6_7b [stderr]   "transformers_version": "4.35.0",
opt-6_7b [stderr]   "use_cache": true,
opt-6_7b [stderr]   "vocab_size": 50272,
opt-6_7b [stderr]   "word_embed_proj_dim": 4096
opt-6_7b [stderr] }
opt-6_7b [stderr] 
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/4358 [00:00<?, ? examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 545/4358 [00:00<00:01, 2996.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 1635/4358 [00:00<00:00, 6222.13 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 4358/4358 [00:00<00:00, 12972.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 4358/4358 [00:00<00:00, 9160.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/1801350 [00:00<?, ? examples/s]
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 1000/1801350 [00:00<11:33, 2594.87 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 3000/1801350 [00:00<04:25, 6784.78 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 9000/1801350 [00:00<01:31, 19611.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 12000/1801350 [00:00<01:38, 18256.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 17000/1801350 [00:00<01:11, 25100.13 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   1%|          | 21000/1801350 [00:01<01:24, 20999.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   1%|▏         | 25000/1801350 [00:01<01:13, 24272.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 28000/1801350 [00:01<01:17, 22914.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 33000/1801350 [00:01<01:12, 24310.75 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 36000/1801350 [00:01<01:11, 24843.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 41000/1801350 [00:01<01:07, 26142.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   2%|▏         | 44000/1801350 [00:02<01:09, 25380.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 48000/1801350 [00:02<01:05, 26737.99 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 51000/1801350 [00:02<01:10, 24932.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 55000/1801350 [00:02<01:04, 27256.65 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 58000/1801350 [00:02<01:08, 25502.96 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   3%|▎         | 61000/1801350 [00:02<01:11, 24273.69 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▎         | 65000/1801350 [00:02<01:03, 27177.83 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 68000/1801350 [00:03<01:12, 23879.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 72000/1801350 [00:03<01:02, 27618.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 75000/1801350 [00:03<01:03, 27015.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   4%|▍         | 78000/1801350 [00:03<01:12, 23815.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▍         | 83000/1801350 [00:03<01:04, 26801.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▍         | 86000/1801350 [00:03<01:08, 25147.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▍         | 90000/1801350 [00:03<01:00, 28403.27 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▌         | 93000/1801350 [00:03<01:03, 27070.80 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   5%|▌         | 96000/1801350 [00:04<01:07, 25142.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 100000/1801350 [00:04<01:05, 26134.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 103000/1801350 [00:04<01:03, 26706.86 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 106000/1801350 [00:04<01:05, 25750.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 109000/1801350 [00:04<01:12, 23408.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▌         | 112000/1801350 [00:04<01:10, 23821.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   6%|▋         | 116000/1801350 [00:04<01:01, 27237.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 119000/1801350 [00:04<01:03, 26288.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 122000/1801350 [00:05<01:02, 26926.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 125000/1801350 [00:05<01:00, 27536.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 128000/1801350 [00:05<01:01, 27030.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 131000/1801350 [00:05<01:06, 24947.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   7%|▋         | 135000/1801350 [00:05<01:02, 26787.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 138000/1801350 [00:05<01:06, 24962.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 142000/1801350 [00:05<01:08, 24383.64 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 145000/1801350 [00:06<01:23, 19729.95 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   8%|▊         | 148000/1801350 [00:06<01:47, 15398.65 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▊         | 154000/1801350 [00:06<01:21, 20142.98 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▊         | 157000/1801350 [00:06<01:18, 20930.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 162000/1801350 [00:06<01:15, 21779.39 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 166000/1801350 [00:07<01:06, 24565.04 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   9%|▉         | 170000/1801350 [00:07<01:11, 22864.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  10%|▉         | 176000/1801350 [00:07<01:00, 26869.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  10%|▉         | 179000/1801350 [00:07<01:03, 25616.78 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 182000/1801350 [00:07<01:04, 25252.74 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 186000/1801350 [00:07<00:57, 28067.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  10%|█         | 189000/1801350 [00:07<01:05, 24547.44 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 194000/1801350 [00:08<00:54, 29320.04 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█         | 198000/1801350 [00:08<00:59, 26841.39 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  11%|█▏        | 203000/1801350 [00:08<01:10, 22788.95 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 210000/1801350 [00:08<00:50, 31266.90 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 214000/1801350 [00:08<01:01, 25948.59 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 219000/1801350 [00:09<01:05, 24091.00 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▏        | 224000/1801350 [00:09<00:55, 28414.44 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 228000/1801350 [00:09<01:03, 24833.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 234000/1801350 [00:09<00:57, 27412.57 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 238000/1801350 [00:09<01:01, 25387.96 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  13%|█▎        | 243000/1801350 [00:09<01:03, 24682.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▎        | 247000/1801350 [00:10<00:56, 27364.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 251000/1801350 [00:10<01:08, 22762.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 256000/1801350 [00:10<00:56, 27129.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  14%|█▍        | 260000/1801350 [00:10<01:03, 24387.57 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▍        | 265000/1801350 [00:10<00:56, 27019.80 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▍        | 268000/1801350 [00:10<01:00, 25481.54 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▌        | 272000/1801350 [00:11<00:55, 27657.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▌        | 275000/1801350 [00:11<00:57, 26417.58 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  15%|█▌        | 278000/1801350 [00:11<01:02, 24536.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 283000/1801350 [00:11<00:56, 26843.82 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 286000/1801350 [00:11<01:02, 24307.08 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▌        | 291000/1801350 [00:11<00:53, 28497.10 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  16%|█▋        | 294000/1801350 [00:11<01:01, 24504.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 299000/1801350 [00:12<00:53, 27899.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 302000/1801350 [00:12<00:57, 25911.06 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 305000/1801350 [00:12<00:58, 25479.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 309000/1801350 [00:12<01:01, 24449.38 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  17%|█▋        | 313000/1801350 [00:12<00:56, 26154.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 316000/1801350 [00:12<01:00, 24696.93 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 319000/1801350 [00:12<01:00, 24673.98 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 323000/1801350 [00:13<01:01, 24215.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 327000/1801350 [00:13<00:58, 25180.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  18%|█▊        | 331000/1801350 [00:13<00:56, 25939.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▊        | 335000/1801350 [00:13<00:57, 25576.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 338000/1801350 [00:13<00:55, 26472.45 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 342000/1801350 [00:13<00:51, 28424.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 345000/1801350 [00:13<01:03, 23030.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  19%|█▉        | 349000/1801350 [00:14<00:55, 26322.28 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 352000/1801350 [00:14<01:05, 22059.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 355000/1801350 [00:14<01:19, 18167.55 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  20%|█▉        | 360000/1801350 [00:14<01:01, 23364.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  20%|██        | 363000/1801350 [00:14<01:03, 22571.88 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  20%|██        | 367000/1801350 [00:14<00:57, 24939.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 370000/1801350 [00:15<01:02, 22895.34 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 374000/1801350 [00:15<00:55, 25922.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 378000/1801350 [00:15<00:56, 25108.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██        | 381000/1801350 [00:15<00:57, 24743.24 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  21%|██▏       | 385000/1801350 [00:15<00:50, 27865.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 388000/1801350 [00:15<00:58, 24303.31 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 392000/1801350 [00:15<00:52, 26796.80 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 395000/1801350 [00:16<00:58, 23892.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 399000/1801350 [00:16<00:51, 27404.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  22%|██▏       | 403000/1801350 [00:16<00:54, 25473.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 407000/1801350 [00:16<00:53, 26307.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 411000/1801350 [00:16<00:49, 28209.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 414000/1801350 [00:16<00:51, 26866.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 417000/1801350 [00:16<00:50, 27396.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  23%|██▎       | 420000/1801350 [00:16<00:52, 26390.69 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▎       | 424000/1801350 [00:17<00:50, 27298.02 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 428000/1801350 [00:17<00:46, 29545.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 431000/1801350 [00:17<00:53, 25759.99 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 435000/1801350 [00:17<00:50, 27048.08 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  24%|██▍       | 438000/1801350 [00:17<00:56, 24154.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▍       | 443000/1801350 [00:17<00:49, 27523.96 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▍       | 446000/1801350 [00:17<00:54, 25057.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▍       | 450000/1801350 [00:18<00:52, 25902.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 453000/1801350 [00:18<00:52, 25458.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 457000/1801350 [00:18<00:52, 25789.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 461000/1801350 [00:18<00:52, 25718.55 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 465000/1801350 [00:18<00:50, 26566.00 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 468000/1801350 [00:18<00:52, 25541.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▌       | 472000/1801350 [00:18<00:46, 28486.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  26%|██▋       | 475000/1801350 [00:19<00:50, 26128.55 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 478000/1801350 [00:19<00:51, 25735.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 482000/1801350 [00:19<00:54, 24317.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 487000/1801350 [00:19<00:45, 28874.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 490000/1801350 [00:19<00:50, 26052.53 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  27%|██▋       | 494000/1801350 [00:19<00:45, 28800.60 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 498000/1801350 [00:19<00:47, 27247.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 501000/1801350 [00:19<00:50, 25965.74 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 505000/1801350 [00:20<00:52, 24915.82 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 508000/1801350 [00:20<00:50, 25748.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  28%|██▊       | 511000/1801350 [00:20<00:49, 25903.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▊       | 515000/1801350 [00:20<00:51, 24803.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 519000/1801350 [00:20<00:48, 26637.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 522000/1801350 [00:20<00:47, 26669.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 525000/1801350 [00:20<00:48, 26527.59 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 528000/1801350 [00:21<00:47, 26932.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  29%|██▉       | 531000/1801350 [00:21<00:56, 22337.98 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  30%|██▉       | 535000/1801350 [00:21<00:48, 25893.66 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  30%|██▉       | 538000/1801350 [00:21<00:55, 22601.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  30%|███       | 542000/1801350 [00:21<00:54, 23050.45 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  30%|███       | 545000/1801350 [00:21<00:54, 22905.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  30%|███       | 548000/1801350 [00:22<01:03, 19787.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 551000/1801350 [00:22<01:00, 20628.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 555000/1801350 [00:22<00:53, 23512.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 558000/1801350 [00:22<00:53, 23205.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███       | 561000/1801350 [00:22<00:53, 23344.27 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  31%|███▏      | 564000/1801350 [00:22<00:56, 22074.24 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 568000/1801350 [00:22<00:49, 25129.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 572000/1801350 [00:22<00:50, 24401.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 575000/1801350 [00:23<00:51, 24023.66 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 580000/1801350 [00:23<00:46, 26312.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  32%|███▏      | 585000/1801350 [00:23<00:45, 26803.99 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 589000/1801350 [00:23<00:44, 27039.45 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 592000/1801350 [00:23<00:46, 25823.02 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 596000/1801350 [00:23<00:43, 27598.56 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  33%|███▎      | 600000/1801350 [00:24<00:46, 25894.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▎      | 604000/1801350 [00:24<00:44, 27118.01 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 608000/1801350 [00:24<00:44, 26898.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 612000/1801350 [00:24<00:40, 29560.66 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 616000/1801350 [00:24<00:47, 25210.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  34%|███▍      | 620000/1801350 [00:24<00:43, 27047.43 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▍      | 623000/1801350 [00:24<00:43, 26829.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▍      | 626000/1801350 [00:25<00:48, 24374.85 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▍      | 630000/1801350 [00:25<00:45, 25807.49 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 633000/1801350 [00:25<00:44, 26327.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 636000/1801350 [00:25<00:43, 26502.26 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  35%|███▌      | 639000/1801350 [00:25<00:44, 26002.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 642000/1801350 [00:25<00:45, 25695.28 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 645000/1801350 [00:25<00:46, 24999.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▌      | 648000/1801350 [00:25<00:49, 23475.78 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▋      | 653000/1801350 [00:26<00:42, 27062.85 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  36%|███▋      | 656000/1801350 [00:26<00:43, 26165.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 659000/1801350 [00:26<00:45, 24967.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 663000/1801350 [00:26<00:40, 28171.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 666000/1801350 [00:26<00:45, 25011.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 669000/1801350 [00:26<00:44, 25490.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  37%|███▋      | 673000/1801350 [00:26<00:39, 28836.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 676000/1801350 [00:26<00:46, 23992.34 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 681000/1801350 [00:27<00:41, 27126.73 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 684000/1801350 [00:27<00:45, 24514.31 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  38%|███▊      | 690000/1801350 [00:27<00:36, 30230.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▊      | 694000/1801350 [00:27<00:40, 27455.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▊      | 697000/1801350 [00:27<00:39, 27887.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 700000/1801350 [00:27<00:44, 24853.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 703000/1801350 [00:27<00:43, 25137.98 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 707000/1801350 [00:28<00:42, 25958.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  39%|███▉      | 711000/1801350 [00:28<00:40, 26821.26 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  40%|███▉      | 715000/1801350 [00:28<00:43, 24692.04 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  40%|███▉      | 720000/1801350 [00:28<00:36, 29998.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  40%|████      | 724000/1801350 [00:28<00:44, 24270.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  40%|████      | 728000/1801350 [00:28<00:44, 24388.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 731000/1801350 [00:29<00:48, 21980.86 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 736000/1801350 [00:29<00:41, 25649.14 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 739000/1801350 [00:29<00:45, 23244.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████      | 742000/1801350 [00:29<00:46, 22853.54 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  41%|████▏     | 745000/1801350 [00:29<00:47, 22054.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 748000/1801350 [00:29<00:46, 22771.47 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 751000/1801350 [00:29<00:48, 21533.38 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 755000/1801350 [00:30<00:43, 24216.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 758000/1801350 [00:30<00:47, 22159.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 762000/1801350 [00:30<00:42, 24260.04 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  42%|████▏     | 765000/1801350 [00:30<00:43, 23887.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 769000/1801350 [00:30<00:38, 26635.24 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 773000/1801350 [00:30<00:40, 25399.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 777000/1801350 [00:30<00:40, 25022.00 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  43%|████▎     | 780000/1801350 [00:31<00:41, 24369.38 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▎     | 784000/1801350 [00:31<00:36, 27624.28 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▎     | 787000/1801350 [00:31<00:38, 26444.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 790000/1801350 [00:31<00:41, 24245.13 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 794000/1801350 [00:31<00:36, 27247.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 797000/1801350 [00:31<00:39, 25234.74 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  44%|████▍     | 801000/1801350 [00:31<00:35, 28284.87 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▍     | 805000/1801350 [00:32<00:42, 23612.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▍     | 810000/1801350 [00:32<00:34, 28543.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▌     | 814000/1801350 [00:32<00:40, 24589.00 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  45%|████▌     | 819000/1801350 [00:32<00:38, 25492.93 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 823000/1801350 [00:32<00:37, 25832.73 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 827000/1801350 [00:32<00:37, 25819.37 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▌     | 830000/1801350 [00:33<00:38, 25448.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▋     | 834000/1801350 [00:33<00:34, 28171.65 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  46%|████▋     | 837000/1801350 [00:33<00:37, 25884.75 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 840000/1801350 [00:33<00:38, 25196.56 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 843000/1801350 [00:33<00:37, 25274.08 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 847000/1801350 [00:33<00:36, 26178.55 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 851000/1801350 [00:33<00:34, 27682.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  47%|████▋     | 854000/1801350 [00:33<00:37, 25150.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 858000/1801350 [00:34<00:37, 25250.44 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 862000/1801350 [00:34<00:33, 27946.06 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 865000/1801350 [00:34<00:39, 23717.14 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  48%|████▊     | 870000/1801350 [00:34<00:32, 28957.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▊     | 874000/1801350 [00:34<00:36, 25227.49 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 879000/1801350 [00:34<00:35, 26179.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 882000/1801350 [00:35<00:35, 25626.86 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 887000/1801350 [00:35<00:34, 26356.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  49%|████▉     | 890000/1801350 [00:35<00:35, 25848.34 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  50%|████▉     | 895000/1801350 [00:35<00:37, 24232.04 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  50%|█████     | 902000/1801350 [00:35<00:27, 32245.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  50%|█████     | 906000/1801350 [00:35<00:35, 25201.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 911000/1801350 [00:36<00:36, 24630.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 916000/1801350 [00:36<00:38, 22882.08 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████     | 919000/1801350 [00:36<00:37, 23763.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████▏    | 924000/1801350 [00:36<00:34, 25425.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  51%|█████▏    | 927000/1801350 [00:36<00:36, 23830.54 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 932000/1801350 [00:36<00:31, 27647.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 935000/1801350 [00:37<00:39, 21732.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 938000/1801350 [00:37<00:41, 20799.60 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  52%|█████▏    | 943000/1801350 [00:37<00:34, 24838.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 946000/1801350 [00:37<00:40, 21219.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 950000/1801350 [00:37<00:36, 23575.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 953000/1801350 [00:37<00:38, 21923.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 958000/1801350 [00:38<00:32, 26132.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  53%|█████▎    | 961000/1801350 [00:38<00:34, 24122.26 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▎    | 964000/1801350 [00:38<00:34, 24406.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▎    | 967000/1801350 [00:38<00:34, 24174.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 970000/1801350 [00:38<00:33, 24826.83 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 973000/1801350 [00:38<00:33, 24804.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 976000/1801350 [00:38<00:31, 25995.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  54%|█████▍    | 979000/1801350 [00:38<00:33, 24907.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▍    | 983000/1801350 [00:39<00:34, 23640.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▍    | 987000/1801350 [00:39<00:31, 25751.38 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 991000/1801350 [00:39<00:31, 25968.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 995000/1801350 [00:39<00:29, 26910.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  55%|█████▌    | 999000/1801350 [00:39<00:31, 25175.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1003000/1801350 [00:39<00:31, 25609.24 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1006000/1801350 [00:40<00:32, 24173.13 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▌    | 1011000/1801350 [00:40<00:29, 26824.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  56%|█████▋    | 1014000/1801350 [00:40<00:32, 24454.37 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1018000/1801350 [00:40<00:29, 26131.98 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1021000/1801350 [00:40<00:29, 26022.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1024000/1801350 [00:40<00:29, 26063.03 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1029000/1801350 [00:40<00:28, 27330.14 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  57%|█████▋    | 1032000/1801350 [00:41<00:32, 23905.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1036000/1801350 [00:41<00:27, 27369.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1039000/1801350 [00:41<00:30, 25122.10 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1043000/1801350 [00:41<00:27, 27969.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1046000/1801350 [00:41<00:29, 25573.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  58%|█████▊    | 1049000/1801350 [00:41<00:29, 25595.73 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▊    | 1054000/1801350 [00:41<00:30, 24805.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1059000/1801350 [00:42<00:25, 28883.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1063000/1801350 [00:42<00:27, 26430.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1067000/1801350 [00:42<00:26, 28185.91 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  59%|█████▉    | 1070000/1801350 [00:42<00:26, 27533.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  60%|█████▉    | 1073000/1801350 [00:42<00:26, 27211.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  60%|█████▉    | 1076000/1801350 [00:42<00:27, 26128.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  60%|█████▉    | 1080000/1801350 [00:42<00:26, 27198.31 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  60%|██████    | 1083000/1801350 [00:42<00:28, 25357.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  60%|██████    | 1087000/1801350 [00:43<00:26, 27130.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1091000/1801350 [00:43<00:27, 25980.09 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1094000/1801350 [00:43<00:26, 26353.84 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1097000/1801350 [00:43<00:28, 24554.99 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████    | 1101000/1801350 [00:43<00:31, 22571.93 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  61%|██████▏   | 1104000/1801350 [00:43<00:29, 23511.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1108000/1801350 [00:43<00:27, 24862.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1111000/1801350 [00:44<00:28, 24374.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1115000/1801350 [00:44<00:24, 27611.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1118000/1801350 [00:44<00:27, 24814.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 1122000/1801350 [00:44<00:27, 24882.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1126000/1801350 [00:44<00:28, 24095.86 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1129000/1801350 [00:44<00:27, 24737.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1133000/1801350 [00:45<00:30, 22178.85 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1137000/1801350 [00:45<00:25, 25798.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1140000/1801350 [00:45<00:25, 26398.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  63%|██████▎   | 1143000/1801350 [00:45<00:28, 23387.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▎   | 1147000/1801350 [00:45<00:25, 25370.12 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1150000/1801350 [00:45<00:27, 24067.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1153000/1801350 [00:45<00:26, 24656.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1156000/1801350 [00:45<00:26, 24703.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  64%|██████▍   | 1159000/1801350 [00:46<00:25, 25500.55 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1162000/1801350 [00:46<00:25, 25479.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1165000/1801350 [00:46<00:23, 26522.49 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▍   | 1168000/1801350 [00:46<00:28, 22344.84 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▌   | 1174000/1801350 [00:46<00:21, 29001.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  65%|██████▌   | 1178000/1801350 [00:46<00:26, 23590.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▌   | 1184000/1801350 [00:47<00:24, 24937.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▌   | 1188000/1801350 [00:47<00:23, 26648.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▌   | 1192000/1801350 [00:47<00:23, 25633.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  66%|██████▋   | 1195000/1801350 [00:47<00:23, 26165.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1200000/1801350 [00:47<00:24, 25029.87 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1203000/1801350 [00:47<00:23, 25092.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1208000/1801350 [00:47<00:22, 25945.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1211000/1801350 [00:48<00:22, 26146.45 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  67%|██████▋   | 1215000/1801350 [00:48<00:20, 28977.82 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1219000/1801350 [00:48<00:21, 26580.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1222000/1801350 [00:48<00:22, 25604.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1226000/1801350 [00:48<00:20, 28625.08 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1229000/1801350 [00:48<00:20, 27774.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  68%|██████▊   | 1232000/1801350 [00:48<00:21, 26791.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▊   | 1235000/1801350 [00:48<00:21, 26759.50 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▊   | 1238000/1801350 [00:49<00:22, 25577.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1241000/1801350 [00:49<00:22, 24869.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1246000/1801350 [00:49<00:21, 25752.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  69%|██████▉   | 1249000/1801350 [00:49<00:21, 25867.22 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  70%|██████▉   | 1254000/1801350 [00:49<00:20, 26093.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  70%|██████▉   | 1257000/1801350 [00:49<00:21, 25866.72 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1262000/1801350 [00:49<00:19, 28133.79 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1265000/1801350 [00:50<00:20, 25939.97 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  70%|███████   | 1269000/1801350 [00:50<00:19, 27464.69 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1272000/1801350 [00:50<00:20, 26288.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1276000/1801350 [00:50<00:20, 26178.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████   | 1280000/1801350 [00:50<00:19, 26092.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  71%|███████▏  | 1284000/1801350 [00:50<00:19, 26567.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1288000/1801350 [00:50<00:18, 27749.23 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1291000/1801350 [00:51<00:20, 24408.15 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1294000/1801350 [00:51<00:21, 23212.58 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1298000/1801350 [00:51<00:18, 26932.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1301000/1801350 [00:51<00:21, 23490.85 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  72%|███████▏  | 1305000/1801350 [00:51<00:18, 26909.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1308000/1801350 [00:51<00:20, 24620.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1311000/1801350 [00:51<00:19, 25239.97 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1314000/1801350 [00:52<00:19, 25257.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1317000/1801350 [00:52<00:18, 25931.38 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  73%|███████▎  | 1320000/1801350 [00:52<00:19, 24308.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▎  | 1325000/1801350 [00:52<00:18, 25819.17 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▎  | 1328000/1801350 [00:52<00:23, 20089.27 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1333000/1801350 [00:52<00:21, 21544.24 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1336000/1801350 [00:52<00:20, 23043.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  74%|███████▍  | 1340000/1801350 [00:53<00:21, 21112.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 1345000/1801350 [00:53<00:17, 26423.84 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 1349000/1801350 [00:53<00:18, 24266.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 1353000/1801350 [00:53<00:16, 27072.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 1357000/1801350 [00:53<00:18, 24441.36 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1362000/1801350 [00:54<00:18, 23255.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1368000/1801350 [00:54<00:14, 29421.22 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▌  | 1372000/1801350 [00:54<00:17, 23982.75 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  76%|███████▋  | 1378000/1801350 [00:54<00:15, 26707.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1381000/1801350 [00:54<00:16, 25831.57 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1386000/1801350 [00:54<00:15, 26158.64 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1389000/1801350 [00:55<00:16, 25667.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1393000/1801350 [00:55<00:14, 28335.56 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  77%|███████▋  | 1396000/1801350 [00:55<00:16, 24911.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1401000/1801350 [00:55<00:15, 25882.73 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1404000/1801350 [00:55<00:16, 24485.53 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1408000/1801350 [00:55<00:14, 26311.72 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1411000/1801350 [00:55<00:14, 26932.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  78%|███████▊  | 1414000/1801350 [00:55<00:15, 25704.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▊  | 1417000/1801350 [00:56<00:15, 24963.53 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1421000/1801350 [00:56<00:15, 24165.34 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1425000/1801350 [00:56<00:13, 27008.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1428000/1801350 [00:56<00:14, 26125.27 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  79%|███████▉  | 1431000/1801350 [00:56<00:14, 25429.83 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  80%|███████▉  | 1435000/1801350 [00:56<00:13, 28122.69 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  80%|███████▉  | 1438000/1801350 [00:56<00:15, 23178.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  80%|████████  | 1442000/1801350 [00:57<00:14, 25153.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  80%|████████  | 1446000/1801350 [00:57<00:14, 23873.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  80%|████████  | 1450000/1801350 [00:57<00:12, 27240.29 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1453000/1801350 [00:57<00:14, 23837.01 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1457000/1801350 [00:57<00:12, 27404.91 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████  | 1460000/1801350 [00:57<00:14, 23430.78 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████▏ | 1465000/1801350 [00:57<00:12, 27356.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  81%|████████▏ | 1468000/1801350 [00:58<00:12, 26259.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1472000/1801350 [00:58<00:12, 25635.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1476000/1801350 [00:58<00:12, 25962.45 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1479000/1801350 [00:58<00:12, 26494.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1482000/1801350 [00:58<00:12, 25306.19 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  82%|████████▏ | 1486000/1801350 [00:58<00:12, 24948.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1489000/1801350 [00:59<00:15, 20694.31 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1494000/1801350 [00:59<00:13, 23324.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1497000/1801350 [00:59<00:12, 24489.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1500000/1801350 [00:59<00:11, 25334.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  83%|████████▎ | 1503000/1801350 [00:59<00:11, 26119.59 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▎ | 1506000/1801350 [00:59<00:13, 22075.35 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1510000/1801350 [00:59<00:11, 25472.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1513000/1801350 [00:59<00:11, 24041.78 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1516000/1801350 [01:00<00:13, 21871.70 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  84%|████████▍ | 1519000/1801350 [01:00<00:12, 22687.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1523000/1801350 [01:00<00:11, 23719.93 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1526000/1801350 [01:00<00:11, 24037.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▍ | 1530000/1801350 [01:00<00:10, 26971.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▌ | 1533000/1801350 [01:00<00:11, 23146.92 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  85%|████████▌ | 1538000/1801350 [01:00<00:10, 25323.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1541000/1801350 [01:01<00:10, 24510.33 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1545000/1801350 [01:01<00:09, 27702.22 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1548000/1801350 [01:01<00:10, 24654.91 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▌ | 1552000/1801350 [01:01<00:09, 24961.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  86%|████████▋ | 1555000/1801350 [01:01<00:09, 24972.99 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1559000/1801350 [01:01<00:08, 28262.21 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1562000/1801350 [01:01<00:08, 26836.72 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1565000/1801350 [01:02<00:09, 25297.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1568000/1801350 [01:02<00:09, 24181.64 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1572000/1801350 [01:02<00:08, 27258.09 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 1575000/1801350 [01:02<00:08, 27326.39 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1578000/1801350 [01:02<00:08, 27043.96 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1581000/1801350 [01:02<00:08, 25100.30 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1586000/1801350 [01:02<00:08, 26403.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1589000/1801350 [01:02<00:08, 26290.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 1594000/1801350 [01:03<00:07, 28439.62 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▊ | 1597000/1801350 [01:03<00:07, 26395.05 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1601000/1801350 [01:03<00:07, 26701.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1604000/1801350 [01:03<00:07, 25900.02 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1609000/1801350 [01:03<00:06, 28161.76 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  89%|████████▉ | 1612000/1801350 [01:03<00:08, 23377.46 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  90%|████████▉ | 1618000/1801350 [01:04<00:06, 28033.60 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  90%|████████▉ | 1621000/1801350 [01:04<00:07, 25623.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  90%|█████████ | 1626000/1801350 [01:04<00:06, 27469.32 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  90%|█████████ | 1629000/1801350 [01:04<00:07, 23833.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1634000/1801350 [01:04<00:05, 28311.10 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1637000/1801350 [01:04<00:06, 23702.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████ | 1643000/1801350 [01:05<00:06, 25416.64 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  91%|█████████▏| 1646000/1801350 [01:05<00:05, 26188.88 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1651000/1801350 [01:05<00:05, 25115.89 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1654000/1801350 [01:05<00:05, 25327.43 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1659000/1801350 [01:05<00:05, 28049.06 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  92%|█████████▏| 1662000/1801350 [01:05<00:05, 24955.61 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1667000/1801350 [01:05<00:04, 29417.18 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1671000/1801350 [01:06<00:04, 26467.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1674000/1801350 [01:06<00:05, 23694.44 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1679000/1801350 [01:06<00:05, 24011.11 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  93%|█████████▎| 1682000/1801350 [01:06<00:05, 23042.77 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▎| 1686000/1801350 [01:06<00:04, 25444.13 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1689000/1801350 [01:06<00:05, 22456.80 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1693000/1801350 [01:07<00:04, 25320.25 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1696000/1801350 [01:07<00:04, 23150.73 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  94%|█████████▍| 1701000/1801350 [01:07<00:03, 26169.22 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▍| 1704000/1801350 [01:07<00:04, 23753.14 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▍| 1708000/1801350 [01:07<00:04, 23282.87 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▍| 1711000/1801350 [01:07<00:04, 22383.26 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▌| 1716000/1801350 [01:07<00:03, 27547.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  95%|█████████▌| 1719000/1801350 [01:08<00:03, 22637.14 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1723000/1801350 [01:08<00:03, 23568.60 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1726000/1801350 [01:08<00:03, 23526.81 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1729000/1801350 [01:08<00:03, 23665.52 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▌| 1733000/1801350 [01:08<00:02, 24515.16 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  96%|█████████▋| 1736000/1801350 [01:08<00:02, 24894.28 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1740000/1801350 [01:08<00:02, 26547.71 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1744000/1801350 [01:09<00:02, 26184.67 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1747000/1801350 [01:09<00:02, 26992.39 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1750000/1801350 [01:09<00:01, 27405.40 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1753000/1801350 [01:09<00:01, 27636.06 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  97%|█████████▋| 1756000/1801350 [01:09<00:01, 26379.07 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1759000/1801350 [01:09<00:01, 25055.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1764000/1801350 [01:09<00:01, 26954.20 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1767000/1801350 [01:09<00:01, 26240.01 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  98%|█████████▊| 1771169/1801350 [01:10<00:01, 26919.54 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▊| 1775169/1801350 [01:10<00:00, 27836.31 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▊| 1778338/1801350 [01:10<00:00, 27970.95 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1781507/1801350 [01:10<00:00, 26391.42 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1785507/1801350 [01:10<00:00, 28006.63 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1788507/1801350 [01:10<00:00, 28075.51 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  99%|█████████▉| 1791507/1801350 [01:10<00:00, 27445.82 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1794507/1801350 [01:10<00:00, 28013.94 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1797676/1801350 [01:11<00:00, 27884.93 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|█████████▉| 1801181/1801350 [01:11<00:00, 25747.16 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 1801350/1801350 [01:11<00:00, 25269.00 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):   0%|          | 0/3760 [00:00<?, ? examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  12%|█▎        | 470/3760 [00:00<00:01, 3245.16 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8):  25%|██▌       | 940/3760 [00:00<00:00, 3620.41 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 3760/3760 [00:00<00:00, 12060.48 examples/s]
opt-6_7b [stderr] Running tokenizer on dataset (num_proc=8): 100%|██████████| 3760/3760 [00:00<00:00, 8344.21 examples/s]
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stderr] libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b [stdout] [02/06/24 12:03:01] WARNING  [0/1] __main__ - The tokenizer picked logging.py:60
opt-6_7b [stdout]                              seems to have a very large
opt-6_7b [stdout]                              `model_max_length`
opt-6_7b [stdout]                              (1000000000000000019884624838656).
opt-6_7b [stdout]                              Picking 1024 instead. You can change
opt-6_7b [stdout]                              that default value by passing
opt-6_7b [stdout]                              --block_size xxx.
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/4358 [00:00<?, ? examples/s]
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  50%|█████     | 2180/4358 [00:00<00:00, 21067.87 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 4358/4358 [00:00<00:00, 22510.17 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/1801350 [00:00<?, ? examples/s]
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 1000/1801350 [00:00<04:18, 6970.40 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   1%|          | 10000/1801350 [00:00<00:40, 44759.78 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   1%|          | 17000/1801350 [00:00<00:33, 53596.76 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   1%|▏         | 24000/1801350 [00:00<00:30, 58620.33 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   2%|▏         | 33000/1801350 [00:00<00:27, 64584.14 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   2%|▏         | 41000/1801350 [00:00<00:25, 67941.91 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   3%|▎         | 50000/1801350 [00:00<00:24, 71621.65 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   3%|▎         | 58000/1801350 [00:00<00:24, 72213.79 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   4%|▎         | 67000/1801350 [00:01<00:23, 73355.06 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   4%|▍         | 75000/1801350 [00:01<00:24, 71495.27 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   5%|▍         | 85000/1801350 [00:01<00:23, 73410.00 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   5%|▌         | 93000/1801350 [00:01<00:24, 70787.55 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   6%|▌         | 101000/1801350 [00:01<00:24, 69943.51 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   6%|▌         | 110000/1801350 [00:01<00:23, 72500.10 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   7%|▋         | 118000/1801350 [00:01<00:23, 70629.82 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   7%|▋         | 126000/1801350 [00:01<00:23, 71563.58 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   7%|▋         | 134000/1801350 [00:01<00:22, 72533.78 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   8%|▊         | 143000/1801350 [00:02<00:22, 75274.49 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   8%|▊         | 151000/1801350 [00:02<00:22, 72154.82 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   9%|▉         | 159000/1801350 [00:02<00:23, 71011.65 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   9%|▉         | 167000/1801350 [00:02<00:22, 71225.66 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  10%|▉         | 175000/1801350 [00:02<00:23, 70230.40 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  10%|█         | 183000/1801350 [00:02<00:22, 72861.06 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  11%|█         | 191000/1801350 [00:02<00:22, 71289.27 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  11%|█         | 199000/1801350 [00:02<00:22, 71860.09 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  12%|█▏        | 208000/1801350 [00:03<00:21, 72537.73 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  12%|█▏        | 217000/1801350 [00:03<00:21, 73985.67 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  13%|█▎        | 226000/1801350 [00:03<00:20, 77918.30 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  13%|█▎        | 234000/1801350 [00:03<00:20, 75043.48 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  13%|█▎        | 242000/1801350 [00:03<00:21, 71891.32 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  14%|█▍        | 250000/1801350 [00:03<00:21, 71522.51 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  14%|█▍        | 258000/1801350 [00:03<00:21, 70662.14 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  15%|█▍        | 266000/1801350 [00:03<00:22, 69595.65 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  15%|█▌        | 274000/1801350 [00:03<00:21, 72326.64 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  16%|█▌        | 282000/1801350 [00:04<00:21, 69151.48 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  16%|█▌        | 290000/1801350 [00:04<00:21, 71738.32 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  17%|█▋        | 298000/1801350 [00:04<00:21, 69164.83 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  17%|█▋        | 306000/1801350 [00:04<00:21, 70564.55 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  17%|█▋        | 315000/1801350 [00:04<00:20, 71106.79 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  18%|█▊        | 323000/1801350 [00:04<00:20, 72277.67 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  18%|█▊        | 331000/1801350 [00:04<00:21, 69146.47 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  19%|█▉        | 339000/1801350 [00:04<00:20, 71444.02 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  19%|█▉        | 347000/1801350 [00:04<00:20, 72056.70 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  20%|█▉        | 355000/1801350 [00:05<00:20, 70075.84 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  20%|██        | 364000/1801350 [00:05<00:19, 74933.70 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  21%|██        | 372000/1801350 [00:05<00:19, 73822.21 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  21%|██        | 380000/1801350 [00:05<00:19, 72120.65 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  22%|██▏       | 388000/1801350 [00:05<00:19, 72596.78 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  22%|██▏       | 397000/1801350 [00:05<00:19, 71169.15 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  22%|██▏       | 405000/1801350 [00:05<00:19, 72113.95 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  23%|██▎       | 413000/1801350 [00:05<00:18, 73414.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  23%|██▎       | 421000/1801350 [00:05<00:18, 73276.37 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  24%|██▍       | 429000/1801350 [00:06<00:18, 72359.13 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  24%|██▍       | 437000/1801350 [00:06<00:18, 72749.69 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  25%|██▍       | 445000/1801350 [00:06<00:18, 73947.95 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  25%|██▌       | 453000/1801350 [00:06<00:19, 70292.64 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  26%|██▌       | 461000/1801350 [00:06<00:18, 71392.13 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  26%|██▌       | 470000/1801350 [00:06<00:18, 71416.03 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  27%|██▋       | 479000/1801350 [00:06<00:17, 75445.06 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  27%|██▋       | 487000/1801350 [00:06<00:18, 72197.12 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  28%|██▊       | 496000/1801350 [00:06<00:18, 72513.51 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  28%|██▊       | 505000/1801350 [00:07<00:17, 72726.20 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  28%|██▊       | 513000/1801350 [00:07<00:17, 73841.35 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  29%|██▉       | 521000/1801350 [00:07<00:17, 74169.56 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  29%|██▉       | 529000/1801350 [00:07<00:18, 70018.31 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  30%|██▉       | 538000/1801350 [00:07<00:18, 69835.56 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  30%|███       | 546000/1801350 [00:07<00:17, 71816.97 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  31%|███       | 555000/1801350 [00:07<00:16, 73595.88 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  31%|███▏      | 564000/1801350 [00:07<00:16, 74085.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  32%|███▏      | 572000/1801350 [00:08<00:16, 72899.11 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  32%|███▏      | 580000/1801350 [00:08<00:16, 73924.33 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  33%|███▎      | 588000/1801350 [00:08<00:16, 72151.58 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  33%|███▎      | 597000/1801350 [00:08<00:17, 67894.13 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  34%|███▎      | 605000/1801350 [00:08<00:17, 68386.81 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  34%|███▍      | 613000/1801350 [00:08<00:16, 70520.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  34%|███▍      | 621000/1801350 [00:08<00:16, 72601.36 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  35%|███▍      | 629000/1801350 [00:08<00:16, 72296.40 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  35%|███▌      | 638000/1801350 [00:08<00:16, 71809.91 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  36%|███▌      | 647000/1801350 [00:09<00:15, 72675.39 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  36%|███▋      | 655000/1801350 [00:09<00:15, 74395.14 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  37%|███▋      | 663000/1801350 [00:09<00:15, 74697.60 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  37%|███▋      | 671000/1801350 [00:09<00:15, 74440.89 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  38%|███▊      | 679000/1801350 [00:09<00:16, 70128.48 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  38%|███▊      | 687000/1801350 [00:09<00:15, 70913.63 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  39%|███▊      | 695000/1801350 [00:09<00:15, 70221.77 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  39%|███▉      | 703000/1801350 [00:09<00:15, 70197.37 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  40%|███▉      | 712000/1801350 [00:09<00:14, 74143.86 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  40%|███▉      | 720000/1801350 [00:10<00:15, 71717.96 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  40%|████      | 728000/1801350 [00:10<00:15, 71296.47 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  41%|████      | 737000/1801350 [00:10<00:14, 74192.71 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  41%|████▏     | 745000/1801350 [00:10<00:14, 72227.84 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  42%|████▏     | 753000/1801350 [00:10<00:14, 70980.30 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  42%|████▏     | 761000/1801350 [00:10<00:15, 69323.39 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  43%|████▎     | 769000/1801350 [00:10<00:14, 69130.30 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  43%|████▎     | 777000/1801350 [00:10<00:14, 70886.36 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  44%|████▎     | 786000/1801350 [00:11<00:13, 74842.92 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  44%|████▍     | 794000/1801350 [00:11<00:13, 72848.54 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  45%|████▍     | 802000/1801350 [00:11<00:13, 73946.00 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  45%|████▍     | 810000/1801350 [00:11<00:13, 74487.85 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  45%|████▌     | 818000/1801350 [00:11<00:13, 71791.20 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  46%|████▌     | 826000/1801350 [00:11<00:13, 70517.35 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  46%|████▋     | 834000/1801350 [00:11<00:13, 72141.90 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  47%|████▋     | 843000/1801350 [00:11<00:13, 69504.95 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  47%|████▋     | 851000/1801350 [00:11<00:13, 71518.98 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  48%|████▊     | 859000/1801350 [00:12<00:12, 73069.34 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  48%|████▊     | 867000/1801350 [00:12<00:12, 72303.89 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  49%|████▊     | 875000/1801350 [00:12<00:12, 71625.09 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  49%|████▉     | 883000/1801350 [00:12<00:12, 73390.50 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  49%|████▉     | 891000/1801350 [00:12<00:12, 74127.04 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  50%|████▉     | 899000/1801350 [00:12<00:12, 74440.05 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  50%|█████     | 907000/1801350 [00:12<00:12, 73431.61 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  51%|█████     | 915000/1801350 [00:12<00:12, 73326.26 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  51%|█████     | 923000/1801350 [00:12<00:11, 73328.50 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  52%|█████▏    | 931000/1801350 [00:13<00:12, 70346.72 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  52%|█████▏    | 940000/1801350 [00:13<00:11, 75641.88 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  53%|█████▎    | 948000/1801350 [00:13<00:12, 69975.88 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  53%|█████▎    | 956000/1801350 [00:13<00:12, 70403.76 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▎    | 964000/1801350 [00:13<00:11, 70507.81 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▍    | 972000/1801350 [00:13<00:11, 69629.82 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  54%|█████▍    | 980000/1801350 [00:13<00:11, 69545.31 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  55%|█████▍    | 987000/1801350 [00:13<00:11, 68557.21 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  55%|█████▌    | 996000/1801350 [00:13<00:11, 68905.27 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  56%|█████▌    | 1004000/1801350 [00:14<00:11, 67157.91 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  56%|█████▌    | 1013000/1801350 [00:14<00:11, 71554.75 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  57%|█████▋    | 1021000/1801350 [00:14<00:10, 72540.42 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  57%|█████▋    | 1029000/1801350 [00:14<00:10, 72441.83 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  58%|█████▊    | 1037000/1801350 [00:14<00:10, 70074.43 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  58%|█████▊    | 1046000/1801350 [00:14<00:10, 74314.04 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  59%|█████▊    | 1054000/1801350 [00:14<00:10, 72356.74 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  59%|█████▉    | 1063000/1801350 [00:14<00:09, 77016.74 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  59%|█████▉    | 1071000/1801350 [00:14<00:09, 77028.34 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  60%|█████▉    | 1079000/1801350 [00:15<00:09, 73673.71 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  60%|██████    | 1087000/1801350 [00:15<00:09, 73132.83 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  61%|██████    | 1095000/1801350 [00:15<00:09, 70774.47 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  61%|██████    | 1103000/1801350 [00:15<00:09, 71592.54 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  62%|██████▏   | 1112000/1801350 [00:15<00:09, 71814.30 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  62%|██████▏   | 1121000/1801350 [00:15<00:09, 74843.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  63%|██████▎   | 1129000/1801350 [00:15<00:09, 73843.16 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  63%|██████▎   | 1137000/1801350 [00:15<00:08, 74384.77 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  64%|██████▎   | 1145000/1801350 [00:15<00:08, 73383.60 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  64%|██████▍   | 1153000/1801350 [00:16<00:09, 71302.50 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▍   | 1162000/1801350 [00:16<00:08, 75148.03 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▍   | 1170000/1801350 [00:16<00:08, 75249.95 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  65%|██████▌   | 1178000/1801350 [00:16<00:08, 74168.59 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  66%|██████▌   | 1186000/1801350 [00:16<00:08, 69926.60 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  66%|██████▋   | 1194000/1801350 [00:16<00:08, 69549.28 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  67%|██████▋   | 1201000/1801350 [00:16<00:08, 69005.18 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  67%|██████▋   | 1208000/1801350 [00:16<00:08, 68534.93 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1217000/1801350 [00:16<00:07, 74361.81 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1225000/1801350 [00:17<00:07, 72692.96 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  68%|██████▊   | 1233000/1801350 [00:17<00:07, 72154.18 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  69%|██████▉   | 1241000/1801350 [00:17<00:07, 71913.93 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  69%|██████▉   | 1249000/1801350 [00:17<00:07, 70350.11 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  70%|██████▉   | 1257000/1801350 [00:17<00:07, 71108.22 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  70%|███████   | 1265000/1801350 [00:17<00:07, 71376.98 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  71%|███████   | 1273000/1801350 [00:17<00:07, 73570.97 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  71%|███████   | 1281000/1801350 [00:17<00:07, 74267.99 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  72%|███████▏  | 1289000/1801350 [00:17<00:07, 72698.89 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  72%|███████▏  | 1297000/1801350 [00:18<00:06, 74360.65 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  72%|███████▏  | 1305000/1801350 [00:18<00:06, 71759.07 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  73%|███████▎  | 1314000/1801350 [00:18<00:06, 72456.79 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  73%|███████▎  | 1322000/1801350 [00:18<00:06, 72571.54 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  74%|███████▍  | 1330000/1801350 [00:18<00:06, 71673.49 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  74%|███████▍  | 1338000/1801350 [00:18<00:06, 69298.11 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  75%|███████▍  | 1347000/1801350 [00:18<00:06, 73017.32 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  75%|███████▌  | 1355000/1801350 [00:18<00:06, 73460.02 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  76%|███████▌  | 1363000/1801350 [00:19<00:06, 72880.69 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  76%|███████▌  | 1372000/1801350 [00:19<00:05, 75187.16 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  77%|███████▋  | 1380000/1801350 [00:19<00:05, 75896.96 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  77%|███████▋  | 1388000/1801350 [00:19<00:05, 70270.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  78%|███████▊  | 1397000/1801350 [00:19<00:05, 68992.78 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  78%|███████▊  | 1406000/1801350 [00:19<00:05, 72047.13 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  78%|███████▊  | 1414000/1801350 [00:19<00:05, 70006.99 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  79%|███████▉  | 1422000/1801350 [00:19<00:05, 69274.02 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  79%|███████▉  | 1431000/1801350 [00:19<00:04, 74189.63 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  80%|███████▉  | 1439000/1801350 [00:20<00:04, 73573.59 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  80%|████████  | 1448000/1801350 [00:20<00:04, 71046.86 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  81%|████████  | 1456000/1801350 [00:20<00:05, 68194.70 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  81%|████████▏ | 1464000/1801350 [00:20<00:04, 70516.73 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  82%|████████▏ | 1472000/1801350 [00:20<00:04, 72421.72 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  82%|████████▏ | 1481000/1801350 [00:20<00:04, 75124.74 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  83%|████████▎ | 1489000/1801350 [00:20<00:04, 70451.94 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  83%|████████▎ | 1497000/1801350 [00:20<00:04, 72888.64 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  84%|████████▎ | 1505000/1801350 [00:21<00:04, 71376.47 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  84%|████████▍ | 1513000/1801350 [00:21<00:03, 73465.32 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  84%|████████▍ | 1521000/1801350 [00:21<00:03, 71070.30 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  85%|████████▍ | 1530000/1801350 [00:21<00:03, 73521.23 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  85%|████████▌ | 1538000/1801350 [00:21<00:03, 72770.01 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  86%|████████▌ | 1548000/1801350 [00:21<00:03, 76682.89 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  86%|████████▋ | 1556000/1801350 [00:21<00:03, 75938.91 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  87%|████████▋ | 1564000/1801350 [00:21<00:03, 70021.73 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  87%|████████▋ | 1572000/1801350 [00:21<00:03, 72447.93 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  88%|████████▊ | 1581000/1801350 [00:22<00:02, 75468.49 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  88%|████████▊ | 1589000/1801350 [00:22<00:02, 72817.36 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  89%|████████▊ | 1597000/1801350 [00:22<00:02, 72346.21 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  89%|████████▉ | 1605000/1801350 [00:22<00:02, 73016.36 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  90%|████████▉ | 1614000/1801350 [00:22<00:02, 76561.39 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  90%|█████████ | 1622000/1801350 [00:22<00:02, 69554.87 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  91%|█████████ | 1631000/1801350 [00:22<00:02, 74731.80 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  91%|█████████ | 1639000/1801350 [00:22<00:02, 74278.48 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  91%|█████████▏| 1647000/1801350 [00:22<00:02, 72033.48 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  92%|█████████▏| 1655000/1801350 [00:23<00:02, 69181.50 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  92%|█████████▏| 1662000/1801350 [00:23<00:02, 67623.29 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  93%|█████████▎| 1669000/1801350 [00:23<00:02, 64907.36 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  93%|█████████▎| 1676000/1801350 [00:23<00:01, 64376.34 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  94%|█████████▎| 1686000/1801350 [00:23<00:01, 71240.40 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  94%|█████████▍| 1694000/1801350 [00:23<00:01, 72734.24 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  94%|█████████▍| 1702000/1801350 [00:23<00:01, 72741.69 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  95%|█████████▌| 1712000/1801350 [00:23<00:01, 70674.78 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▌| 1721000/1801350 [00:23<00:01, 73778.07 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▌| 1729000/1801350 [00:24<00:00, 74672.42 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  96%|█████████▋| 1737000/1801350 [00:24<00:00, 74040.54 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  97%|█████████▋| 1745000/1801350 [00:24<00:00, 71104.33 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  97%|█████████▋| 1754000/1801350 [00:24<00:00, 71374.04 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  98%|█████████▊| 1761169/1801350 [00:24<00:00, 68602.76 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  98%|█████████▊| 1770169/1801350 [00:24<00:00, 74145.76 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  99%|█████████▊| 1778169/1801350 [00:24<00:00, 74885.03 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  99%|█████████▉| 1786676/1801350 [00:24<00:00, 69458.61 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|█████████▉| 1793844/1801350 [00:25<00:00, 64413.38 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|█████████▉| 1801181/1801350 [00:25<00:00, 65444.02 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 1801350/1801350 [00:25<00:00, 71254.50 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):   0%|          | 0/3760 [00:00<?, ? examples/s]
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b [stderr]   return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8):  25%|██▌       | 940/3760 [00:00<00:00, 8514.09 examples/s]
opt-6_7b [stderr] Grouping texts in chunks of 1024 (num_proc=8): 100%|██████████| 3760/3760 [00:00<00:00, 18531.43 examples/s]
opt-6_7b [end] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-6.7b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:03:29.637320]
opt-6_7b-multinode [config.system.arch] cuda
opt-6_7b-multinode [config.system.sshkey] None
opt-6_7b-multinode [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
opt-6_7b-multinode [config.system.gpu.capacity] 81920 MiB
opt-6_7b-multinode [config.system.self.name] local
opt-6_7b-multinode [config.system.self.ip] 127.0.0.1
opt-6_7b-multinode [config.system.self.port] 8123
opt-6_7b-multinode [config.system.self.user] root
opt-6_7b-multinode [config.system.self.main] True
opt-6_7b-multinode [config.system.self.hostname] localhost
opt-6_7b-multinode [config.system.self.aliaslist] []
opt-6_7b-multinode [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
opt-6_7b-multinode [config.system.self.local] True
opt-6_7b-multinode [config.dirs.base] /Tmp/slurm.4115007.0/base
opt-6_7b-multinode [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
opt-6_7b-multinode [config.dirs.data] /Tmp/slurm.4115007.0/base/data
opt-6_7b-multinode [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
opt-6_7b-multinode [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/opt
opt-6_7b-multinode [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
opt-6_7b-multinode [config.group] opt
opt-6_7b-multinode [config.install_group] torch
opt-6_7b-multinode [config.install_variant] cuda
opt-6_7b-multinode [config.run_name] prepare.2024-02-06_11:57:01.236402
opt-6_7b-multinode [config.enabled] True
opt-6_7b-multinode [config.capabilities.nodes] 1
opt-6_7b-multinode [config.max_duration] 600
opt-6_7b-multinode [config.voir.options.stop] 60
opt-6_7b-multinode [config.voir.options.interval] 1s
opt-6_7b-multinode [config.validation.usage.gpu_load_threshold] 0.5
opt-6_7b-multinode [config.validation.usage.gpu_mem_threshold] 0.5
opt-6_7b-multinode [config.config_base] /Tmp/slurm.4115007.0/milabench/config
opt-6_7b-multinode [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
opt-6_7b-multinode [config.tags] ['huggingface', 'language-modeling', 'llm', 'multigpu', 'multinode', 'nlp', 'transformer']
opt-6_7b-multinode [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt
opt-6_7b-multinode [config.plan.method] njobs
opt-6_7b-multinode [config.plan.n] 1
opt-6_7b-multinode [config.argv.--max_train_steps] 100
opt-6_7b-multinode [config.argv.--dataset_name] wikitext
opt-6_7b-multinode [config.argv.--dataset_config_name] wikitext-103-v1
opt-6_7b-multinode [config.argv.--dataset_rev] b08601e
opt-6_7b-multinode [config.argv.--validation_split_percentage] 5
opt-6_7b-multinode [config.argv.--per_gpu_batch_size] 1
opt-6_7b-multinode [config.argv.--cpus_per_gpu] 8
opt-6_7b-multinode [config.argv.--model_name] facebook/opt-6.7b
opt-6_7b-multinode [config.gradient_accumulation_steps] 1
opt-6_7b-multinode [config.use_deepspeed] True
opt-6_7b-multinode [config.num_machines] 2
opt-6_7b-multinode [config.weight] 10.0
opt-6_7b-multinode [config.requires_capabilities] ['len(nodes) >= 2']
opt-6_7b-multinode [config.docker_image] ghcr.io/mila-iqia/milabench:cuda-nightly
opt-6_7b-multinode [config.name] opt-6_7b-multinode
opt-6_7b-multinode [config.tag] ['opt-6_7b-multinode']
opt-6_7b-multinode [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 81.714,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 35,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707257012.062557,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
opt-6_7b-multinode [start] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-6.7b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:03:32.081712]
opt-6_7b-multinode [stderr] The following values were not passed to `accelerate launch` and had defaults used instead:
opt-6_7b-multinode [stderr] 		More than one GPU was found, enabling multi-GPU training.
opt-6_7b-multinode [stderr] 		If this was unintended please pass in `--num_processes=1`.
opt-6_7b-multinode [stderr] To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
opt-6_7b-multinode [stderr] Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
opt-6_7b-multinode [stdout] [02/06/24 12:03:35] INFO     [0/1] __main__ - Distributed          logging.py:60
opt-6_7b-multinode [stdout]                              environment: MULTI_GPU  Backend: nccl
opt-6_7b-multinode [stdout]                              Num processes: 1
opt-6_7b-multinode [stdout]                              Process index: 0
opt-6_7b-multinode [stdout]                              Local process index: 0
opt-6_7b-multinode [stdout]                              Device: cuda:0
opt-6_7b-multinode [stdout] 
opt-6_7b-multinode [stdout]                              Mixed precision type: fp16
opt-6_7b-multinode [stdout] 
opt-6_7b-multinode [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
opt-6_7b-multinode [stderr]   table = cls._concat_blocks(blocks, axis=0)
opt-6_7b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b-multinode [stderr] Model config OPTConfig {
opt-6_7b-multinode [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-6_7b-multinode [stderr]   "activation_dropout": 0.0,
opt-6_7b-multinode [stderr]   "activation_function": "relu",
opt-6_7b-multinode [stderr]   "architectures": [
opt-6_7b-multinode [stderr]     "OPTForCausalLM"
opt-6_7b-multinode [stderr]   ],
opt-6_7b-multinode [stderr]   "attention_dropout": 0.0,
opt-6_7b-multinode [stderr]   "bos_token_id": 2,
opt-6_7b-multinode [stderr]   "do_layer_norm_before": true,
opt-6_7b-multinode [stderr]   "dropout": 0.1,
opt-6_7b-multinode [stderr]   "enable_bias": true,
opt-6_7b-multinode [stderr]   "eos_token_id": 2,
opt-6_7b-multinode [stderr]   "ffn_dim": 16384,
opt-6_7b-multinode [stderr]   "hidden_size": 4096,
opt-6_7b-multinode [stderr]   "init_std": 0.02,
opt-6_7b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b-multinode [stderr]   "layerdrop": 0.0,
opt-6_7b-multinode [stderr]   "max_position_embeddings": 2048,
opt-6_7b-multinode [stderr]   "model_type": "opt",
opt-6_7b-multinode [stderr]   "num_attention_heads": 32,
opt-6_7b-multinode [stderr]   "num_hidden_layers": 32,
opt-6_7b-multinode [stderr]   "pad_token_id": 1,
opt-6_7b-multinode [stderr]   "prefix": "</s>",
opt-6_7b-multinode [stderr]   "torch_dtype": "float16",
opt-6_7b-multinode [stderr]   "transformers_version": "4.35.0",
opt-6_7b-multinode [stderr]   "use_cache": true,
opt-6_7b-multinode [stderr]   "vocab_size": 50272,
opt-6_7b-multinode [stderr]   "word_embed_proj_dim": 4096
opt-6_7b-multinode [stderr] }
opt-6_7b-multinode [stderr] 
opt-6_7b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b-multinode [stderr] Model config OPTConfig {
opt-6_7b-multinode [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-6_7b-multinode [stderr]   "activation_dropout": 0.0,
opt-6_7b-multinode [stderr]   "activation_function": "relu",
opt-6_7b-multinode [stderr]   "architectures": [
opt-6_7b-multinode [stderr]     "OPTForCausalLM"
opt-6_7b-multinode [stderr]   ],
opt-6_7b-multinode [stderr]   "attention_dropout": 0.0,
opt-6_7b-multinode [stderr]   "bos_token_id": 2,
opt-6_7b-multinode [stderr]   "do_layer_norm_before": true,
opt-6_7b-multinode [stderr]   "dropout": 0.1,
opt-6_7b-multinode [stderr]   "enable_bias": true,
opt-6_7b-multinode [stderr]   "eos_token_id": 2,
opt-6_7b-multinode [stderr]   "ffn_dim": 16384,
opt-6_7b-multinode [stderr]   "hidden_size": 4096,
opt-6_7b-multinode [stderr]   "init_std": 0.02,
opt-6_7b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b-multinode [stderr]   "layerdrop": 0.0,
opt-6_7b-multinode [stderr]   "max_position_embeddings": 2048,
opt-6_7b-multinode [stderr]   "model_type": "opt",
opt-6_7b-multinode [stderr]   "num_attention_heads": 32,
opt-6_7b-multinode [stderr]   "num_hidden_layers": 32,
opt-6_7b-multinode [stderr]   "pad_token_id": 1,
opt-6_7b-multinode [stderr]   "prefix": "</s>",
opt-6_7b-multinode [stderr]   "torch_dtype": "float16",
opt-6_7b-multinode [stderr]   "transformers_version": "4.35.0",
opt-6_7b-multinode [stderr]   "use_cache": true,
opt-6_7b-multinode [stderr]   "vocab_size": 50272,
opt-6_7b-multinode [stderr]   "word_embed_proj_dim": 4096
opt-6_7b-multinode [stderr] }
opt-6_7b-multinode [stderr] 
opt-6_7b-multinode [stderr] loading file vocab.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/vocab.json
opt-6_7b-multinode [stderr] loading file merges.txt from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/merges.txt
opt-6_7b-multinode [stderr] loading file tokenizer.json from cache at None
opt-6_7b-multinode [stderr] loading file added_tokens.json from cache at None
opt-6_7b-multinode [stderr] loading file special_tokens_map.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/special_tokens_map.json
opt-6_7b-multinode [stderr] loading file tokenizer_config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/tokenizer_config.json
opt-6_7b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b-multinode [stderr] Model config OPTConfig {
opt-6_7b-multinode [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-6_7b-multinode [stderr]   "activation_dropout": 0.0,
opt-6_7b-multinode [stderr]   "activation_function": "relu",
opt-6_7b-multinode [stderr]   "architectures": [
opt-6_7b-multinode [stderr]     "OPTForCausalLM"
opt-6_7b-multinode [stderr]   ],
opt-6_7b-multinode [stderr]   "attention_dropout": 0.0,
opt-6_7b-multinode [stderr]   "bos_token_id": 2,
opt-6_7b-multinode [stderr]   "do_layer_norm_before": true,
opt-6_7b-multinode [stderr]   "dropout": 0.1,
opt-6_7b-multinode [stderr]   "enable_bias": true,
opt-6_7b-multinode [stderr]   "eos_token_id": 2,
opt-6_7b-multinode [stderr]   "ffn_dim": 16384,
opt-6_7b-multinode [stderr]   "hidden_size": 4096,
opt-6_7b-multinode [stderr]   "init_std": 0.02,
opt-6_7b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b-multinode [stderr]   "layerdrop": 0.0,
opt-6_7b-multinode [stderr]   "max_position_embeddings": 2048,
opt-6_7b-multinode [stderr]   "model_type": "opt",
opt-6_7b-multinode [stderr]   "num_attention_heads": 32,
opt-6_7b-multinode [stderr]   "num_hidden_layers": 32,
opt-6_7b-multinode [stderr]   "pad_token_id": 1,
opt-6_7b-multinode [stderr]   "prefix": "</s>",
opt-6_7b-multinode [stderr]   "torch_dtype": "float16",
opt-6_7b-multinode [stderr]   "transformers_version": "4.35.0",
opt-6_7b-multinode [stderr]   "use_cache": true,
opt-6_7b-multinode [stderr]   "vocab_size": 50272,
opt-6_7b-multinode [stderr]   "word_embed_proj_dim": 4096
opt-6_7b-multinode [stderr] }
opt-6_7b-multinode [stderr] 
opt-6_7b-multinode [stderr] loading configuration file config.json from cache at /Tmp/slurm.4115007.0/base/cache/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
opt-6_7b-multinode [stderr] Model config OPTConfig {
opt-6_7b-multinode [stderr]   "_name_or_path": "facebook/opt-6.7b",
opt-6_7b-multinode [stderr]   "_remove_final_layer_norm": false,
opt-6_7b-multinode [stderr]   "activation_dropout": 0.0,
opt-6_7b-multinode [stderr]   "activation_function": "relu",
opt-6_7b-multinode [stderr]   "architectures": [
opt-6_7b-multinode [stderr]     "OPTForCausalLM"
opt-6_7b-multinode [stderr]   ],
opt-6_7b-multinode [stderr]   "attention_dropout": 0.0,
opt-6_7b-multinode [stderr]   "bos_token_id": 2,
opt-6_7b-multinode [stderr]   "do_layer_norm_before": true,
opt-6_7b-multinode [stderr]   "dropout": 0.1,
opt-6_7b-multinode [stderr]   "enable_bias": true,
opt-6_7b-multinode [stderr]   "eos_token_id": 2,
opt-6_7b-multinode [stderr]   "ffn_dim": 16384,
opt-6_7b-multinode [stderr]   "hidden_size": 4096,
opt-6_7b-multinode [stderr]   "init_std": 0.02,
opt-6_7b-multinode [stderr]   "layer_norm_elementwise_affine": true,
opt-6_7b-multinode [stderr]   "layerdrop": 0.0,
opt-6_7b-multinode [stderr]   "max_position_embeddings": 2048,
opt-6_7b-multinode [stderr]   "model_type": "opt",
opt-6_7b-multinode [stderr]   "num_attention_heads": 32,
opt-6_7b-multinode [stderr]   "num_hidden_layers": 32,
opt-6_7b-multinode [stderr]   "pad_token_id": 1,
opt-6_7b-multinode [stderr]   "prefix": "</s>",
opt-6_7b-multinode [stderr]   "torch_dtype": "float16",
opt-6_7b-multinode [stderr]   "transformers_version": "4.35.0",
opt-6_7b-multinode [stderr]   "use_cache": true,
opt-6_7b-multinode [stderr]   "vocab_size": 50272,
opt-6_7b-multinode [stderr]   "word_embed_proj_dim": 4096
opt-6_7b-multinode [stderr] }
opt-6_7b-multinode [stderr] 
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stderr] libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
opt-6_7b-multinode [stdout] [02/06/24 12:03:37] WARNING  [0/1] __main__ - The tokenizer picked logging.py:60
opt-6_7b-multinode [stdout]                              seems to have a very large
opt-6_7b-multinode [stdout]                              `model_max_length`
opt-6_7b-multinode [stdout]                              (1000000000000000019884624838656).
opt-6_7b-multinode [stdout]                              Picking 1024 instead. You can change
opt-6_7b-multinode [stdout]                              that default value by passing
opt-6_7b-multinode [stdout]                              --block_size xxx.
opt-6_7b-multinode [end] accelerate launch --mixed_precision=fp16 --num_machines=1 --dynamo_backend=no --num_processes=1 --num_cpu_threads_per_process=8 /Tmp/slurm.4115007.0/milabench/benchmarks/accelerate_opt/main.py --max_train_steps 100 --dataset_name wikitext --dataset_config_name wikitext-103-v1 --dataset_rev b08601e --validation_split_percentage 5 --per_gpu_batch_size 1 --cpus_per_gpu 8 --model_name facebook/opt-6.7b --prepare_only --cache /Tmp/slurm.4115007.0/base/cache [at 2024-02-06 12:03:38.845481]
stargan [config.system.arch] cuda
stargan [config.system.sshkey] None
stargan [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
stargan [config.system.gpu.capacity] 81920 MiB
stargan [config.system.self.name] local
stargan [config.system.self.ip] 127.0.0.1
stargan [config.system.self.port] 8123
stargan [config.system.self.user] root
stargan [config.system.self.main] True
stargan [config.system.self.hostname] localhost
stargan [config.system.self.aliaslist] []
stargan [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
stargan [config.system.self.local] True
stargan [config.dirs.base] /Tmp/slurm.4115007.0/base
stargan [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
stargan [config.dirs.data] /Tmp/slurm.4115007.0/base/data
stargan [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
stargan [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/stargan
stargan [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
stargan [config.group] stargan
stargan [config.install_group] torch
stargan [config.install_variant] cuda
stargan [config.run_name] prepare.2024-02-06_11:57:01.236402
stargan [config.enabled] True
stargan [config.capabilities.nodes] 1
stargan [config.max_duration] 600
stargan [config.voir.options.stop] 60
stargan [config.voir.options.interval] 1s
stargan [config.validation.usage.gpu_load_threshold] 0.5
stargan [config.validation.usage.gpu_mem_threshold] 0.5
stargan [config.config_base] /Tmp/slurm.4115007.0/milabench/config
stargan [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
stargan [config.tags] ['gan', 'resnet', 'vision']
stargan [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/stargan
stargan [config.plan.method] per_gpu
stargan [config.argv.--image_size] 512
stargan [config.argv.--c_dim] 5
stargan [config.argv.--batch_size] 16
stargan [config.weight] 1.0
stargan [config.name] stargan
stargan [config.tag] ['stargan']
stargan [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 81.714,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 35,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.619,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707257021.258754,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
stargan [start] true [at 2024-02-06 12:03:41.278657]
stargan [end] true [at 2024-02-06 12:03:41.279431]
super-slomo [config.system.arch] cuda
super-slomo [config.system.sshkey] None
super-slomo [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
super-slomo [config.system.gpu.capacity] 81920 MiB
super-slomo [config.system.self.name] local
super-slomo [config.system.self.ip] 127.0.0.1
super-slomo [config.system.self.port] 8123
super-slomo [config.system.self.user] root
super-slomo [config.system.self.main] True
super-slomo [config.system.self.hostname] localhost
super-slomo [config.system.self.aliaslist] []
super-slomo [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
super-slomo [config.system.self.local] True
super-slomo [config.dirs.base] /Tmp/slurm.4115007.0/base
super-slomo [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
super-slomo [config.dirs.data] /Tmp/slurm.4115007.0/base/data
super-slomo [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
super-slomo [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/super-slomo
super-slomo [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
super-slomo [config.group] super-slomo
super-slomo [config.install_group] torch
super-slomo [config.install_variant] cuda
super-slomo [config.run_name] prepare.2024-02-06_11:57:01.236402
super-slomo [config.enabled] True
super-slomo [config.capabilities.nodes] 1
super-slomo [config.max_duration] 600
super-slomo [config.voir.options.stop] 60
super-slomo [config.voir.options.interval] 1s
super-slomo [config.validation.usage.gpu_load_threshold] 0.5
super-slomo [config.validation.usage.gpu_mem_threshold] 0.5
super-slomo [config.config_base] /Tmp/slurm.4115007.0/milabench/config
super-slomo [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
super-slomo [config.tags] ['convnet', 'unet', 'video-interpolation', 'vision']
super-slomo [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo
super-slomo [config.plan.method] per_gpu
super-slomo [config.argv.--train_batch_size] 32
super-slomo [config.weight] 1.0
super-slomo [config.name] super-slomo
super-slomo [config.tag] ['super-slomo']
super-slomo [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 64.782,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 34,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.687,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707257023.6471,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
super-slomo [start] /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/prepare.py --train_batch_size 32 [at 2024-02-06 12:03:43.665775]
super-slomo [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
super-slomo [stderr]   warnings.warn(
super-slomo [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
super-slomo [stderr]   warnings.warn(msg)
super-slomo [stderr] Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /Tmp/slurm.4115007.0/base/cache/hub/checkpoints/vgg16-397923af.pth
super-slomo [stderr]   0%|          | 0.00/528M [00:00<?, ?B/s]
super-slomo [stderr]   1%|▏         | 7.52M/528M [00:00<00:06, 78.9MB/s]
super-slomo [stderr]   3%|▎         | 16.6M/528M [00:00<00:06, 88.7MB/s]
super-slomo [stderr]   5%|▍         | 25.1M/528M [00:00<00:06, 87.8MB/s]
super-slomo [stderr]   6%|▋         | 33.6M/528M [00:00<00:05, 88.4MB/s]
super-slomo [stderr]   8%|▊         | 42.1M/528M [00:00<00:05, 88.2MB/s]
super-slomo [stderr]  10%|▉         | 50.8M/528M [00:00<00:05, 89.3MB/s]
super-slomo [stderr]  11%|█         | 59.3M/528M [00:00<00:05, 88.0MB/s]
super-slomo [stderr]  13%|█▎        | 68.4M/528M [00:00<00:05, 90.3MB/s]
super-slomo [stderr]  15%|█▍        | 77.3M/528M [00:00<00:05, 91.0MB/s]
super-slomo [stderr]  16%|█▋        | 86.4M/528M [00:01<00:05, 92.2MB/s]
super-slomo [stderr]  18%|█▊        | 95.2M/528M [00:01<00:04, 92.4MB/s]
super-slomo [stderr]  20%|█▉        | 104M/528M [00:01<00:04, 92.8MB/s]
super-slomo [stderr]  21%|██▏       | 113M/528M [00:01<00:04, 93.0MB/s]
super-slomo [stderr]  23%|██▎       | 122M/528M [00:01<00:04, 92.4MB/s]
super-slomo [stderr]  25%|██▍       | 131M/528M [00:01<00:04, 92.6MB/s]
super-slomo [stderr]  26%|██▋       | 140M/528M [00:01<00:04, 93.0MB/s]
super-slomo [stderr]  28%|██▊       | 149M/528M [00:01<00:04, 92.2MB/s]
super-slomo [stderr]  30%|██▉       | 157M/528M [00:01<00:04, 91.7MB/s]
super-slomo [stderr]  32%|███▏      | 166M/528M [00:01<00:04, 91.9MB/s]
super-slomo [stderr]  33%|███▎      | 175M/528M [00:02<00:04, 91.0MB/s]
super-slomo [stderr]  35%|███▍      | 184M/528M [00:02<00:03, 91.2MB/s]
super-slomo [stderr]  37%|███▋      | 193M/528M [00:02<00:03, 92.0MB/s]
super-slomo [stderr]  38%|███▊      | 202M/528M [00:02<00:03, 90.5MB/s]
super-slomo [stderr]  40%|███▉      | 210M/528M [00:02<00:03, 90.7MB/s]
super-slomo [stderr]  42%|████▏     | 219M/528M [00:02<00:03, 92.0MB/s]
super-slomo [stderr]  43%|████▎     | 229M/528M [00:02<00:03, 93.2MB/s]
super-slomo [stderr]  45%|████▍     | 238M/528M [00:02<00:03, 93.4MB/s]
super-slomo [stderr]  47%|████▋     | 247M/528M [00:02<00:03, 93.9MB/s]
super-slomo [stderr]  48%|████▊     | 256M/528M [00:02<00:03, 92.8MB/s]
super-slomo [stderr]  50%|█████     | 264M/528M [00:03<00:02, 92.9MB/s]
super-slomo [stderr]  52%|█████▏    | 273M/528M [00:03<00:02, 93.5MB/s]
super-slomo [stderr]  54%|█████▎    | 283M/528M [00:03<00:02, 94.4MB/s]
super-slomo [stderr]  55%|█████▌    | 292M/528M [00:03<00:02, 93.4MB/s]
super-slomo [stderr]  57%|█████▋    | 301M/528M [00:03<00:02, 94.0MB/s]
super-slomo [stderr]  59%|█████▊    | 310M/528M [00:03<00:02, 93.5MB/s]
super-slomo [stderr]  60%|██████    | 319M/528M [00:03<00:02, 92.7MB/s]
super-slomo [stderr]  62%|██████▏   | 328M/528M [00:03<00:02, 91.4MB/s]
super-slomo [stderr]  64%|██████▎   | 336M/528M [00:03<00:02, 90.3MB/s]
super-slomo [stderr]  65%|██████▌   | 346M/528M [00:03<00:02, 92.3MB/s]
super-slomo [stderr]  67%|██████▋   | 354M/528M [00:04<00:01, 91.1MB/s]
super-slomo [stderr]  69%|██████▉   | 363M/528M [00:04<00:01, 91.6MB/s]
super-slomo [stderr]  71%|███████   | 372M/528M [00:04<00:01, 92.6MB/s]
super-slomo [stderr]  72%|███████▏  | 381M/528M [00:04<00:01, 91.6MB/s]
super-slomo [stderr]  74%|███████▍  | 390M/528M [00:04<00:01, 91.6MB/s]
super-slomo [stderr]  76%|███████▌  | 399M/528M [00:04<00:01, 92.7MB/s]
super-slomo [stderr]  77%|███████▋  | 408M/528M [00:04<00:01, 93.8MB/s]
super-slomo [stderr]  79%|███████▉  | 418M/528M [00:04<00:01, 95.1MB/s]
super-slomo [stderr]  81%|████████  | 427M/528M [00:04<00:01, 94.8MB/s]
super-slomo [stderr]  83%|████████▎ | 436M/528M [00:04<00:01, 94.7MB/s]
super-slomo [stderr]  84%|████████▍ | 445M/528M [00:05<00:00, 94.8MB/s]
super-slomo [stderr]  86%|████████▌ | 454M/528M [00:05<00:00, 95.5MB/s]
super-slomo [stderr]  88%|████████▊ | 463M/528M [00:05<00:00, 94.5MB/s]
super-slomo [stderr]  89%|████████▉ | 472M/528M [00:05<00:00, 94.5MB/s]
super-slomo [stderr]  91%|█████████ | 481M/528M [00:05<00:00, 93.1MB/s]
super-slomo [stderr]  93%|█████████▎| 490M/528M [00:05<00:00, 92.6MB/s]
super-slomo [stderr]  95%|█████████▍| 499M/528M [00:05<00:00, 92.5MB/s]
super-slomo [stderr]  96%|█████████▋| 508M/528M [00:05<00:00, 94.2MB/s]
super-slomo [stderr]  98%|█████████▊| 517M/528M [00:05<00:00, 94.8MB/s]
super-slomo [stderr] 100%|█████████▉| 526M/528M [00:05<00:00, 94.7MB/s]
super-slomo [stderr] 100%|██████████| 528M/528M [00:05<00:00, 92.3MB/s]
super-slomo [end] /Tmp/slurm.4115007.0/milabench/benchmarks/super-slomo/prepare.py --train_batch_size 32 [at 2024-02-06 12:03:52.491228]
dlrm [config.system.arch] cuda
dlrm [config.system.sshkey] None
dlrm [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
dlrm [config.system.gpu.capacity] 81920 MiB
dlrm [config.system.self.name] local
dlrm [config.system.self.ip] 127.0.0.1
dlrm [config.system.self.port] 8123
dlrm [config.system.self.user] root
dlrm [config.system.self.main] True
dlrm [config.system.self.hostname] localhost
dlrm [config.system.self.aliaslist] []
dlrm [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
dlrm [config.system.self.local] True
dlrm [config.dirs.base] /Tmp/slurm.4115007.0/base
dlrm [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
dlrm [config.dirs.data] /Tmp/slurm.4115007.0/base/data
dlrm [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
dlrm [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/dlrm
dlrm [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
dlrm [config.group] dlrm
dlrm [config.install_group] torch
dlrm [config.install_variant] cuda
dlrm [config.run_name] prepare.2024-02-06_11:57:01.236402
dlrm [config.enabled] True
dlrm [config.capabilities.nodes] 1
dlrm [config.max_duration] 600
dlrm [config.voir.options.stop] 60
dlrm [config.voir.options.interval] 1s
dlrm [config.validation.usage.gpu_load_threshold] 0.5
dlrm [config.validation.usage.gpu_mem_threshold] 0.5
dlrm [config.config_base] /Tmp/slurm.4115007.0/milabench/config
dlrm [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
dlrm [config.tags] ['nlp', 'rl']
dlrm [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/dlrm
dlrm [config.plan.method] njobs
dlrm [config.plan.n] 1
dlrm [config.argv.--num-batches] 1000
dlrm [config.argv.--data-generation] random
dlrm [config.argv.--arch-mlp-bot] 512-512-64
dlrm [config.argv.--arch-mlp-top] 1024-1024-1024-1
dlrm [config.argv.--arch-sparse-feature-size] 64
dlrm [config.argv.--arch-embedding-size] 1000000-1000000-1000000-1000000-1000000-1000000-1000000-1000000
dlrm [config.argv.--num-indices-per-lookup] 100
dlrm [config.argv.--arch-interaction-op] dot
dlrm [config.argv.--numpy-rand-seed] 727
dlrm [config.argv.--print-freq] 999999
dlrm [config.argv.--mini-batch-size] 16384
dlrm [config.argv.--test-mini-batch-size] 16384
dlrm [config.argv.--test-num-workers] 0
dlrm [config.argv.--use-gpu] True
dlrm [config.weight] 1.0
dlrm [config.name] dlrm
dlrm [config.tag] ['dlrm']
dlrm [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 64.642,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 34,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.824,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707257034.894007,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
dlrm [start] true [at 2024-02-06 12:03:54.912864]
dlrm [end] true [at 2024-02-06 12:03:54.913618]
rwkv [config.system.arch] cuda
rwkv [config.system.sshkey] None
rwkv [config.system.nodes] [{'aliaslist': [],
  'hostname': 'localhost',
  'ip': '127.0.0.1',
  'ipaddrlist': ['fe80::ce48:3aff:fe1c:9c24%eno8303',
                 'fe80::1270:fd03:ee:7a3a%ib1',
                 '10.20.9.34',
                 '00:00:00:00:00:00',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
                 'cc:48:3a:1c:9c:24',
                 '::1',
                 '10.20.137.34',
                 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
                 '127.0.0.1',
                 '172.16.9.34',
                 'fe80::1270:fd03:ee:7b02%ib0'],
  'local': True,
  'main': True,
  'name': 'local',
  'port': 8123,
  'user': 'root'}]
rwkv [config.system.gpu.capacity] 81920 MiB
rwkv [config.system.self.name] local
rwkv [config.system.self.ip] 127.0.0.1
rwkv [config.system.self.port] 8123
rwkv [config.system.self.user] root
rwkv [config.system.self.main] True
rwkv [config.system.self.hostname] localhost
rwkv [config.system.self.aliaslist] []
rwkv [config.system.self.ipaddrlist] ['fe80::ce48:3aff:fe1c:9c24%eno8303',
 'fe80::1270:fd03:ee:7a3a%ib1',
 '10.20.9.34',
 '00:00:00:00:00:00',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7b:02',
 'cc:48:3a:1c:9c:24',
 '::1',
 '10.20.137.34',
 '00:00:10:29:fe:80:00:00:00:00:00:00:10:70:fd:03:00:ee:7a:3a',
 '127.0.0.1',
 '172.16.9.34',
 'fe80::1270:fd03:ee:7b02%ib0']
rwkv [config.system.self.local] True
rwkv [config.dirs.base] /Tmp/slurm.4115007.0/base
rwkv [config.dirs.venv] /Tmp/slurm.4115007.0/base/venv/torch
rwkv [config.dirs.data] /Tmp/slurm.4115007.0/base/data
rwkv [config.dirs.runs] /Tmp/slurm.4115007.0/base/runs
rwkv [config.dirs.extra] /Tmp/slurm.4115007.0/base/extra/rwkv
rwkv [config.dirs.cache] /Tmp/slurm.4115007.0/base/cache
rwkv [config.group] rwkv
rwkv [config.install_group] torch
rwkv [config.install_variant] cuda
rwkv [config.run_name] prepare.2024-02-06_11:57:01.236402
rwkv [config.enabled] True
rwkv [config.capabilities.nodes] 1
rwkv [config.max_duration] 600
rwkv [config.voir.options.stop] 60
rwkv [config.voir.options.interval] 1s
rwkv [config.validation.usage.gpu_load_threshold] 0.5
rwkv [config.validation.usage.gpu_mem_threshold] 0.5
rwkv [config.config_base] /Tmp/slurm.4115007.0/milabench/config
rwkv [config.config_file] /Tmp/slurm.4115007.0/milabench/config/standard.yaml
rwkv [config.definition] /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv
rwkv [config.tags] ['llm', 'no-rocm', 'rnn']
rwkv [config.plan.method] per_gpu
rwkv [config.argv.--data_type] dummy
rwkv [config.argv.--ctx_len] 128
rwkv [config.argv.--epoch_steps] 1000
rwkv [config.argv.--epoch_count] 20
rwkv [config.argv.--epoch_begin] 0
rwkv [config.argv.--epoch_save] 0
rwkv [config.argv.--micro_bsz] 16
rwkv [config.argv.--n_layer] 12
rwkv [config.argv.--n_embd] 768
rwkv [config.argv.--pre_ffn] 0
rwkv [config.argv.--head_qk] 0
rwkv [config.argv.--lr_init] 6e-4
rwkv [config.argv.--lr_final] 1e-5
rwkv [config.argv.--warmup_steps] 0
rwkv [config.argv.--beta1] 0.9
rwkv [config.argv.--beta2] 0.99
rwkv [config.argv.--adam_eps] 1e-8
rwkv [config.argv.--accelerator] gpu
rwkv [config.argv.--devices] 1
rwkv [config.argv.--precision] tf32
rwkv [config.argv.--strategy] ddp_find_unused_parameters_false
rwkv [config.argv.--grad_cp] 0
rwkv [config.argv.--random_seed] 1234
rwkv [config.argv.--enable_progress_bar] False
rwkv [config.weight] 1.0
rwkv [config.name] rwkv
rwkv [config.tag] ['rwkv']
rwkv [meta] {'accelerators': {'arch': 'cuda',
                  'gpus': {'GPU-80d2dc6e-14f7-798e-5ce0-647e33324ef0': {'device': '0',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 64.932,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 34,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}},
                           'GPU-8b2c653d-fcd1-4ab5-7b87-4ac3d77d309a': {'device': '1',
                                                                        'memory': {'total': 81920.0, 'used': 693.5625},
                                                                        'power': 58.824,
                                                                        'product': 'NVIDIA A100-SXM4-80GB',
                                                                        'selection_variable': 'CUDA_VISIBLE_DEVICES',
                                                                        'temperature': 25,
                                                                        'utilization': {'compute': 0,
                                                                                        'memory': 0.008466339111328125}}}},
 'cpu': {'brand': 'AMD EPYC 7543 32-Core Processor', 'count': 64},
 'date': 1707257037.331198,
 'milabench': {'commit': '4c8961898aa0dc59a9227c32d562c7a0be37ea03',
               'date': '2024-02-06 11:53:56 -0500',
               'tag': '4c89618'},
 'os': {'machine': 'x86_64',
        'nodename': 'cn-g024.server.mila.quebec',
        'release': '4.15.0-213-generic',
        'sysname': 'Linux',
        'version': '#224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023'},
 'pytorch': {'build_settings': {'BLAS_INFO': 'mkl',
                                'BUILD_TYPE': 'Release',
                                'CUDA_VERSION': '11.8',
                                'CUDNN_VERSION': '8.7.0',
                                'CXX_COMPILER': '/opt/rh/devtoolset-9/root/usr/bin/c++',
                                'CXX_FLAGS': '-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden '
                                             '-DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER '
                                             '-DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK '
                                             '-DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra '
                                             '-Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation '
                                             '-Wnarrowing -Wno-missing-field-initializers -Wno-type-limits '
                                             '-Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter '
                                             '-Wno-unused-function -Wno-unused-result -Wno-strict-overflow '
                                             '-Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi '
                                             '-Wno-error=pedantic -Wno-error=old-style-cast '
                                             '-Wno-invalid-partial-specialization -Wno-unused-private-field '
                                             '-Wno-aligned-allocation-unavailable -Wno-missing-braces '
                                             '-fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable '
                                             '-Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math '
                                             '-Werror=format -Werror=cast-function-type -Wno-stringop-overflow',
                                'LAPACK_INFO': 'mkl',
                                'PERF_WITH_AVX': '1',
                                'PERF_WITH_AVX2': '1',
                                'PERF_WITH_AVX512': '1',
                                'TORCH_DISABLE_GPU_ASSERTS': 'ON',
                                'TORCH_VERSION': '2.1.0',
                                'USE_CUDA': 'ON',
                                'USE_CUDNN': 'ON',
                                'USE_EXCEPTION_PTR': '1',
                                'USE_GFLAGS': 'OFF',
                                'USE_GLOG': 'OFF',
                                'USE_MKL': 'ON',
                                'USE_MKLDNN': 'ON',
                                'USE_MPI': 'OFF',
                                'USE_NCCL': '1',
                                'USE_NNPACK': 'ON',
                                'USE_OPENMP': 'ON',
                                'USE_ROCM': 'OFF'},
             'compiler': 'GCC 9.3',
             'cpp': 'C++ Version: 201703',
             'cpu': 'CPU capability usage: AVX2',
             'intel': 'Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 '
                      'architecture applications',
             'lapack': 'LAPACK is enabled (usually provided by MKL)',
             'mkl': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'nnpack': 'NNPACK is enabled',
             'openmp': 'OpenMP 201511 (a.k.a. OpenMP 4.5)',
             'torch': '2.1.0+cu118'}}
rwkv [start] /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/prepare.py --data_type dummy --ctx_len 128 --epoch_steps 1000 --epoch_count 20 --epoch_begin 0 --epoch_save 0 --micro_bsz 16 --n_layer 12 --n_embd 768 --pre_ffn 0 --head_qk 0 --lr_init 6e-4 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.99 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision tf32 --strategy ddp_find_unused_parameters_false --grad_cp 0 --random_seed 1234 --enable_progress_bar False [at 2024-02-06 12:03:57.350559]
rwkv [stdout] Run the training process, but only one step.
rwkv [stdout] This will compile the appropriate torch extensions.
rwkv [stdout] ================================================================================
rwkv [stdout] [2024-02-06 12:03:59,189] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
rwkv [stderr] ########## work in progress ##########
rwkv [stdout] ########## WARNING: GLOBAL SEED 1234 THIS WILL AFFECT MULTIGPU SAMPLING ##########
rwkv [stdout] ########## WARNING: GLOBAL SEED 1234 THIS WILL AFFECT MULTIGPU SAMPLING ##########
rwkv [stdout] ########## WARNING: GLOBAL SEED 1234 THIS WILL AFFECT MULTIGPU SAMPLING ##########
rwkv [stdout] 
rwkv [stderr] [rank: 0] Global seed set to 1234
rwkv [stderr] 
rwkv [stderr] ############################################################################
rwkv [stderr] #
rwkv [stderr] # RWKV-4 TF32 on 1x1 GPU, bsz 1x1x16=16, ddp_find_unused_parameters_false
rwkv [stderr] #
rwkv [stderr] # Data =  (dummy), ProjDir = /Tmp/slurm.4115007.0/base/proj/rwkv/
rwkv [stderr] #
rwkv [stderr] # Epoch = 0 to 19 (will continue afterwards), save every 0 epoch
rwkv [stderr] #
rwkv [stderr] # Each "epoch" = 1000 steps, 16000 samples, 2048000 tokens
rwkv [stderr] #
rwkv [stderr] # Model = 12 n_layer, 768 n_embd, 128 ctx_len
rwkv [stderr] #
rwkv [stderr] # Adam = lr 0.0006 to 1e-05, warmup 0 steps, beta (0.9, 0.99), eps 1e-08
rwkv [stderr] #
rwkv [stderr] # Found torch 2.1.0+cu118, recommend 1.13.1+cu117 or newer
rwkv [stderr] # Found deepspeed 0.12.2, recommend 0.7.0 (faster than newer versions)
rwkv [stderr] # Found pytorch_lightning 1.9.5, recommend 1.9.1 or newer
rwkv [stderr] #
rwkv [stderr] ############################################################################
rwkv [stderr] 
rwkv [stderr] {'load_model': '', 'wandb': '', 'proj_dir': '/Tmp/slurm.4115007.0/base/proj/rwkv/', 'random_seed': 1234, 'data_file': '', 'data_type': 'dummy', 'vocab_size': 0, 'ctx_len': 128, 'epoch_steps': 1000, 'epoch_count': 20, 'epoch_begin': 0, 'epoch_save': 0, 'micro_bsz': 16, 'n_layer': 12, 'n_embd': 768, 'dim_att': 768, 'dim_ffn': 3072, 'pre_ffn': 0, 'head_qk': 0, 'tiny_att_dim': 0, 'tiny_att_layer': -999, 'lr_init': 0.0006, 'lr_final': 1e-05, 'warmup_steps': 0, 'beta1': 0.9, 'beta2': 0.99, 'adam_eps': 1e-08, 'grad_cp': 0, 'my_pile_version': 1, 'my_pile_stage': 0, 'my_pile_shift': -1, 'my_pile_edecay': 0, 'layerwise_lr': 1, 'ds_bucket_mb': 200, 'my_img_version': 0, 'my_img_size': 0, 'my_img_bit': 0, 'my_img_clip': 'x', 'my_img_clip_scale': 1, 'my_img_l1_scale': 0, 'my_img_encoder': 'x', 'my_sample_len': 0, 'my_ffn_shift': 1, 'my_att_shift': 1, 'my_pos_emb': 0, 'load_partial': 0, 'magic_prime': 0, 'my_qa_mask': 0, 'my_testing': '', 'logger': False, 'enable_checkpointing': False, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': None, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': False, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 100000000000000000000, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': -1, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 100000000000000000000, 'accelerator': 'gpu', 'strategy': 'ddp_find_unused_parameters_false', 'sync_batchnorm': False, 'precision': 'tf32', 'enable_model_summary': True, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': None, 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True, 'my_timestamp': '2024-02-06-12-04-00', 'betas': (0.9, 0.99), 'real_bsz': 16, 'run_name': '0 ctx128 L12 D768'}
rwkv [stderr] 
rwkv [stderr] Building dummy data...
rwkv [stderr] Building token list...
rwkv [stderr] Data has 1620950 tokens, 13 vocab size.
rwkv [stdout] RWKV_MY_TESTING
rwkv [stderr] Using /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
rwkv [stderr] Creating extension directory /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118/wkv_128...
rwkv [stderr] Detected CUDA files, patching ldflags
rwkv [stderr] Emitting ninja build file /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118/wkv_128/build.ninja...
rwkv [stderr] Building extension module wkv_128...
rwkv [stderr] Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
rwkv [stdout] [1/3] /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_128 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/TH -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/THC -isystem /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/include -isystem /Tmp/slurm.4115007.0/env/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=128 -std=c++17 -c /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/rwkv-v4neo/cuda/wkv_cuda.cu -o wkv_cuda.cuda.o
rwkv [stdout] ptxas info    : 0 bytes gmem
rwkv [stdout] ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_80'
rwkv [stdout] ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_
rwkv [stdout]     1024 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
rwkv [stdout] ptxas info    : Used 56 registers, 448 bytes cmem[0], 16 bytes cmem[2]
rwkv [stdout] ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_80'
rwkv [stdout] ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_
rwkv [stdout]     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
rwkv [stdout] ptxas info    : Used 40 registers, 408 bytes cmem[0]
rwkv [stdout] [2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_128 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/TH -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/THC -isystem /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/include -isystem /Tmp/slurm.4115007.0/env/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/rwkv-v4neo/cuda/wkv_op.cpp -o wkv_op.o
rwkv [stdout] [3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/lib64 -lcudart -o wkv_128.so
rwkv [stderr] Loading extension module wkv_128...
rwkv [stdout] 
rwkv [stdout] ############################################################################
rwkv [stdout] #
rwkv [stdout] # Init model weight (slow for large models)...
rwkv [stdout] #
rwkv [stdout] ############################################################################
rwkv [stdout] 
rwkv [stdout] 13    768   -0.0006 emb.weight
rwkv [stdout] 768   768   0    blocks.0.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.0.att.value.weight
rwkv [stdout] 768   768   0    blocks.0.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.0.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.0.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.0.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.0.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.1.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.1.att.value.weight
rwkv [stdout] 768   768   0    blocks.1.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.1.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.1.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.1.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.1.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.2.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.2.att.value.weight
rwkv [stdout] 768   768   0    blocks.2.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.2.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.2.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.2.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.2.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.3.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.3.att.value.weight
rwkv [stdout] 768   768   0    blocks.3.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.3.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.3.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.3.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.3.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.4.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.4.att.value.weight
rwkv [stdout] 768   768   0    blocks.4.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.4.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.4.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.4.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.4.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.5.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.5.att.value.weight
rwkv [stdout] 768   768   0    blocks.5.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.5.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.5.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.5.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.5.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.6.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.6.att.value.weight
rwkv [stdout] 768   768   0    blocks.6.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.6.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.6.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.6.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.6.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.7.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.7.att.value.weight
rwkv [stdout] 768   768   0    blocks.7.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.7.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.7.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.7.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.7.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.8.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.8.att.value.weight
rwkv [stdout] 768   768   0    blocks.8.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.8.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.8.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.8.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.8.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.9.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.9.att.value.weight
rwkv [stdout] 768   768   0    blocks.9.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.9.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.9.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.9.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.9.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.10.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.10.att.value.weight
rwkv [stdout] 768   768   0    blocks.10.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.10.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.10.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.10.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.10.ffn.value.weight
rwkv [stdout] 768   768   0    blocks.11.att.key.weight
rwkv [stdout] 768   768   1.0  blocks.11.att.value.weight
rwkv [stdout] 768   768   0    blocks.11.att.receptance.weight
rwkv [stdout] 768   768   0    blocks.11.att.output.weight
rwkv [stdout] 3072  768   1.0  blocks.11.ffn.key.weight
rwkv [stdout] 768   768   0    blocks.11.ffn.receptance.weight
rwkv [stdout] 768   3072  0    blocks.11.ffn.value.weight
rwkv [stdout] 13    768   0.5  head.weight
rwkv [stderr] GPU available: True (cuda), used: True
rwkv [stderr] TPU available: False, using: 0 TPU cores
rwkv [stderr] IPU available: False, using: 0 IPUs
rwkv [stderr] HPU available: False, using: 0 HPUs
rwkv [stdout] 13    768   emb.weight
rwkv [stdout] 768         blocks.0.ln1.weight
rwkv [stdout] 768         blocks.0.ln1.bias
rwkv [stdout] 768         blocks.0.ln2.weight
rwkv [stdout] 768         blocks.0.ln2.bias
rwkv [stdout] 768         blocks.0.ln0.weight
rwkv [stdout] 768         blocks.0.ln0.bias
rwkv [stdout] 768         blocks.0.att.time_decay
rwkv [stdout] 768         blocks.0.att.time_first
rwkv [stdout] 768         blocks.0.att.time_mix_k
rwkv [stdout] 768         blocks.0.att.time_mix_v
rwkv [stdout] 768         blocks.0.att.time_mix_r
rwkv [stdout] 768   768   blocks.0.att.key.weight
rwkv [stdout] 768   768   blocks.0.att.value.weight
rwkv [stdout] 768   768   blocks.0.att.receptance.weight
rwkv [stdout] 768   768   blocks.0.att.output.weight
rwkv [stdout] 768         blocks.0.ffn.time_mix_k
rwkv [stdout] 768         blocks.0.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.0.ffn.key.weight
rwkv [stdout] 768   768   blocks.0.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.0.ffn.value.weight
rwkv [stdout] 768         blocks.1.ln1.weight
rwkv [stdout] 768         blocks.1.ln1.bias
rwkv [stdout] 768         blocks.1.ln2.weight
rwkv [stdout] 768         blocks.1.ln2.bias
rwkv [stdout] 768         blocks.1.att.time_decay
rwkv [stdout] 768         blocks.1.att.time_first
rwkv [stdout] 768         blocks.1.att.time_mix_k
rwkv [stdout] 768         blocks.1.att.time_mix_v
rwkv [stdout] 768         blocks.1.att.time_mix_r
rwkv [stdout] 768   768   blocks.1.att.key.weight
rwkv [stdout] 768   768   blocks.1.att.value.weight
rwkv [stdout] 768   768   blocks.1.att.receptance.weight
rwkv [stdout] 768   768   blocks.1.att.output.weight
rwkv [stdout] 768         blocks.1.ffn.time_mix_k
rwkv [stdout] 768         blocks.1.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.1.ffn.key.weight
rwkv [stdout] 768   768   blocks.1.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.1.ffn.value.weight
rwkv [stdout] 768         blocks.2.ln1.weight
rwkv [stdout] 768         blocks.2.ln1.bias
rwkv [stdout] 768         blocks.2.ln2.weight
rwkv [stdout] 768         blocks.2.ln2.bias
rwkv [stdout] 768         blocks.2.att.time_decay
rwkv [stdout] 768         blocks.2.att.time_first
rwkv [stdout] 768         blocks.2.att.time_mix_k
rwkv [stdout] 768         blocks.2.att.time_mix_v
rwkv [stdout] 768         blocks.2.att.time_mix_r
rwkv [stdout] 768   768   blocks.2.att.key.weight
rwkv [stdout] 768   768   blocks.2.att.value.weight
rwkv [stdout] 768   768   blocks.2.att.receptance.weight
rwkv [stdout] 768   768   blocks.2.att.output.weight
rwkv [stdout] 768         blocks.2.ffn.time_mix_k
rwkv [stdout] 768         blocks.2.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.2.ffn.key.weight
rwkv [stdout] 768   768   blocks.2.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.2.ffn.value.weight
rwkv [stdout] 768         blocks.3.ln1.weight
rwkv [stdout] 768         blocks.3.ln1.bias
rwkv [stdout] 768         blocks.3.ln2.weight
rwkv [stdout] 768         blocks.3.ln2.bias
rwkv [stdout] 768         blocks.3.att.time_decay
rwkv [stdout] 768         blocks.3.att.time_first
rwkv [stdout] 768         blocks.3.att.time_mix_k
rwkv [stdout] 768         blocks.3.att.time_mix_v
rwkv [stdout] 768         blocks.3.att.time_mix_r
rwkv [stdout] 768   768   blocks.3.att.key.weight
rwkv [stdout] 768   768   blocks.3.att.value.weight
rwkv [stdout] 768   768   blocks.3.att.receptance.weight
rwkv [stdout] 768   768   blocks.3.att.output.weight
rwkv [stdout] 768         blocks.3.ffn.time_mix_k
rwkv [stdout] 768         blocks.3.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.3.ffn.key.weight
rwkv [stdout] 768   768   blocks.3.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.3.ffn.value.weight
rwkv [stdout] 768         blocks.4.ln1.weight
rwkv [stdout] 768         blocks.4.ln1.bias
rwkv [stdout] 768         blocks.4.ln2.weight
rwkv [stdout] 768         blocks.4.ln2.bias
rwkv [stdout] 768         blocks.4.att.time_decay
rwkv [stdout] 768         blocks.4.att.time_first
rwkv [stdout] 768         blocks.4.att.time_mix_k
rwkv [stdout] 768         blocks.4.att.time_mix_v
rwkv [stdout] 768         blocks.4.att.time_mix_r
rwkv [stdout] 768   768   blocks.4.att.key.weight
rwkv [stdout] 768   768   blocks.4.att.value.weight
rwkv [stdout] 768   768   blocks.4.att.receptance.weight
rwkv [stdout] 768   768   blocks.4.att.output.weight
rwkv [stdout] 768         blocks.4.ffn.time_mix_k
rwkv [stdout] 768         blocks.4.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.4.ffn.key.weight
rwkv [stdout] 768   768   blocks.4.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.4.ffn.value.weight
rwkv [stdout] 768         blocks.5.ln1.weight
rwkv [stdout] 768         blocks.5.ln1.bias
rwkv [stdout] 768         blocks.5.ln2.weight
rwkv [stdout] 768         blocks.5.ln2.bias
rwkv [stdout] 768         blocks.5.att.time_decay
rwkv [stdout] 768         blocks.5.att.time_first
rwkv [stdout] 768         blocks.5.att.time_mix_k
rwkv [stdout] 768         blocks.5.att.time_mix_v
rwkv [stdout] 768         blocks.5.att.time_mix_r
rwkv [stdout] 768   768   blocks.5.att.key.weight
rwkv [stdout] 768   768   blocks.5.att.value.weight
rwkv [stdout] 768   768   blocks.5.att.receptance.weight
rwkv [stdout] 768   768   blocks.5.att.output.weight
rwkv [stdout] 768         blocks.5.ffn.time_mix_k
rwkv [stdout] 768         blocks.5.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.5.ffn.key.weight
rwkv [stdout] 768   768   blocks.5.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.5.ffn.value.weight
rwkv [stdout] 768         blocks.6.ln1.weight
rwkv [stdout] 768         blocks.6.ln1.bias
rwkv [stdout] 768         blocks.6.ln2.weight
rwkv [stdout] 768         blocks.6.ln2.bias
rwkv [stdout] 768         blocks.6.att.time_decay
rwkv [stdout] 768         blocks.6.att.time_first
rwkv [stdout] 768         blocks.6.att.time_mix_k
rwkv [stdout] 768         blocks.6.att.time_mix_v
rwkv [stdout] 768         blocks.6.att.time_mix_r
rwkv [stdout] 768   768   blocks.6.att.key.weight
rwkv [stdout] 768   768   blocks.6.att.value.weight
rwkv [stdout] 768   768   blocks.6.att.receptance.weight
rwkv [stdout] 768   768   blocks.6.att.output.weight
rwkv [stdout] 768         blocks.6.ffn.time_mix_k
rwkv [stdout] 768         blocks.6.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.6.ffn.key.weight
rwkv [stdout] 768   768   blocks.6.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.6.ffn.value.weight
rwkv [stdout] 768         blocks.7.ln1.weight
rwkv [stdout] 768         blocks.7.ln1.bias
rwkv [stdout] 768         blocks.7.ln2.weight
rwkv [stdout] 768         blocks.7.ln2.bias
rwkv [stdout] 768         blocks.7.att.time_decay
rwkv [stdout] 768         blocks.7.att.time_first
rwkv [stdout] 768         blocks.7.att.time_mix_k
rwkv [stdout] 768         blocks.7.att.time_mix_v
rwkv [stdout] 768         blocks.7.att.time_mix_r
rwkv [stdout] 768   768   blocks.7.att.key.weight
rwkv [stdout] 768   768   blocks.7.att.value.weight
rwkv [stdout] 768   768   blocks.7.att.receptance.weight
rwkv [stdout] 768   768   blocks.7.att.output.weight
rwkv [stdout] 768         blocks.7.ffn.time_mix_k
rwkv [stdout] 768         blocks.7.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.7.ffn.key.weight
rwkv [stdout] 768   768   blocks.7.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.7.ffn.value.weight
rwkv [stdout] 768         blocks.8.ln1.weight
rwkv [stdout] 768         blocks.8.ln1.bias
rwkv [stdout] 768         blocks.8.ln2.weight
rwkv [stdout] 768         blocks.8.ln2.bias
rwkv [stdout] 768         blocks.8.att.time_decay
rwkv [stdout] 768         blocks.8.att.time_first
rwkv [stdout] 768         blocks.8.att.time_mix_k
rwkv [stdout] 768         blocks.8.att.time_mix_v
rwkv [stdout] 768         blocks.8.att.time_mix_r
rwkv [stdout] 768   768   blocks.8.att.key.weight
rwkv [stdout] 768   768   blocks.8.att.value.weight
rwkv [stdout] 768   768   blocks.8.att.receptance.weight
rwkv [stdout] 768   768   blocks.8.att.output.weight
rwkv [stdout] 768         blocks.8.ffn.time_mix_k
rwkv [stdout] 768         blocks.8.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.8.ffn.key.weight
rwkv [stdout] 768   768   blocks.8.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.8.ffn.value.weight
rwkv [stdout] 768         blocks.9.ln1.weight
rwkv [stdout] 768         blocks.9.ln1.bias
rwkv [stdout] 768         blocks.9.ln2.weight
rwkv [stdout] 768         blocks.9.ln2.bias
rwkv [stdout] 768         blocks.9.att.time_decay
rwkv [stdout] 768         blocks.9.att.time_first
rwkv [stdout] 768         blocks.9.att.time_mix_k
rwkv [stdout] 768         blocks.9.att.time_mix_v
rwkv [stdout] 768         blocks.9.att.time_mix_r
rwkv [stdout] 768   768   blocks.9.att.key.weight
rwkv [stdout] 768   768   blocks.9.att.value.weight
rwkv [stdout] 768   768   blocks.9.att.receptance.weight
rwkv [stdout] 768   768   blocks.9.att.output.weight
rwkv [stdout] 768         blocks.9.ffn.time_mix_k
rwkv [stdout] 768         blocks.9.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.9.ffn.key.weight
rwkv [stdout] 768   768   blocks.9.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.9.ffn.value.weight
rwkv [stdout] 768         blocks.10.ln1.weight
rwkv [stdout] 768         blocks.10.ln1.bias
rwkv [stdout] 768         blocks.10.ln2.weight
rwkv [stdout] 768         blocks.10.ln2.bias
rwkv [stdout] 768         blocks.10.att.time_decay
rwkv [stdout] 768         blocks.10.att.time_first
rwkv [stdout] 768         blocks.10.att.time_mix_k
rwkv [stdout] 768         blocks.10.att.time_mix_v
rwkv [stdout] 768         blocks.10.att.time_mix_r
rwkv [stdout] 768   768   blocks.10.att.key.weight
rwkv [stdout] 768   768   blocks.10.att.value.weight
rwkv [stdout] 768   768   blocks.10.att.receptance.weight
rwkv [stdout] 768   768   blocks.10.att.output.weight
rwkv [stdout] 768         blocks.10.ffn.time_mix_k
rwkv [stdout] 768         blocks.10.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.10.ffn.key.weight
rwkv [stdout] 768   768   blocks.10.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.10.ffn.value.weight
rwkv [stdout] 768         blocks.11.ln1.weight
rwkv [stdout] 768         blocks.11.ln1.bias
rwkv [stdout] 768         blocks.11.ln2.weight
rwkv [stdout] 768         blocks.11.ln2.bias
rwkv [stdout] 768         blocks.11.att.time_decay
rwkv [stdout] 768         blocks.11.att.time_first
rwkv [stdout] 768         blocks.11.att.time_mix_k
rwkv [stdout] 768         blocks.11.att.time_mix_v
rwkv [stdout] 768         blocks.11.att.time_mix_r
rwkv [stdout] 768   768   blocks.11.att.key.weight
rwkv [stdout] 768   768   blocks.11.att.value.weight
rwkv [stdout] 768   768   blocks.11.att.receptance.weight
rwkv [stdout] 768   768   blocks.11.att.output.weight
rwkv [stdout] 768         blocks.11.ffn.time_mix_k
rwkv [stdout] 768         blocks.11.ffn.time_mix_r
rwkv [stdout] 3072  768   blocks.11.ffn.key.weight
rwkv [stdout] 768   768   blocks.11.ffn.receptance.weight
rwkv [stdout] 768   3072  blocks.11.ffn.value.weight
rwkv [stdout] 768         ln_out.weight
rwkv [stdout] 768         ln_out.bias
rwkv [stdout] 13    768   head.weight
rwkv [stderr] [rank: 0] Global seed set to 1234
rwkv [stderr] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
rwkv [stderr] ----------------------------------------------------------------------------------------------------
rwkv [stderr] distributed_backend=nccl
rwkv [stderr] All distributed processes registered. Starting with 1 processes
rwkv [stderr] ----------------------------------------------------------------------------------------------------
rwkv [stderr] 
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
rwkv [stderr] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
rwkv [stderr] Using /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
rwkv [stderr] Creating extension directory /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118/fused_adam...
rwkv [stderr] Detected CUDA files, patching ldflags
rwkv [stderr] Emitting ninja build file /Tmp/slurm.4115007.0/base/cache/torch_extensions/py39_cu118/fused_adam/build.ninja...
rwkv [stderr] Building extension module fused_adam...
rwkv [stderr] Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
rwkv [stdout] [1/3] /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/TH -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/THC -isystem /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/include -isystem /Tmp/slurm.4115007.0/env/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o
rwkv [stdout] [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/TH -isystem /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/include/THC -isystem /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/include -isystem /Tmp/slurm.4115007.0/env/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o
rwkv [stdout] [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.8/lib64 -lcudart -o fused_adam.so
rwkv [stderr] Loading extension module fused_adam...
rwkv [stdout] Time to load fused_adam op: 21.610455989837646 seconds
rwkv [stderr] /Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
rwkv [stderr]   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
rwkv [stderr] 
rwkv [stderr]   | Name   | Type       | Params
rwkv [stderr] --------------------------------------
rwkv [stderr] 0 | emb    | Embedding  | 10.0 K
rwkv [stderr] 1 | blocks | ModuleList | 92.1 M
rwkv [stderr] 2 | ln_out | LayerNorm  | 1.5 K
rwkv [stderr] 3 | head   | Linear     | 10.0 K
rwkv [stderr] --------------------------------------
rwkv [stderr] 92.1 M    Trainable params
rwkv [stderr] 0         Non-trainable params
rwkv [stderr] 92.1 M    Total params
rwkv [stderr] 368.548   Total estimated model params size (MB)
rwkv [stderr] SLURM auto-requeueing enabled. Setting signal handlers.
rwkv [stdout] ================================================================================
rwkv [stdout] Done
rwkv [end] /Tmp/slurm.4115007.0/milabench/benchmarks/rwkv/prepare.py --data_type dummy --ctx_len 128 --epoch_steps 1000 --epoch_count 20 --epoch_begin 0 --epoch_save 0 --micro_bsz 16 --n_layer 12 --n_embd 768 --pre_ffn 0 --head_qk 0 --lr_init 6e-4 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.99 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision tf32 --strategy ddp_find_unused_parameters_false --grad_cp 0 --random_seed 1234 --enable_progress_bar False [at 2024-02-06 12:04:42.700989]
[DONE] Reports directory: /Tmp/slurm.4115007.0/base/runs/prepare.2024-02-06_11:57:01.236402
---
Virtual Env
===========
commands.sh: line 145: export: `BASH_FUNC_module%%=() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}': not a valid identifier
commands.sh: line 147: export: `BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd "$@")
}': not a valid identifier
---
Milabench
=========
---
llama
=====
Dataset
Dataset
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
  table = cls._concat_blocks(blocks, axis=0)
Tokenizer
Model
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.
  table = cls._concat_blocks(blocks, axis=0)
Tokenizer
Model
Pipeline
Pipeline
Starting
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Starting
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =9.71549940109253, total / elapsed =204.62149375218377 in_token_count =9 out_token_count =1979
[7;227461736;223:2022747261696>222<202272617465223:203230342>36323134393337353231383337372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393136302>3038383434357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373033372>343337352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2032372<2022706?776572223:2038372>3431337=7=2<202274223:20313730373233393135302>34333135317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373035392>343337352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2032372<2022706?776572223:2038362>3335367=7=2<202274223:20313730373233393135302>393434313837397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373039352>343337352<2038313932302>305=2<20226<6?6164223:20302>37372<202274656=7065726174757265223:2033312<2022706?776572223:203236352>3932397=7=2<202274223:20313730373233393135312>3435373135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373137352>343337352<2038313932302>305=2<20226<6?6164223:20302>39322<202274656=7065726174757265223:2033322<2022706?776572223:203131372>3239337=7=2<202274223:20313730373233393135312>393639363232347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373332312>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2033342<2022706?776572223:203236362>3436327=7=2<202274223:20313730373233393135322>343832383535387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373338312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033342<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393135322>393935333437357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373434312>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2033352<2022706?776572223:203236342>3538337=7=2<202274223:20313730373233393135332>353039333036347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373438312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033352<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393135342>3032313638367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373532312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033362<2022706?776572223:203236352>3932397=7=2<202274223:20313730373233393135342>3533333733347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373536312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033362<2022706?776572223:203236362>3938387=7=2<202274223:20313730373233393135352>303438363331327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373632312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033362<2022706?776572223:203235382>3432377=7=2<202274223:20313730373233393135352>353630383632337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373732312>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2033372<2022706?776572223:203236392>36387=7=2<202274223:20313730373233393135362>303736363733337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373732312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033372<2022706?776572223:203236392>3932357=7=2<202274223:20313730373233393135362>353838373533327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373834312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033372<2022706?776572223:203237302>3231327=7=2<202274223:20313730373233393135372>313137313936387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373834312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033372<2022706?776572223:203236382>3836377=7=2<202274223:20313730373233393135372>363239373134337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32373834312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033382<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393135382>313432363938337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033382<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393135382>363536393537347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033382<2022706?776572223:203236392>3135337=7=2<202274223:20313730373233393135392>313639333333377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033382<2022706?776572223:203236372>3830387=7=2<202274223:20313730373233393135392>373133323138327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =9.656087875366211, total / elapsed =210.8514365526923 in_token_count =9 out_token_count =2027
[7;227461736;223:2022747261696>222<202272617465223:203231302>383531343336353532363932332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393136302>313338373131377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373033372>343337352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2033362<2022706?776572223:2039332>3034357=7=2<202274223:20313730373233393135302>3536333339337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373035392>343337352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2033362<2022706?776572223:2039332>3034357=7=2<202274223:20313730373233393135312>303739373538327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373039392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236362>3836337=7=2<202274223:20313730373233393135312>353932313037337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373138312>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034322<2022706?776572223:203235342>3231317=7=2<202274223:20313730373233393135322>313037383430357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373334312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236392>3230397=7=2<202274223:20313730373233393135322>363230303439357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373430312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203230362>3338367=7=2<202274223:20313730373233393135332>313630363236347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373434312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237312>3636377=7=2<202274223:20313730373233393135332>3637333438327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373438312>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3235367=7=2<202274223:20313730373233393135342>313837313537397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373536312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3636377=7=2<202274223:20313730373233393135342>373035393635357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373632312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237352>3432377=7=2<202274223:20313730373233393135352>323138363731387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373632312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237372>3833377=7=2<202274223:20313730373233393135352>373432363833367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373732312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237362>3732387=7=2<202274223:20313730373233393135362>323538303237367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373732312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203237362>3732387=7=2<202274223:20313730373233393135362>373732373531367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373834312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203237312>3930377=7=2<202274223:20313730373233393135372>323835333539317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373834312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203236382>3634387=7=2<202274223:20313730373233393135372>373937343139337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32373934312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203236382>3433357=7=2<202274223:20313730373233393135382>333433323334357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034372<2022706?776572223:203237372>3031357=7=2<202274223:20313730373233393135382>3835353435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034382<2022706?776572223:203237352>3432377=7=2<202274223:20313730373233393135392>3336393835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383036312>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034382<2022706?776572223:203237332>3235367=7=2<202274223:20313730373233393135392>383832323832377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.084743499755859, total / elapsed =386.8435054972673 in_token_count =185 out_token_count =1782
[7;227461736;223:2022747261696>222<202272617465223:203338362>383433353035343937323637332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393136352>313734313434357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2034342<2022706?776572223:203339362>3033357=7=2<202274223:20313730373233393136302>323330343430397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236352>3131367=7=2<202274223:20313730373233393136302>373433343430327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203235392>3438367=7=2<202274223:20313730373233393136312>323538373330347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236302>3534357=7=2<202274223:20313730373233393136312>3737353031387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236352>3039327=7=2<202274223:20313730373233393136322>333139353338367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236372>3532317=7=2<202274223:20313730373233393136322>3833313730337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236362>3938387=7=2<202274223:20313730373233393136332>3334353138357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236322>3431377=7=2<202274223:20313730373233393136332>383537343435357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2033392<2022706?776572223:203236372>3532317=7=2<202274223:20313730373233393136342>333731363837347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203236352>3131367=7=2<202274223:20313730373233393136342>393231393731367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.077516078948975, total / elapsed =396.6506395420199 in_token_count =185 out_token_count =1829
[7;227461736;223:2022747261696>222<202272617465223:203339362>363530363339353432303139392<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393136352>323136323837397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035312<2022706?776572223:203238362>3431387=7=2<202274223:20313730373233393136302>343033363037387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237382>3033397=7=2<202274223:20313730373233393136302>393432303936377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237322>3139347=7=2<202274223:20313730373233393136312>343534363136357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237352>3133397=7=2<202274223:20313730373233393136312>393638393438317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237342>3834347=7=2<202274223:20313730373233393136322>34383134317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237342>3834347=7=2<202274223:20313730373233393136322>393938343131347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237372>3031357=7=2<202274223:20313730373233393136332>353433383934387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034392<2022706?776572223:203237382>3336347=7=2<202274223:20313730373233393136342>303535383634387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203238322>3839387=7=2<202274223:20313730373233393136342>353638323630347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393136352>3038313338387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.479267597198486, total / elapsed =308.98554041287434 in_token_count =121 out_token_count =1881
[7;227461736;223:2022747261696>222<202272617465223:203330382>39383535343034313238373433342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393137312>363534343434327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034312<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393136352>343333393631367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393136352>393436383034387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203236322>3431377=7=2<202274223:20313730373233393136362>3435383834337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203236352>3631377=7=2<202274223:20313730373233393136362>393738383038327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203236392>36387=7=2<202274223:20313730373233393136372>353036333634337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034302<2022706?776572223:203236382>3034377=7=2<202274223:20313730373233393136382>303230303735337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236392>3932357=7=2<202274223:20313730373233393136382>353332383532367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236352>3430337=7=2<202274223:20313730373233393136392>303436383331317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393136392>353630333832317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236382>35387=7=2<202274223:20313730373233393137302>313032343631337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393137302>363235323034387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236322>3939387=7=2<202274223:20313730373233393137312>313339313131387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203237302>3231327=7=2<202274223:20313730373233393137312>3635323034327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.465346574783325, total / elapsed =312.8989260823651 in_token_count =121 out_token_count =1902
[7;227461736;223:2022747261696>222<202272617465223:203331322>383938393236303832333635312<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393137312>363831373232327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203237392>3432367=7=2<202274223:20313730373233393136352>353933383839327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203238332>3437337=7=2<202274223:20313730373233393136362>313331323831367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203238312>3330397=7=2<202274223:20313730373233393136362>363438383431347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203238322>33377=7=2<202274223:20313730373233393136372>313632373538387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035302<2022706?776572223:203237382>3037377=7=2<202274223:20313730373233393136372>363734383430327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203237382>3336347=7=2<202274223:20313730373233393136382>313839333539327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203237372>3833377=7=2<202274223:20313730373233393136382>373334323539347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203237372>3833377=7=2<202274223:20313730373233393136392>323437353539337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203238332>3637357=7=2<202274223:20313730373233393136392>373631303237367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203237392>39367=7=2<202274223:20313730373233393137302>323737313433327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203237342>3630357=7=2<202274223:20313730373233393137302>373931393231367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035312<2022706?776572223:203238342>3738317=7=2<202274223:20313730373233393137312>3333343933377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.343978404998779, total / elapsed =324.55974288588334 in_token_count =127 out_token_count =1932
[7;227461736;223:2022747261696>222<202272617465223:203332342>35353937343238383538383333342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393137372>393939353230357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236342>3538337=7=2<202274223:20313730373233393137322>313635333739357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236382>35387=7=2<202274223:20313730373233393137322>373036333839377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236392>3135337=7=2<202274223:20313730373233393137332>323238383236357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034312<2022706?776572223:203236372>3532317=7=2<202274223:20313730373233393137332>373431343138317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393137342>323535333631367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236312>3839317=7=2<202274223:20313730373233393137342>373637343539327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236352>3131367=7=2<202274223:20313730373233393137352>333036383536327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236352>3430337=7=2<202274223:20313730373233393137352>383230393832327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236322>3932397=7=2<202274223:20313730373233393137362>3333333232337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203235392>3438367=7=2<202274223:20313730373233393137362>383435363532337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203237312>3535387=7=2<202274223:20313730373233393137372>333538343332337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393137372>393034353835317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.330975770950317, total / elapsed =323.48883870275546 in_token_count =127 out_token_count =1921
[7;227461736;223:2022747261696>222<202272617465223:203332332>34383838333837303237353534362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393137382>3031323735387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035342<2022706?776572223:203335312>3036327=7=2<202274223:20313730373233393137312>38343639357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238332>3134327=7=2<202274223:20313730373233393137322>3336313337357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238332>3731397=7=2<202274223:20313730373233393137322>383734313939397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393137332>333837303235387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3032337=7=2<202274223:20313730373233393137332>3934323034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3036397=7=2<202274223:20313730373233393137342>343536393532367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3036397=7=2<202274223:20313730373233393137342>393639313439367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203237392>39327=7=2<202274223:20313730373233393137352>343831353738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238302>31367=7=2<202274223:20313730373233393137352>393934383230387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203237382>3630347=7=2<202274223:20313730373233393137362>353334303133357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203237372>3833377=7=2<202274223:20313730373233393137372>3034363237367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393137372>353638363932377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.949285745620728, total / elapsed =214.76574272315727 in_token_count =6 out_token_count =1916
[7;227461736;223:2022747261696>222<202272617465223:203231342>37363537343237323331353732372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393138362>393633303532357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3234377=7=2<202274223:20313730373233393137382>3038333137357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203237392>3432367=7=2<202274223:20313730373233393137382>3630303937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035322<2022706?776572223:203238302>3737357=7=2<202274223:20313730373233393137392>3133323138397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3233397=7=2<202274223:20313730373233393137392>363437383236347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393138302>313539393737377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238372>36377=7=2<202274223:20313730373233393138302>363735363434367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3432367=7=2<202274223:20313730373233393138312>313837363837327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238362>31337=7=2<202274223:20313730373233393138312>373237343630397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3533347=7=2<202274223:20313730373233393138322>323339363836337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3335367=7=2<202274223:20313730373233393138322>373534343034387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393138332>323639383331347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393138332>373832333637327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393138342>3330383632387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238362>36317=7=2<202274223:20313730373233393138342>383232373339347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035332<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393138352>333334373734337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238342>3738317=7=2<202274223:20313730373233393138352>383439313731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393138362>333632303134357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238312>3031357=7=2<202274223:20313730373233393138362>393033343734337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.969506740570068, total / elapsed =222.6432353261243 in_token_count =6 out_token_count =1991
[7;227461736;223:2022747261696>222<202272617465223:203232322>363433323335333236313234332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393138362>393639303936377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203235362>3439347=7=2<202274223:20313730373233393137382>343139313333327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203237302>3435327=7=2<202274223:20313730373233393137382>393331313635327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393137392>343436343838397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203237312>3739377=7=2<202274223:20313730373233393137392>393538363238347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236362>3137357=7=2<202274223:20313730373233393138302>343934343130357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393138312>303036343839337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034322<2022706?776572223:203236302>32347=7=2<202274223:20313730373233393138312>353138363330377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236382>3034377=7=2<202274223:20313730373233393138322>303330363737387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203237312>3739377=7=2<202274223:20313730373233393138322>353437333337337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236362>3137357=7=2<202274223:20313730373233393138332>303833393034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236362>3637357=7=2<202274223:20313730373233393138332>363034323035317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236302>3031327=7=2<202274223:20313730373233393138342>3131383634357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236362>3730317=7=2<202274223:20313730373233393138342>3633303831367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203237302>3435327=7=2<202274223:20313730373233393138352>313434393332377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236312>3538347=7=2<202274223:20313730373233393138352>3637373038347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393138362>313931323338367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383038352>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393138362>373033343837327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.5116212368011475, total / elapsed =621.0806499127865 in_token_count =256 out_token_count =1925
[7;227461736;223:2022747261696>222<202272617465223:203632312>303830363439393132373836352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139302>343735363739327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2035352<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393138372>3431373234387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238352>3739367=7=2<202274223:20313730373233393138372>3933313138337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393138382>343433333732357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238342>3533347=7=2<202274223:20313730373233393138382>3935353435367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238372>3233397=7=2<202274223:20313730373233393138392>343939373634327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238332>3437337=7=2<202274223:20313730373233393139302>3031323637357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.5189626216888428, total / elapsed =634.2778369520744 in_token_count =256 out_token_count =1976
[7;227461736;223:2022747261696>222<202272617465223:203633342>323737383336393532303734342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139302>343838313139317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034382<2022706?776572223:203336362>3934347=7=2<202274223:20313730373233393138372>323136323139377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237312>3737337=7=2<202274223:20313730373233393138372>373239383930367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393138382>323637353436327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236332>37377=7=2<202274223:20313730373233393138382>373831303335377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236392>36387=7=2<202274223:20313730373233393138392>3239367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236372>3237357=7=2<202274223:20313730373233393138392>383038313832377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034332<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393139302>333232343130337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.6794850826263428, total / elapsed =1348.627637977017 in_token_count =340 out_token_count =1925
[7;227461736;223:2022747261696>222<202272617465223:20313334382>3632373633373937373031372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139322>313536313733377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383131372>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2035362<2022706?776572223:203235352>3237337=7=2<202274223:20313730373233393139302>353234373837347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2035362<2022706?776572223:203238362>3332327=7=2<202274223:20313730373233393139312>3036373339357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238312>3330397=7=2<202274223:20313730373233393139312>353831373130387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393139322>303934303631317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.6830041408538818, total / elapsed =1325.0115943675555 in_token_count =340 out_token_count =1890
[7;227461736;223:2022747261696>222<202272617465223:20313332352>303131353934333637353535352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139322>3137313138327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2034372<2022706?776572223:203334332>3639357=7=2<202274223:20313730373233393139302>383631383231347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393139312>3430383434327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236302>3532377=7=2<202274223:20313730373233393139312>3932323435337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.021060466766357, total / elapsed =300.52440225910607 in_token_count =95 out_token_count =2015
[7;227461736;223:2022747261696>222<202272617465223:203330302>35323434303232353931303630372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139392>313738313730377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238312>3833367=7=2<202274223:20313730373233393139322>363131333035357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393139332>313235333233337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035342<2022706?776572223:203237372>3833377=7=2<202274223:20313730373233393139332>363633303731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238342>3030377=7=2<202274223:20313730373233393139342>313737313432367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238372>3437397=7=2<202274223:20313730373233393139342>3639303433377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393139352>323034313538337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238332>3138357=7=2<202274223:20313730373233393139352>373136353436357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238322>33377=7=2<202274223:20313730373233393139362>3235363635397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238332>3138357=7=2<202274223:20313730373233393139362>373639323937387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238322>3635387=7=2<202274223:20313730373233393139372>3238313434367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393139372>373935343639387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393139382>333037363637377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238382>37337=7=2<202274223:20313730373233393139382>383433383835377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.0354368686676025, total / elapsed =288.2550206699631 in_token_count =95 out_token_count =1933
[7;227461736;223:2022747261696>222<202272617465223:203238382>323535303230363639393633312<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393139392>323037333038337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034342<2022706?776572223:203236322>3430337=7=2<202274223:20313730373233393139322>34333630357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393139322>3935303634347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237322>3631377=7=2<202274223:20313730373233393139332>343637333333367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236312>3837317=7=2<202274223:20313730373233393139332>3938303934367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236352>3634327=7=2<202274223:20313730373233393139342>3439343034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3436327=7=2<202274223:20313730373233393139352>303036393137357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393139352>353230353232367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203235392>3234377=7=2<202274223:20313730373233393139362>3034333635367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236352>3932397=7=2<202274223:20313730373233393139362>353535393139327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393139372>303730303233387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3333347=7=2<202274223:20313730373233393139372>353832373233347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393139382>303939323535367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3830387=7=2<202274223:20313730373233393139382>363237313731357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236342>3237337=7=2<202274223:20313730373233393139392>313431353539367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.973330974578857, total / elapsed =234.36113144134856 in_token_count =5 out_token_count =2098
[7;227461736;223:2022747261696>222<202272617465223:203233342>33363131333134343133343835362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393230382>3135323436377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393139392>333536353437387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238362>3332327=7=2<202274223:20313730373233393139392>383639323135357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238352>3539367=7=2<202274223:20313730373233393230302>333831343932317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203239302>3336357=7=2<202274223:20313730373233393230302>383935353237347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238352>3539367=7=2<202274223:20313730373233393230312>343332343037367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238392>36357=7=2<202274223:20313730373233393230312>393434353936387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393230322>343539303930377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238352>3534397=7=2<202274223:20313730373233393230322>393732303732347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238332>3437337=7=2<202274223:20313730373233393230332>343835383935327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238332>3338387=7=2<202274223:20313730373233393230332>393937383831347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238382>3239347=7=2<202274223:20313730373233393230342>353131373438387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393230352>303233393932387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238362>36317=7=2<202274223:20313730373233393230352>353338333836337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238372>3736377=7=2<202274223:20313730373233393230362>303530353637327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3031377=7=2<202274223:20313730373233393230362>353632373439347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393230372>3037363236367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393230372>353838363632397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035352<2022706?776572223:203238352>3838337=7=2<202274223:20313730373233393230382>313034383531357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.992095947265625, total / elapsed =226.30986278775373 in_token_count =5 out_token_count =2030
[7;227461736;223:2022747261696>222<202272617465223:203232362>33303938363237383737353337332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393230382>323030303533327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393139392>363536353838387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236332>3734387=7=2<202274223:20313730373233393230302>3137313435337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393230302>363833353633377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393230312>323231393335337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393230312>373335313430337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393230322>323437323538327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237302>3935347=7=2<202274223:20313730373233393230322>373631343639347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3637357=7=2<202274223:20313730373233393230332>323734313438357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236392>3932357=7=2<202274223:20313730373233393230332>383136303033337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3830387=7=2<202274223:20313730373233393230342>333238363439357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3137357=7=2<202274223:20313730373233393230342>383431393430397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393230352>333534313837337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3830387=7=2<202274223:20313730373233393230352>383638313534387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393230362>3339353934357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203237302>3733387=7=2<202274223:20313730373233393230362>3930383031317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236342>3832327=7=2<202274223:20313730373233393230372>343139393836357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3532317=7=2<202274223:20313730373233393230372>393430373339327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.578835964202881, total / elapsed =566.3852773010447 in_token_count =253 out_token_count =1774
[7;227461736;223:2022747261696>222<202272617465223:203536362>333835323737333031303434372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231312>373332333337357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393230382>3636363739347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393230392>3138383935327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238352>3236327=7=2<202274223:20313730373233393230392>3730343431387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393231302>3231373738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393231302>373239393137387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393231312>32343238377=z[0:z/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.5856056213378906, total / elapsed =562.805900345233 in_token_count =253 out_token_count =1765
[7;227461736;223:2022747261696>222<202272617465223:203536322>3830353930303334353233332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231312>373835373233347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034382<2022706?776572223:203337322>3137347=7=2<202274223:20313730373233393230382>3436353732377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3830387=7=2<202274223:20313730373233393230382>393738393133387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3137357=7=2<202274223:20313730373233393230392>3439323333377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393231302>303036343433337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393231302>353230323932357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393231312>303332363533387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393231312>353531343638367=z[0:z/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =2.972625494003296, total / elapsed =712.8378614375332 in_token_count =282 out_token_count =1837
[7;227461736;223:2022747261696>222<202272617465223:203731322>383337383631343337353333322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231342>373035393031367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203237392>3731337=7=2<202274223:20313730373233393231312>3737393039367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238342>3738317=7=2<202274223:20313730373233393231322>3331313233317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238372>3437397=7=2<202274223:20313730373233393231322>383533313231387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238372>3437397=7=2<202274223:20313730373233393231332>333731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203237392>3133387=7=2<202274223:20313730373233393231332>3838343932357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393231342>3339393032377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =2.977924346923828, total / elapsed =696.4582569541853 in_token_count =282 out_token_count =1792
[7;227461736;223:2022747261696>222<202272617465223:203639362>343538323536393534313835332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231342>373633373235387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035302<2022706?776572223:203336342>3638337=7=2<202274223:20313730373233393231322>303838383639367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3836337=7=2<202274223:20313730373233393231322>363031323032377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>3430337=7=2<202274223:20313730373233393231332>313133373632397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393231332>363236303131347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236312>3837317=7=2<202274223:20313730373233393231342>313631343735347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393231342>363737313036347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.5106115341186523, total / elapsed =586.5075016102336 in_token_count =256 out_token_count =1803
[7;227461736;223:2022747261696>222<202272617465223:203538362>353037353031363130323333362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231382>323137343734357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2036332<2022706?776572223:203339362>3134377=7=2<202274223:20313730373233393231342>393439393935337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393231352>343635353135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393231352>39373833357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238332>3931357=7=2<202274223:20313730373233393231362>3439323035357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238352>3236327=7=2<202274223:20313730373233393231372>3030343235327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393231372>3531393035387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393231382>3034343731357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.516043186187744, total / elapsed =598.3999309977912 in_token_count =256 out_token_count =1848
[7;227461736;223:2022747261696>222<202272617465223:203539382>333939393330393937373931322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393231382>323830353535327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393231352>3138393335367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393231352>373031363034387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393231362>323136303637367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393231362>373434383638357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3634327=7=2<202274223:20313730373233393231372>323537353231397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3231327=7=2<202274223:20313730373233393231372>373639363831377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =9.002534627914429, total / elapsed =224.38125300140757 in_token_count =5 out_token_count =2015
[7;227461736;223:2022747261696>222<202272617465223:203232342>33383132353330303134303735372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393232372>323230393838387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393231382>3535363635347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238362>3038337=7=2<202274223:20313730373233393231392>303639393231377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>3430357=7=2<202274223:20313730373233393231392>3538313832317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238342>3537397=7=2<202274223:20313730373233393232302>303936323833377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239302>3531367=7=2<202274223:20313730373233393232302>363038343739337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238302>3537357=7=2<202274223:20313730373233393232312>3132323530387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238372>3038377=7=2<202274223:20313730373233393232312>363334313839317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>3639337=7=2<202274223:20313730373233393232322>313637303731387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238352>3430327=7=2<202274223:20313730373233393232322>3637393133397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035362<2022706?776572223:203238362>3735327=7=2<202274223:20313730373233393232332>313931303633347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238342>3035317=7=2<202274223:20313730373233393232332>373033373536337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238322>3738337=7=2<202274223:20313730373233393232342>323136323330397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035362<2022706?776572223:203238382>3130337=7=2<202274223:20313730373233393232342>373239343834367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239312>3039357=7=2<202274223:20313730373233393232352>323431363132327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035362<2022706?776572223:203238392>3430357=7=2<202274223:20313730373233393232352>373536303036377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238332>30337=7=2<202274223:20313730373233393232362>323933343231337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238352>3733357=7=2<202274223:20313730373233393232362>3831313935367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =9.16187858581543, total / elapsed =227.5734152631133 in_token_count =5 out_token_count =2080
[7;227461736;223:2022747261696>222<202272617465223:203232372>353733343135323633313133332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393232372>343433333032367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3932357=7=2<202274223:20313730373233393231382>323836343535327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3339337=7=2<202274223:20313730373233393231382>383034373839337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393231392>33343434367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236342>3839337=7=2<202274223:20313730373233393231392>383538353237377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3936327=7=2<202274223:20313730373233393232302>333732323530367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034342<2022706?776572223:203236382>3336317=7=2<202274223:20313730373233393232302>393138333639357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>32347=7=2<202274223:20313730373233393232312>343330313537347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3134317=7=2<202274223:20313730373233393232312>393439353731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3438367=7=2<202274223:20313730373233393232322>343633363532347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39312<202274656=7065726174757265223:2034342<2022706?776572223:203235362>3836327=7=2<202274223:20313730373233393232322>393736343936377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39312<202274656=7065726174757265223:2034342<2022706?776572223:203235352>3530317=7=2<202274223:20313730373233393232332>343936383738367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2034342<2022706?776572223:203236312>3432357=7=2<202274223:20313730373233393232342>303038393430327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034342<2022706?776572223:203235382>3231317=7=2<202274223:20313730373233393232342>3532323637357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034342<2022706?776572223:203236312>3434337=7=2<202274223:20313730373233393232352>30333439317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034342<2022706?776572223:203236352>3432367=7=2<202274223:20313730373233393232352>353439313038337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034342<2022706?776572223:203235362>3835367=7=2<202274223:20313730373233393232362>303631343635377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034342<2022706?776572223:203235332>3932347=7=2<202274223:20313730373233393232362>353735323731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383135392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034342<2022706?776572223:203236302>3037377=7=2<202274223:20313730373233393232372>303837333032377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.4791243076324463, total / elapsed =1408.9417564476007 in_token_count =349 out_token_count =1735
[7;227461736;223:2022747261696>222<202272617465223:20313430382>393431373536343437363030372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393232382>373030393939377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036312<2022706?776572223:203336332>3938357=7=2<202274223:20313730373233393232372>333234383830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3430327=7=2<202274223:20313730373233393232372>383337393832347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3433397=7=2<202274223:20313730373233393232382>333439393636387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.506643295288086, total / elapsed =1397.8092934049866 in_token_count =349 out_token_count =1757
[7;227461736;223:2022747261696>222<202272617465223:20313339372>383039323933343034393836362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393232382>393530303139387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035302<2022706?776572223:203335372>3236367=7=2<202274223:20313730373233393232372>363432393231347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3833327=7=2<202274223:20313730373233393232382>313535313037357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3336317=7=2<202274223:20313730373233393232382>363637323734377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =2.872809410095215, total / elapsed =791.9077374243907 in_token_count =287 out_token_count =1988
[7;227461736;223:2022747261696>222<202272617465223:203739312>393037373337343234333930372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393233312>353734373930327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2036322<2022706?776572223:203338362>3738397=7=2<202274223:20313730373233393232382>383632313633387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>32387=7=2<202274223:20313730373233393232392>333734353236337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3939327=7=2<202274223:20313730373233393232392>3838373431327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3430327=7=2<202274223:20313730373233393233302>343031393435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035362<2022706?776572223:203238372>3831357=7=2<202274223:20313730373233393233302>393135343633347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238312>3134337=7=2<202274223:20313730373233393233312>343337313438367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =2.9347219467163086, total / elapsed =787.4681288241983 in_token_count =287 out_token_count =2024
[7;227461736;223:2022747261696>222<202272617465223:203738372>343638313238383234313938332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393233312>383834383330327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2035312<2022706?776572223:203336392>3035317=7=2<202274223:20313730373233393232392>313838393932337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3236347=7=2<202274223:20313730373233393232392>37303235317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3737347=7=2<202274223:20313730373233393233302>323431343830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034352<2022706?776572223:203236322>3139387=7=2<202274223:20313730373233393233302>373539393137377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3731337=7=2<202274223:20313730373233393233312>3237323332377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3930317=7=2<202274223:20313730373233393233312>383132343039327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.994917869567871, total / elapsed =220.45782171163572 in_token_count =7 out_token_count =1976
[7;227461736;223:2022747261696>222<202272617465223:203232302>34353738323137313136333537322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393234302>3537303739357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39312<202274656=7065726174757265223:2035362<2022706?776572223:203237392>3031357=7=2<202274223:20313730373233393233312>393531333139377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39332<202274656=7065726174757265223:2035362<2022706?776572223:203237322>3534327=7=2<202274223:20313730373233393233322>34363434397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39322<202274656=7065726174757265223:2035362<2022706?776572223:203238302>3238377=7=2<202274223:20313730373233393233322>3937373338327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2035362<2022706?776572223:203238322>3435337=7=2<202274223:20313730373233393233332>343930343539377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238332>3531367=7=2<202274223:20313730373233393233342>3030333534317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239322>3434387=7=2<202274223:20313730373233393233342>353137333536367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239312>3931397=7=2<202274223:20313730373233393233352>303331313432327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238342>3836377=7=2<202274223:20313730373233393233352>353433393931387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238382>3130337=7=2<202274223:20313730373233393233362>303538323835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393233362>353733323935347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238322>33377=7=2<202274223:20313730373233393233372>303837373036367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238312>3539377=7=2<202274223:20313730373233393233372>363038373530337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238352>3539367=7=2<202274223:20313730373233393233382>313230373530377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203238312>3539377=7=2<202274223:20313730373233393233382>363430323531327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239312>3233397=7=2<202274223:20313730373233393233392>313532333032337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393233392>363636343335327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035362<2022706?776572223:203239302>3431377=7=2<202274223:20313730373233393234302>313738363435317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.941888809204102, total / elapsed =228.25155216637782 in_token_count =7 out_token_count =2034
[7;227461736;223:2022747261696>222<202272617465223:203232382>32353135353231363633373738322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393234302>383236373931337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3134317=7=2<202274223:20313730373233393233322>3332353236357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3035367=7=2<202274223:20313730373233393233322>383338383739337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3439367=7=2<202274223:20313730373233393233332>333531343231367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3833347=7=2<202274223:20313730373233393233332>383636353537347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3736387=7=2<202274223:20313730373233393233342>333738383030347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3833347=7=2<202274223:20313730373233393233342>383930393236347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3538387=7=2<202274223:20313730373233393233352>34303335387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3538397=7=2<202274223:20313730373233393233352>393235393931387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3832347=7=2<202274223:20313730373233393233362>343430333038337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393233362>3935333336397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3231357=7=2<202274223:20313730373233393233372>343636343639357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>3932397=7=2<202274223:20313730373233393233372>393739353137357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236302>3532377=7=2<202274223:20313730373233393233382>343932383635337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3231357=7=2<202274223:20313730373233393233392>3030363932337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3637357=7=2<202274223:20313730373233393233392>353139333930367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393234302>303332343739387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383230332>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393234302>3534353733317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.1536359786987305, total / elapsed =1907.8808572550479 in_token_count =363 out_token_count =1838
[7;227461736;223:2022747261696>222<202272617465223:20313930372>383830383537323535303437392<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393234312>3732353332357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2036312<2022706?776572223:203332312>3437337=7=2<202274223:20313730373233393234302>3639323533337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393234312>323335323330347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.1548726558685303, total / elapsed =1910.1673148031737 in_token_count =363 out_token_count =1843
[7;227461736;223:2022747261696>222<202272617465223:20313931302>313637333134383033313733372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393234312>3938313732357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2035302<2022706?776572223:203333392>34367=7=2<202274223:20313730373233393234312>303731393130317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3231357=7=2<202274223:20313730373233393234312>353835313638367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.924768447875977, total / elapsed =225.55207025892958 in_token_count =7 out_token_count =2006
[7;227461736;223:2022747261696>222<202272617465223:203232352>35353230373032353839323935382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235302>363531303937387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3335367=7=2<202274223:20313730373233393234312>373437313935377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>31337=7=2<202274223:20313730373233393234322>3236343637377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393234322>3737363536377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393234332>323930353833367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238342>3030377=7=2<202274223:20313730373233393234332>383234383832357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393234342>3333393436337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3539367=7=2<202274223:20313730373233393234342>3835313730317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393234352>333635367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393234352>383737353837337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393234362>3431333232377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238342>3234377=7=2<202274223:20313730373233393234362>393235323937357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3838337=7=2<202274223:20313730373233393234372>343337363036337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3137377=7=2<202274223:20313730373233393234372>393439353739327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3031377=7=2<202274223:20313730373233393234382>343633373130357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3335357=7=2<202274223:20313730373233393234382>393938353136337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3635377=7=2<202274223:20313730373233393234392>353131323133357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393235302>3032333939337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393235302>353431393535327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.941908359527588, total / elapsed =225.79072820051428 in_token_count =7 out_token_count =2012
[7;227461736;223:2022747261696>222<202272617465223:203232352>37393037323832303035313432382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235302>3932333730337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034352<2022706?776572223:203239312>3033377=7=2<202274223:20313730373233393234322>303938383835387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393234322>3631313837337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393234332>313235333539387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393234332>363639363034337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393234342>3138323336387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393234342>363936323238337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393234352>323038363333347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3734387=7=2<202274223:20313730373233393234352>373233353231377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393234362>323631353939387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393234362>373735383834347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3935347=7=2<202274223:20313730373233393234372>323838303738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393234372>383030363733327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393234382>333132373037327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393234382>3835313538337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393234392>3336343936347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>32347=7=2<202274223:20313730373233393234392>383738313738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3039327=7=2<202274223:20313730373233393235302>3339303130357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.5885844230651855, total / elapsed =1309.9712988401932 in_token_count =344 out_token_count =1737
[7;227461736;223:2022747261696>222<202272617465223:20313330392>393731323938383430313933322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235322>323430363432387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203339362>3936367=7=2<202274223:20313730373233393235312>303534303537317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238322>3635387=7=2<202274223:20313730373233393235312>3633393530397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393235322>313536363332347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.5916545391082764, total / elapsed =1292.3658680057754 in_token_count =344 out_token_count =1713
[7;227461736;223:2022747261696>222<202272617465223:20313239322>333635383638303035373735342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235322>353136333534337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236312>3837317=7=2<202274223:20313730373233393235302>393238303139357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034372<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393235312>343430353739347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393235312>393634383137387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393235322>343830333237317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.444571495056152, total / elapsed =302.4250722480375 in_token_count =122 out_token_count =1827
[7;227461736;223:2022747261696>222<202272617465223:203330322>343235303732323438303337352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235382>363836323034377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393235322>363638393632357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3230337=7=2<202274223:20313730373233393235332>3138303937367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3532367=7=2<202274223:20313730373233393235332>363933333336377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3431377=7=2<202274223:20313730373233393235342>3232383338327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238322>3635387=7=2<202274223:20313730373233393235342>3734323632327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>36317=7=2<202274223:20313730373233393235352>3235393233387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3335367=7=2<202274223:20313730373233393235352>373734363830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3038337=7=2<202274223:20313730373233393235362>323836383835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3036397=7=2<202274223:20313730373233393235362>383237313434397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393235372>333339303838327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3032337=7=2<202274223:20313730373233393235372>383532363733337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393235382>333635303038367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.456521987915039, total / elapsed =316.11446593386205 in_token_count =122 out_token_count =1919
[7;227461736;223:2022747261696>222<202272617465223:203331362>31313434363539333338363230352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393235382>393732393531347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393235322>3939323237387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393235332>353037333534337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393235342>3032313332367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393235342>353334363833327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3237337=7=2<202274223:20313730373233393235352>303437343535387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236312>3131397=7=2<202274223:20313730373233393235352>353631333234387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3131367=7=2<202274223:20313730373233393235362>3037353735377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393235362>363130323236327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393235372>3132353239377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236302>3534357=7=2<202274223:20313730373233393235372>363337343337337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393235382>313439343632357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393235382>363632323538347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.94690227508545, total / elapsed =229.4648959916568 in_token_count =6 out_token_count =2047
[7;227461736;223:2022747261696>222<202272617465223:203232392>343634383935393931363536382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393236372>363334323138357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393235382>38373930347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3532367=7=2<202274223:20313730373233393235392>343131353039387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3431377=7=2<202274223:20313730373233393235392>393235343735317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3230337=7=2<202274223:20313730373233393236302>343337343734357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393236302>393532343836387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238342>3733367=7=2<202274223:20313730373233393236312>343634343337377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393236312>393935343435337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3832377=7=2<202274223:20313730373233393236322>353037343331377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3335367=7=2<202274223:20313730373233393236332>303233353030377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393236332>3533353632387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3832377=7=2<202274223:20313730373233393236342>303532373834347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3239347=7=2<202274223:20313730373233393236342>3538363139327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393236352>303938393930347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>30367=7=2<202274223:20313730373233393236352>3631313134377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3432357=7=2<202274223:20313730373233393236362>313234383030327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393236362>363337333533347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393236372>313732303439357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.96792197227478, total / elapsed =219.00279753457943 in_token_count =6 out_token_count =1958
[7;227461736;223:2022747261696>222<202272617465223:203231392>30303237393735333435373934332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393236372>393430393633357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393235392>323035393234337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393235392>373138373435327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393236302>323331383738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393236302>373533373635367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3436317=7=2<202274223:20313730373233393236312>3236373839347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3730317=7=2<202274223:20313730373233393236312>373835333933327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393236322>323939353433347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3436317=7=2<202274223:20313730373233393236322>383131373937317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393236332>333236303535387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393236332>3833383633387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>31317=7=2<202274223:20313730373233393236342>333737323131337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>31317=7=2<202274223:20313730373233393236342>383931313033337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203235372>3833387=7=2<202274223:20313730373233393236352>343034313838327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3039327=7=2<202274223:20313730373233393236352>393137393138377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>35387=7=2<202274223:20313730373233393236362>343331303037347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393236362>393538333737347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>3932397=7=2<202274223:20313730373233393236372>3437313331397=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.110610723495483, total / elapsed =271.2847144937965 in_token_count =91 out_token_count =1838
[7;227461736;223:2022747261696>222<202272617465223:203237312>323834373134343933373936352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393237342>373435383338347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203339352>3739317=7=2<202274223:20313730373233393236372>363834313039327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393236382>313938313033327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>3233397=7=2<202274223:20313730373233393236382>373130323039387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3336357=7=2<202274223:20313730373233393236392>323233373137377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3731397=7=2<202274223:20313730373233393236392>373335383534347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393237302>3235313831377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>3134337=7=2<202274223:20313730373233393237302>373634313031337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>31337=7=2<202274223:20313730373233393237312>323736363739357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393237312>373930353030347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>337=7=2<202274223:20313730373233393237322>3331303238367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393237322>383234323230347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3432357=7=2<202274223:20313730373233393237332>33343434327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393237332>383536393430337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393237342>333730313431377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.125333309173584, total / elapsed =281.671033889189 in_token_count =91 out_token_count =1916
[7;227461736;223:2022747261696>222<202272617465223:203238312>3637313033333838393138392<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393237352>3036363336347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393236372>393833353334367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3836377=7=2<202274223:20313730373233393236382>343938323132337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3239387=7=2<202274223:20313730373233393236392>303139373434397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3237337=7=2<202274223:20313730373233393236392>353437353735327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393237302>303539353435357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393237302>353734333136337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>32347=7=2<202274223:20313730373233393237312>303836313239347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393237312>3630313735367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393237322>313331363533387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236312>3538347=7=2<202274223:20313730373233393237322>363435313633337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393237332>313537333933357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393237332>3638313034377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393237342>3139343533337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393237342>3732363831347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.590300559997559, total / elapsed =376.187286788909 in_token_count =162 out_token_count =1941
[7;227461736;223:2022747261696>222<202272617465223:203337362>3138373238363738383930392<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393238302>333337323238337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2036322<2022706?776572223:203339342>3439337=7=2<202274223:20313730373233393237342>383832363439377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393237352>3339363832387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239352>3437347=7=2<202274223:20313730373233393237352>393130373933337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393237362>3432343535357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3538387=7=2<202274223:20313730373233393237362>393336383836387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3839387=7=2<202274223:20313730373233393237372>343733323736367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>37337=7=2<202274223:20313730373233393237372>393931343132367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>36357=7=2<202274223:20313730373233393237382>3530363031367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>36317=7=2<202274223:20313730373233393237392>303231313836367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393237392>353334323337347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3236327=7=2<202274223:20313730373233393238302>303730323537347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.602091550827026, total / elapsed =385.2132690836547 in_token_count =162 out_token_count =1996
[7;227461736;223:2022747261696>222<202272617465223:203338352>323133323639303833363534372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393238302>363638353532327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035302<2022706?776572223:203334312>3137357=7=2<202274223:20313730373233393237352>323432313932337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393237352>373534323836387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393237362>323637393833377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393237362>3738303439337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393237372>333230303730357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393237372>3833323937377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393237382>333435333830357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3634327=7=2<202274223:20313730373233393237382>3835383834357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393237392>3337313635347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3231357=7=2<202274223:20313730373233393237392>3931373538387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>35367=7=2<202274223:20313730373233393238302>343239353031387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.050517320632935, total / elapsed =406.2950129122301 in_token_count =186 out_token_count =1866
[7;227461736;223:2022747261696>222<202272617465223:203430362>323935303132393132323330312<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393238352>3338383732317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2036302<2022706?776572223:203330322>3639377=7=2<202274223:20313730373233393238302>353834333430337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393238312>303938323638357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239342>3635347=7=2<202274223:20313730373233393238312>363130383737387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238312>3330397=7=2<202274223:20313730373233393238322>313235303031377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393238322>363639313837357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393238332>313831323536357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>36357=7=2<202274223:20313730373233393238332>363935323631357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3739367=7=2<202274223:20313730373233393238342>323037333132337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3538387=7=2<202274223:20313730373233393238342>37323132357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>30367=7=2<202274223:20313730373233393238352>323632303837387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.059572696685791, total / elapsed =412.09013586577237 in_token_count =186 out_token_count =1899
[7;227461736;223:2022747261696>222<202272617465223:203431322>30393031333538363537373233372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393238352>3732383232317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034372<2022706?776572223:203238302>30337=7=2<202274223:20313730373233393238302>393431383331367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393238312>343535343335337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3938387=7=2<202274223:20313730373233393238312>393736313730337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393238322>353136343736367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3938367=7=2<202274223:20313730373233393238332>303330333039347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3335367=7=2<202274223:20313730373233393238332>353433353131327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3335367=7=2<202274223:20313730373233393238342>303537343435337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393238342>353639353334357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393238352>313039373237317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393238352>363235323039337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.555857419967651, total / elapsed =319.1039502519142 in_token_count =117 out_token_count =1975
[7;227461736;223:2022747261696>222<202272617465223:203331392>313033393530323531393134322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393239312>393435353930377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393238352>373734333230387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>36377=7=2<202274223:20313730373233393238362>323935373935377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393238362>383038363436377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393238372>333232353735337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393238372>383531363533387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238322>3839387=7=2<202274223:20313730373233393238382>333634393835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>31337=7=2<202274223:20313730373233393238382>383738383637317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393238392>333931313636377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>337=7=2<202274223:20313730373233393238392>393033383638347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>3233397=7=2<202274223:20313730373233393239302>343431343030337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393239302>393537333035377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393239312>343639373237337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.571342468261719, total / elapsed =309.22144292648835 in_token_count =117 out_token_count =1915
[7;227461736;223:2022747261696>222<202272617465223:203330392>32323134343239323634383833352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393239322>323939363435377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393238362>313337363737377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393238362>363531303633347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3634327=7=2<202274223:20313730373233393238372>313633353538327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393238372>363934383031387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236322>36397=7=2<202274223:20313730373233393238382>323036393837367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393238382>3732303537387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393238392>3233333330327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3935347=7=2<202274223:20313730373233393238392>373437313737387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393239302>323933313237387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236352>3333317=7=2<202274223:20313730373233393239302>3830353231347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393239312>333139363533337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393239312>383331373736317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.94868016242981, total / elapsed =218.46797119958353 in_token_count =6 out_token_count =1949
[7;227461736;223:2022747261696>222<202272617465223:203231382>34363739373131393935383335332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393330302>383935333336327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035372<2022706?776572223:203239322>30367=7=2<202274223:20313730373233393239312>3938333732367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393239322>343936323932367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393239332>3033313730347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238322>33377=7=2<202274223:20313730373233393239332>3534343933377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393239342>303537313038367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238322>3034317=7=2<202274223:20313730373233393239342>353639313036337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238382>3030367=7=2<202274223:20313730373233393239352>3038323734337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393239352>363137303235317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238342>3533347=7=2<202274223:20313730373233393239362>313239373437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238392>3331317=7=2<202274223:20313730373233393239362>363431393035357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393239372>313536343630337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239352>3731347=7=2<202274223:20313730373233393239372>363639323334357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239362>3334377=7=2<202274223:20313730373233393239382>323232343639387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3833337=7=2<202274223:20313730373233393239382>373334363835347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393239392>323439363130347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393239392>373631373731377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393330302>323735343831377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393330302>383133323735367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.967316627502441, total / elapsed =220.46729050880296 in_token_count =6 out_token_count =1971
[7;227461736;223:2022747261696>222<202272617465223:203232302>34363732393035303838303239362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393330312>323637303335357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393239322>333436303532327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393239322>383839373537367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203235382>3635377=7=2<202274223:20313730373233393239332>343035313531317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393239332>3932313330377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393239342>343333313939367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393239342>3934373636357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393239352>343738333933337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393239352>393934313631317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237332>3131377=7=2<202274223:20313730373233393239362>353036333335357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237322>3538357=7=2<202274223:20313730373233393239372>303231313538377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203235372>3330367=7=2<202274223:20313730373233393239372>353333343036377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393239382>303636383037337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393239382>353830333532357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393239392>303935313738367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393239392>3630373837397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393330302>313230363538367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393330302>363537383531357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393330312>313639393036397=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.110152721405029, total / elapsed =290.00783538620396 in_token_count =91 out_token_count =1971
[7;227461736;223:2022747261696>222<202272617465223:203239302>30303738333533383632303339362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393330382>303036363435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3431377=7=2<202274223:20313730373233393330312>3332373538387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3539337=7=2<202274223:20313730373233393330312>383339373337377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238382>37337=7=2<202274223:20313730373233393330322>333536323037387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3131357=7=2<202274223:20313730373233393330322>383735343133327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>3335357=7=2<202274223:20313730373233393330332>343135343834347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393330332>393237343139327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393330342>343431393337347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393330342>393534303333317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393330352>343734353638317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393330362>303134303733387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239352>3532357=7=2<202274223:20313730373233393330362>353335393930357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3131357=7=2<202274223:20313730373233393330372>303438303435397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393330372>353634333938387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =7.124405860900879, total / elapsed =290.129456456686 in_token_count =91 out_token_count =1976
[7;227461736;223:2022747261696>222<202272617465223:203239302>3132393435363435363638362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393330382>333932323739397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393330312>363834333630377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3436317=7=2<202274223:20313730373233393330322>3230343138357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3231327=7=2<202274223:20313730373233393330322>3731363437367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3131377=7=2<202274223:20313730373233393330332>323539333131347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3131377=7=2<202274223:20313730373233393330332>373731333333377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>31317=7=2<202274223:20313730373233393330342>3238333335397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>36397=7=2<202274223:20313730373233393330342>3739353632367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236312>3538347=7=2<202274223:20313730373233393330352>333038313439387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393330352>383538333137317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3034377=7=2<202274223:20313730373233393330362>333730333239347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393330362>3838353135397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>3430337=7=2<202274223:20313730373233393330372>333937313438317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393330372>393130313937377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.878129005432129, total / elapsed =222.56941736158308 in_token_count =9 out_token_count =1967
[7;227461736;223:2022747261696>222<202272617465223:203232322>35363934313733363135383330382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393331362>383835393430337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2035392<2022706?776572223:203234312>3832337=7=2<202274223:20313730373233393330382>303736343333327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239362>32347=7=2<202274223:20313730373233393330382>363033313334397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238372>36377=7=2<202274223:20313730373233393330392>313230383735347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3833337=7=2<202274223:20313730373233393330392>363337363132387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238382>3832387=7=2<202274223:20313730373233393331302>313439373336367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3731397=7=2<202274223:20313730373233393331302>3636343035347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238332>3931357=7=2<202274223:20313730373233393331312>313938313031387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3731397=7=2<202274223:20313730373233393331312>373132343031397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393331322>323330323233347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3730347=7=2<202274223:20313730373233393331322>3734333939347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3935317=7=2<202274223:20313730373233393331332>323537303439337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393331332>373835313638327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393331342>3239373131317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3432357=7=2<202274223:20313730373233393331342>3831353639397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393331352>333330313537337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393331352>3834343138327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3839387=7=2<202274223:20313730373233393331362>333836323432397=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.900080442428589, total / elapsed =228.20018460931973 in_token_count =9 out_token_count =2022
[7;227461736;223:2022747261696>222<202272617465223:203232382>32303031383436303933313937332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393331372>3239323433387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203333352>33327=7=2<202274223:20313730373233393330382>343532323031347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3131377=7=2<202274223:20313730373233393330382>393634323530387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393330392>343830353730387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3131377=7=2<202274223:20313730373233393330392>393932353538357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393331302>353134383630367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393331312>303430353432317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393331312>353532353436357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>3137357=7=2<202274223:20313730373233393331322>303635303234367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393331322>353738343831327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393331332>3039303339367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393331332>363238363438337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393331342>313437303838357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393331342>363630323630377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393331352>313734323039347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236392>3135337=7=2<202274223:20313730373233393331352>363837353036327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203236332>3436317=7=2<202274223:20313730373233393331362>323332383538327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034352<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393331362>373435373131337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393331372>323537383637387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.1765854358673096, total / elapsed =609.1446425937803 in_token_count =273 out_token_count =1662
[7;227461736;223:2022747261696>222<202272617465223:203630392>313434363432353933373830332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332302>3036333439347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035372<2022706?776572223:203238352>3739367=7=2<202274223:20313730373233393331362>393030343834367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39382<202274656=7065726174757265223:2035392<2022706?776572223:203238382>3135337=7=2<202274223:20313730373233393331372>343133393335377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393331372>39333836397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238372>3736377=7=2<202274223:20313730373233393331382>343530363530357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393331382>393738393733327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>3036387=7=2<202274223:20313730373233393331392>343937343532337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393332302>303039373830327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.1843204498291016, total / elapsed =594.1613068815172 in_token_count =273 out_token_count =1619
[7;227461736;223:2022747261696>222<202272617465223:203539342>313631333036383831353137322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332302>343737353739367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393331372>383338313832327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3838317=7=2<202274223:20313730373233393331382>333530333430317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393331382>383633343436357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393331392>333737303835327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393331392>3838393736347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3339387=7=2<202274223:20313730373233393332302>3430313733317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.2655043601989746, total / elapsed =648.9043548148514 in_token_count =269 out_token_count =1850
[7;227461736;223:2022747261696>222<202272617465223:203634382>393034333534383134383531342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332332>333330303239327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036302<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393332302>3534303837357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3334387=7=2<202274223:20313730373233393332312>30353736387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239372>3334387=7=2<202274223:20313730373233393332312>353935353335387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239382>3934327=7=2<202274223:20313730373233393332322>313132343530317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239372>3933367=7=2<202274223:20313730373233393332322>363237323932367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393332332>313435343434327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.273555278778076, total / elapsed =646.0865389111341 in_token_count =269 out_token_count =1846
[7;227461736;223:2022747261696>222<202272617465223:203634362>303836353338393131313334312<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332332>373532303433327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2034372<2022706?776572223:203237352>3531397=7=2<202274223:20313730373233393332302>393134373132347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3237337=7=2<202274223:20313730373233393332312>343236373032337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3333317=7=2<202274223:20313730373233393332312>393431323933377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393332322>3435393239327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393332322>393733393834357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3135327=7=2<202274223:20313730373233393332332>343933343332387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =4.483115196228027, total / elapsed =406.85994451685394 in_token_count =213 out_token_count =1611
[7;227461736;223:2022747261696>222<202272617465223:203430362>38353939343435313638353339342<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332372>383134333033327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036312<2022706?776572223:203330352>3436327=7=2<202274223:20313730373233393332332>3635383538337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393332342>313832373136387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393332342>363935303930357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3233397=7=2<202274223:20313730373233393332352>323132303837327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393332352>3732343332387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239342>3934317=7=2<202274223:20313730373233393332362>323337373133387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393332362>373539353834347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393332372>323731363034337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239382>3639367=7=2<202274223:20313730373233393332372>373837303732377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =4.494894027709961, total / elapsed =405.7937715006117 in_token_count =213 out_token_count =1611
[7;227461736;223:2022747261696>222<202272617465223:203430352>373933373731353030363131372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393332382>3234373031397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2034392<2022706?776572223:203338312>3233397=7=2<202274223:20313730373233393332342>303239363036387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393332342>3534323536367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393332352>3035343339347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>31317=7=2<202274223:20313730373233393332352>353638363237367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236312>3334367=7=2<202274223:20313730373233393332362>303830363731387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>3938377=7=2<202274223:20313730373233393332362>353934323932347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3239387=7=2<202274223:20313730373233393332372>313036323939347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393332372>363230363839367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393332382>313332383335367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.835041284561157, total / elapsed =233.04927885260832 in_token_count =11 out_token_count =2048
[7;227461736;223:2022747261696>222<202272617465223:203233332>30343932373838353236303833322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393333362>363530333631337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393332382>323939343536387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3634397=7=2<202274223:20313730373233393332382>383133383939387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239362>3334377=7=2<202274223:20313730373233393332392>333530363338347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239352>3437347=7=2<202274223:20313730373233393332392>383735343839377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239312>3731397=7=2<202274223:20313730373233393333302>3338373339317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3131357=7=2<202274223:20313730373233393333302>393030383934397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239362>3030317=7=2<202274223:20313730373233393333312>3431323735387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238372>36377=7=2<202274223:20313730373233393333312>393437303237347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393333322>343539323734337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393333322>3937323935397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239382>34377=7=2<202274223:20313730373233393333332>3438353632337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239382>3639367=7=2<202274223:20313730373233393333342>3030313032347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393333342>353339303339317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393333352>3035313739337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239352>3731347=7=2<202274223:20313730373233393333352>3536343231337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393333362>3037383038367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035382<2022706?776572223:203238392>3031377=7=2<202274223:20313730373233393333362>353930313838357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.853619813919067, total / elapsed =224.54092696352285 in_token_count =11 out_token_count =1977
[7;227461736;223:2022747261696>222<202272617465223:203232342>35343039323639363335323238352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393333372>313030373130397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393332382>3634373131327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393332392>313630323139327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393332392>363735303337367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393333302>313837313539387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236312>3837317=7=2<202274223:20313730373233393333302>373135373831357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393333312>323238353132337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3634327=7=2<202274223:20313730373233393333312>373435333837367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393333322>3235373634377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393333322>373733323735347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3832347=7=2<202274223:20313730373233393333332>333134393535327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>3137357=7=2<202274223:20313730373233393333332>383335323834327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>377=7=2<202274223:20313730373233393333342>333437343536357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393333342>383539383837317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393333352>333733353432357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393333352>393132323938347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393333362>343234333835357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3836337=7=2<202274223:20313730373233393333362>393338383930377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.88587498664856, total / elapsed =341.83532687391323 in_token_count =148 out_token_count =1864
[7;227461736;223:2022747261696>222<202272617465223:203334312>38333533323638373339313332332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393334322>353337323737327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3833337=7=2<202274223:20313730373233393333372>313239313035367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3036317=7=2<202274223:20313730373233393333372>363432383834357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393333382>3135343936377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>34377=7=2<202274223:20313730373233393333382>363639323239357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393333392>313831383337387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3136397=7=2<202274223:20313730373233393333392>3732323832327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3538377=7=2<202274223:20313730373233393334302>323334383034367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393334302>373438373030347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393334312>3236303638377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393334312>3738303138387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3437347=7=2<202274223:20313730373233393334322>333137343133367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =5.896437883377075, total / elapsed =328.67301213559915 in_token_count =148 out_token_count =1790
[7;227461736;223:2022747261696>222<202272617465223:203332382>36373330313231333535393931352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393334322>393937323137377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034372<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393333372>3435303839327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393333372>3936343639317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3634327=7=2<202274223:20313730373233393333382>3530373433317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236312>3538347=7=2<202274223:20313730373233393333392>303139383430377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393333392>3533353238387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393334302>303437353335327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393334302>3536323736357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>3936327=7=2<202274223:20313730373233393334312>303931303431337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393334312>363033313737387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3733337=7=2<202274223:20313730373233393334322>313234353734327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393334322>3634303932327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.711548566818237, total / elapsed =303.20871252589893 in_token_count =110 out_token_count =1925
[7;227461736;223:2022747261696>222<202272617465223:203330332>32303837313235323538393839332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393334392>323439383236327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035392<2022706?776572223:203239392>3436397=7=2<202274223:20313730373233393334322>3832393534377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3832377=7=2<202274223:20313730373233393334332>333435363331317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3532357=7=2<202274223:20313730373233393334332>3835383630367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3334387=7=2<202274223:20313730373233393334342>333731333839327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393334342>393034363534377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3336367=7=2<202274223:20313730373233393334352>343235373437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238392>3031377=7=2<202274223:20313730373233393334352>393430363430377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3538387=7=2<202274223:20313730373233393334362>343532393831327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393334362>393636393838337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3731317=7=2<202274223:20313730373233393334372>3530343538347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3934317=7=2<202274223:20313730373233393334382>303136383230377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3330367=7=2<202274223:20313730373233393334382>3533313039387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393334392>3034333034357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.7264509201049805, total / elapsed =301.9422908341539 in_token_count =110 out_token_count =1921
[7;227461736;223:2022747261696>222<202272617465223:203330312>393432323930383334313533392<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393334392>373234353538387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2034382<2022706?776572223:203332352>3134347=7=2<202274223:20313730373233393334332>3135323836387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393334332>363830383332317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>31357=7=2<202274223:20313730373233393334342>313934313130397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3935347=7=2<202274223:20313730373233393334342>373037353133337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393334352>3232323237317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393334352>373334333335347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393334362>323736323234397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3734387=7=2<202274223:20313730373233393334362>373838343334337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3333317=7=2<202274223:20313730373233393334372>333032383237317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393334372>383135343737367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393334382>333239373338317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393334382>383638373032377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3037377=7=2<202274223:20313730373233393334392>333831343736397=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.945547580718994, total / elapsed =223.79848544040613 in_token_count =6 out_token_count =1996
[7;227461736;223:2022747261696>222<202272617465223:203232332>37393834383534343034303631332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393335382>313936333339367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393334392>353537303437317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3432357=7=2<202274223:20313730373233393335302>313030353637367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393335302>3631323635377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3334377=7=2<202274223:20313730373233393335312>313236373433387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393335312>363338383835337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3634397=7=2<202274223:20313730373233393335322>3135333934337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3430397=7=2<202274223:20313730373233393335322>363932333931367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3731347=7=2<202274223:20313730373233393335332>3230353538357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3334387=7=2<202274223:20313730373233393335332>373137373334367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3430397=7=2<202274223:20313730373233393335342>323332303331367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238392>3031377=7=2<202274223:20313730373233393335342>373434313238377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393335352>323834383435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393335352>373937303337347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238392>36357=7=2<202274223:20313730373233393335362>333039323133397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393335362>383233343633327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3330367=7=2<202274223:20313730373233393335372>333335363139327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393335372>3837333030377=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.966439962387085, total / elapsed =212.01279526483412 in_token_count =6 out_token_count =1895
[7;227461736;223:2022747261696>222<202272617465223:203231322>30313237393532363438333431322<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393335382>363931383037377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3737337=7=2<202274223:20313730373233393334392>383933363136347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393335302>343038303733347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393335302>3932303138327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393335312>343536393533387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393335312>3937313939317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3734387=7=2<202274223:20313730373233393335322>3438343134357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393335322>393938343035327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3737337=7=2<202274223:20313730373233393335332>353130333437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393335342>3034393730327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3836377=7=2<202274223:20313730373233393335342>3536313932347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393335352>303736363231387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393335352>3538383932397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>3437397=7=2<202274223:20313730373233393335362>313033323233387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236302>32347=7=2<202274223:20313730373233393335362>363433393533337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393335372>313538303032367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393335372>3637303337397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393335382>313834373532357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.0618157386779785, total / elapsed =621.5266242055675 in_token_count =278 out_token_count =1625
[7;227461736;223:2022747261696>222<202272617465223:203632312>353236363234323035353637352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393336312>3235393232337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2036352<2022706?776572223:203430312>3432317=7=2<202274223:20313730373233393335382>333835373734397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036302<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393335382>383938333034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>31387=7=2<202274223:20313730373233393335392>3431303337317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>31387=7=2<202274223:20313730373233393335392>393233313739317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393336302>343730303335387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3437347=7=2<202274223:20313730373233393336302>393832303437387=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =3.0696775913238525, total / elapsed =651.2084544806858 in_token_count =278 out_token_count =1721
[7;227461736;223:2022747261696>222<202272617465223:203635312>323038343534343830363835382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393336312>3736323333367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393335382>363937313733387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034372<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393335392>323334323536337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3630397=7=2<202274223:20313730373233393335392>373437363238377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393336302>323631323735357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393336302>373734313630367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>35387=7=2<202274223:20313730373233393336312>323933393931367=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.946122407913208, total / elapsed =221.21315915032577 in_token_count =6 out_token_count =1973
[7;227461736;223:2022747261696>222<202272617465223:203232312>32313331353931353033323537372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337302>323036343030347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393336312>343934323931387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393336322>303039303836347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3838317=7=2<202274223:20313730373233393336322>353231323031347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>38327=7=2<202274223:20313730373233393336332>3035333937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3832377=7=2<202274223:20313730373233393336332>353636353535357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239392>3034357=7=2<202274223:20313730373233393336342>303738373332337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238382>37337=7=2<202274223:20313730373233393336342>3539323136317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3330367=7=2<202274223:20313730373233393336352>313034303136337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393336352>363530363133337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393336362>313632373135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3833337=7=2<202274223:20313730373233393336362>363734393330337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>32347=7=2<202274223:20313730373233393336372>313839333831367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3635347=7=2<202274223:20313730373233393336372>3730313736357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3132377=7=2<202274223:20313730373233393336382>323431303230347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3233397=7=2<202274223:20313730373233393336382>373533303637337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393336392>323638323838347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3030317=7=2<202274223:20313730373233393336392>373830333430327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.966529846191406, total / elapsed =226.28598073108867 in_token_count =6 out_token_count =2023
[7;227461736;223:2022747261696>222<202272617465223:203232362>32383539383037333130383836372<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337302>373239373637367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2034362<2022706?776572223:203234352>3030397=7=2<202274223:20313730373233393336312>383330373334357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393336322>3334343235367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>35387=7=2<202274223:20313730373233393336322>383536333939337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393336332>3337303439357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237372>3338397=7=2<202274223:20313730373233393336332>383833313237327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393336342>343137303636387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237332>3335367=7=2<202274223:20313730373233393336342>393239303536327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393336352>343434393038397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393336352>393537343335317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3938367=7=2<202274223:20313730373233393336362>343639383832327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>32347=7=2<202274223:20313730373233393336372>30313238367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3031317=7=2<202274223:20313730373233393336372>353234363539367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>377=7=2<202274223:20313730373233393336382>30333932347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393336382>353531313434347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393336392>303636383038357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393336392>363038343739337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3833387=7=2<202274223:20313730373233393337302>313230343830387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3439347=7=2<202274223:20313730373233393337302>363334343937327=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.58854341506958, total / elapsed =1392.4705985471048 in_token_count =344 out_token_count =1868
[7;227461736;223:2022747261696>222<202272617465223:20313339322>343730353938353437313034382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337312>373935383233337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036322<2022706?776572223:203239352>3731347=7=2<202274223:20313730373233393337302>323933303238387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036302<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393337302>383733343937327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3934327=7=2<202274223:20313730373233393337312>33383533397=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =1.5912888050079346, total / elapsed =1386.2976934529495 in_token_count =344 out_token_count =1862
[7;227461736;223:2022747261696>222<202272617465223:20313338362>323937363933343532393439352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337322>333231313237347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2034382<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393337312>313436343435357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203235392>3436397=7=2<202274223:20313730373233393337312>363539383936397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393337322>3137353930347=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.825048208236694, total / elapsed =307.3970961066713 in_token_count =105 out_token_count =1993
[7;227461736;223:2022747261696>222<202272617465223:203330372>333937303936313036363731332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337382>363231373930367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2036332<2022706?776572223:203237362>3139347=7=2<202274223:20313730373233393337312>383937353638327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393337322>343130323639357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3539337=7=2<202274223:20313730373233393337322>393233393132387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3538387=7=2<202274223:20313730373233393337332>343536353336337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3731317=7=2<202274223:20313730373233393337332>393638353931327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3432357=7=2<202274223:20313730373233393337342>343833393935377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3935397=7=2<202274223:20313730373233393337342>3939363134347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3336367=7=2<202274223:20313730373233393337352>353131393439357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393337362>303532313435327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3334387=7=2<202274223:20313730373233393337362>35363435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239342>3336367=7=2<202274223:20313730373233393337372>3038303130397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3634397=7=2<202274223:20313730373233393337372>353932333432367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239312>3532367=7=2<202274223:20313730373233393337382>3130373235317=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =6.841781854629517, total / elapsed =297.29097524845616 in_token_count =105 out_token_count =1929
[7;227461736;223:2022747261696>222<202272617465223:203239372>32393039373532343834353631362<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393337392>313632393932357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393337322>363838333832347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3133357=7=2<202274223:20313730373233393337332>323030383332367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3039327=7=2<202274223:20313730373233393337332>3731333130387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3336347=7=2<202274223:20313730373233393337342>3232363238387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393337342>373539383232387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236312>3334367=7=2<202274223:20313730373233393337352>323731393630357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236302>3532377=7=2<202274223:20313730373233393337352>373838313530357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236302>3831337=7=2<202274223:20313730373233393337362>333030333035347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236302>3831337=7=2<202274223:20313730373233393337362>3832313039377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393337372>333537363832357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203235382>3635377=7=2<202274223:20313730373233393337372>383730303335347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3031397=7=2<202274223:20313730373233393337382>3338333537367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3236357=7=2<202274223:20313730373233393337382>383935363738357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.704673051834106, total / elapsed =229.30212175854618 in_token_count =17 out_token_count =1979
[7;227461736;223:2022747261696>222<202272617465223:203232392>33303231323137353835343631382<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393338372>33323735327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3639367=7=2<202274223:20313730373233393337382>363435353738397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393337392>313630333839327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393337392>363735343033367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239392>3436397=7=2<202274223:20313730373233393338302>313837363239357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3437347=7=2<202274223:20313730373233393338302>373031303738347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3430397=7=2<202274223:20313730373233393338312>323436343436347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3833337=7=2<202274223:20313730373233393338312>373538343638327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238372>3134337=7=2<202274223:20313730373233393338322>323732373839377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3238367=7=2<202274223:20313730373233393338322>373834383434397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3334377=7=2<202274223:20313730373233393338332>323937343533327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3533357=7=2<202274223:20313730373233393338332>38303937317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3832327=7=2<202274223:20313730373233393338342>333233363430387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393338342>383335363738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3639367=7=2<202274223:20313730373233393338352>333530353634327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239352>3731347=7=2<202274223:20313730373233393338352>383632353931357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239332>3430397=7=2<202274223:20313730373233393338362>333737303137337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238362>3934357=7=2<202274223:20313730373233393338362>383839303930357=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =8.726301431655884, total / elapsed =232.2862688018973 in_token_count =17 out_token_count =2010
[7;227461736;223:2022747261696>222<202272617465223:203233322>323836323638383031383937332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393338372>383930313739347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39342<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393337392>343037373232357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3531327=7=2<202274223:20313730373233393337392>393531383532367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393338302>3436393438337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>31317=7=2<202274223:20313730373233393338302>393832363332397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236332>3233377=7=2<202274223:20313730373233393338312>3530313630357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393338322>3031333631377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393338322>353534373331387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3039327=7=2<202274223:20313730373233393338332>3037303237377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236352>3333317=7=2<202274223:20313730373233393338332>353834303237387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236312>3334367=7=2<202274223:20313730373233393338342>303936353932347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3935347=7=2<202274223:20313730373233393338342>363130353436367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393338352>313338353337327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236382>3535327=7=2<202274223:20313730373233393338352>363533313432377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236372>3230377=7=2<202274223:20313730373233393338362>313635333233337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236322>3430337=7=2<202274223:20313730373233393338362>363739303630327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3432317=7=2<202274223:20313730373233393338372>3139313233337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237342>3137357=7=2<202274223:20313730373233393338372>373537343236337=z[0:zSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.
elapsed =4.420693635940552, total / elapsed =476.62203570723403 in_token_count =216 out_token_count =1891
[7;227461736;223:2022747261696>222<202272617465223:203437362>36323230333537303732333430332<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393339312>373439323138327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036332<2022706?776572223:203339392>3038317=7=2<202274223:20313730373233393338372>3430333634377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2036302<2022706?776572223:203238342>3230327=7=2<202274223:20313730373233393338372>393438323730387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393338382>343539383234387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238392>38397=7=2<202274223:20313730373233393338382>393833353635387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393338392>343938363131327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393339302>303131323534337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239382>3132317=7=2<202274223:20313730373233393339302>353235353337357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239362>3030317=7=2<202274223:20313730373233393339312>3033393336337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2035392<2022706?776572223:203239372>3838327=7=2<202274223:20313730373233393339312>353735353735367=z[0:zelapsed =4.431237697601318, total / elapsed =484.74041488741125 in_token_count =216 out_token_count =1932
[7;227461736;223:2022747261696>222<202272617465223:203438342>37343034313438383734313132352<2022756>697473223:2022546?6;2?73222<202274223:20313730373233393339322>333231353637387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2034372<2022706?776572223:203236392>3839367=7=2<202274223:20313730373233393338382>3237323031337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237312>32347=7=2<202274223:20313730373233393338382>3738353130367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393338392>333031353334377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393338392>383133373437327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237302>3731357=7=2<202274223:20313730373233393339302>333438363239327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236362>3338387=7=2<202274223:20313730373233393339302>383630373333377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203236342>3830357=7=2<202274223:20313730373233393339312>333733323838347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;32383234392>343337352<2038313932302>305=2<20226<6?6164223:20302>39372<202274656=7065726174757265223:2034362<2022706?776572223:203237322>3131367=7=2<202274223:20313730373233393339312>3838353337347=z[0:z
real	5m8.730s
user	9m25.050s
sys	1m6.798s
---
fp16
====
[7;227461736;223:2022747261696>222<202272617465223:203233362>30373834333633333038373937382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339342>373534393934367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;3639342>3132352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2035302<2022706?776572223:203130342>3332387=7=2<202274223:20313730373233393339342>323230353538367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>31362<202274656=7065726174757265223:2036312<2022706?776572223:203433362>30347=7=2<202274223:20313730373233393339342>373432363838347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203234372>393830373732303331323934382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339342>373833303133387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;3639332>353632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2033382<2022706?776572223:2038392>3732347=7=2<202274223:20313730373233393339342>323137393233397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>33362<202274656=7065726174757265223:2034382<2022706?776572223:203430362>3030357=7=2<202274223:20313730373233393339342>373339393932347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>303533343838303633313732362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339342>393739393931347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343438323338313631373138382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>303036383531377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383335373435323030303735352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>3230333335377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343635353839373831343835392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>323330323139317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>31343132393430313433363638352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>3432363937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203435362>3632347=7=2<202274223:20313730373233393339352>323634333631347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383034313931343433363333342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>343533353732387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203431322>34317=7=2<202274223:20313730373233393339352>3236313535397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383133363537343939383035372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>363530343232387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>313830363530353532383534312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>363737323534327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343331353139343338303133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>3837343437377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037302<2022706?776572223:203436372>3534367=7=2<202274223:20313730373233393339352>383038353435347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33323134373534353435323636352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339352>393030373334327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203432302>3633357=7=2<202274223:20313730373233393339352>373834323137347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343434353236323436303035362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>303938303636337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33373838353230353738313531352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>313234333631337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34373835323539353133323339352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>333231343235347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037312<2022706?776572223:203437302>3735337=7=2<202274223:20313730373233393339362>333230373034377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343233303033303739383230382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>33343737367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035362<2022706?776572223:203432322>3236357=7=2<202274223:20313730373233393339362>323936333533387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343637313637333032353238382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>353434383737337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343133383536343237313635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>353731323536367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343631343838333035363037392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>3736393431397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313239313032353430343437332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>3739343636367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343033353137333535323431332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339362>393932383835367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037322<2022706?776572223:203437312>35377=7=2<202274223:20313730373233393339362>383530353936377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3336323435363436373733312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>303138313838327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035372<2022706?776572223:203432332>3032367=7=2<202274223:20313730373233393339362>383137373135367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3437343130383539353234322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>323136333533377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343136363934393832383934362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>323431363737387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343136303634313838303138342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>343339373637347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037332<2022706?776572223:203437332>36387=7=2<202274223:20313730373233393339372>333632373334387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33363638373034383630303634362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>343635313139387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035372<2022706?776572223:203432332>3630337=7=2<202274223:20313730373233393339372>333332353333387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343233303033303739383230382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>363633333539327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333834323132353439333932332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>363839383532357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343634363433323736393435372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>383836373333387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3432303437393830383732332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339372>393133323536347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035382<2022706?776572223:203432342>3336347=7=2<202274223:20313730373233393339372>383734373732337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343436333435333831303333372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>313130313138367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037342<2022706?776572223:203437342>3733357=7=2<202274223:20313730373233393339372>383935363339327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333938373138333834353039322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>313336373536347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343630323939313939343430372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>3333333639317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333839353733323335353335392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>3336303235377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33303838363831393937373935372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>353537313736387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037342<2022706?776572223:203437342>3733357=7=2<202274223:20313730373233393339382>343235323430337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313438303236303633343735362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>353833363635317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035382<2022706?776572223:203432352>3137397=7=2<202274223:20313730373233393339382>343133353535397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34373132363839333630373634362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>373831373931377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313133333333313233313138352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339382>383037313634327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343139323839383730383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>3030353138327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037352<2022706?776572223:203437372>3431387=7=2<202274223:20313730373233393339382>393434373133387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34303231383733383232353134342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>303330353735337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035392<2022706?776572223:203432342>3635367=7=2<202274223:20313730373233393339382>3933323438327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3434393138343536313135362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>3232383735397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313733323537383034363436342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>323534303531377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34373835323539353133323339352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>343532313231357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333832303035323634353931392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>343737343830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035392<2022706?776572223:203432352>3137397=7=2<202274223:20313730373233393339392>343438353433387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343539353935333535313436312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>363735343934377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037352<2022706?776572223:203437372>3934327=7=2<202274223:20313730373233393339392>343839393130367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333636353535313934363131362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>373032323336347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3432383939363032313433342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>383939303534387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343430303336323837323333362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393339392>393235363232357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343536373535393734393331392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>313232343635317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037352<2022706?776572223:203437382>3939377=7=2<202274223:20313730373233393430302>303031393139377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323836383035393733393235342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>3134393031377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035392<2022706?776572223:203432352>3439327=7=2<202274223:20313730373233393339392>393630363730357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343832363237393030323338322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>333435383936377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343137363431313830323539382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>333732343932387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383139393638323337363135362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>353639323531387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203438302>3333387=7=2<202274223:20313730373233393430302>3531373034327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323538343138313133323839352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>3539353838397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432362>3531377=7=2<202274223:20313730373233393430302>3437353330397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343730393533343231373630362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>373933393737357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33393330343230313834383932372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430302>383139333931377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343936353132323634373535332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>3031373332377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333735363938393138333538372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>303432393038377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432372>3034377=7=2<202274223:20313730373233393430312>3030303335317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343139323839383730383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>323430373134337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203438312>3434317=7=2<202274223:20313730373233393430312>303239303536337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323532313039373733393032342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>3236363339377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3434393138343536313135362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>343634323931387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343139383438393937363833352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>3438393831367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34393035313635383336343736362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>363837363430347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203438302>3836397=7=2<202274223:20313730373233393430312>353433313336367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333731393732383239333232332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>373134343537387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432372>3333327=7=2<202274223:20313730373233393430312>353132353333327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383335373435323030303735352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>393131303637357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323734313839303739363232352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430312>393337393335347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333634393738373437373333382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>31333435317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438312>3634347=7=2<202274223:20313730373233393430322>303834383430337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333533303436343337313032352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>313631333231397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432372>3034377=7=2<202274223:20313730373233393430322>303238363533397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333532363833303339353933362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>333538303433377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343330383838353739383237322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>3338343738397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343831303530323134313030392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>353831343035327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>313832383534383239333634362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>363038333637377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432362>3939387=7=2<202274223:20313730373233393430322>353432383737327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3433383737343530303830382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>383036303630367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438332>3337387=7=2<202274223:20313730373233393430322>353936393237367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>323837373533343539333931322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430322>383331393532337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343435333938393939373835372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>303239363432337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>32343936323835313830333135362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>303535343739387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383630393838363930343530372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>323533303437357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438332>3334347=7=2<202274223:20313730373233393430332>3131313137337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343134343837323132363133352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>323738383831337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432372>3233367=7=2<202274223:20313730373233393430332>303730313235387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343839353639393139333938332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>343736343836347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33393032303339313732383630342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>3530323337377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34393834303536383832383635342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>37303037347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037372<2022706?776572223:203438342>3433377=7=2<202274223:20313730373233393430332>363336353233327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>33373235343538343632323033362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>3732353931327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432372>3532327=7=2<202274223:20313730373233393430332>3538323333347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353032353038313839313739382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>393234313537367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333135313731363932353939352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430332>393439343833397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353131333434373331383630342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>3134373438347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343333343132303238373337342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>3137323837317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432382>3333397=7=2<202274223:20313730373233393430342>303935353433367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35303337373035323030363434352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>333730383139337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037382<2022706?776572223:203438342>3638387=7=2<202274223:20313730373233393430342>313438323730347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343033353137333535323431332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>333936333330347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34393032303130323832323339362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>3539343235377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343230313634343032383636352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>363139373237387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432382>3333397=7=2<202274223:20313730373233393430342>3630373437337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383733363130353937333832352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>383138363330357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037382<2022706?776572223:203437352>33357=7=2<202274223:20313730373233393430342>36363434347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343230313634343032383636352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430342>383433323032367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34393134363332353339363237362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>303432303539327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34303530323537313337333430362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>303636363133377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353032353038313839313739382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>323635333937387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037382<2022706?776572223:203438362>3630327=7=2<202274223:20313730373233393430352>313736383333397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343236343732363437393631372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>323930303034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432392>3130317=7=2<202274223:20313730373233393430352>313531363931347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343934333033333031333032382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>3438383831347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343033353137333535323431332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>353133343539377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35303032393931333630383235362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>373133333033387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037382<2022706?776572223:203438352>3530377=7=2<202274223:20313730373233393430352>363930383136347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343332373831313632343638342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>373336383437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432382>3836337=7=2<202274223:20313730373233393430352>3636343435357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>31383835323331323031333431362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>393336393734357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343032383138313137383637332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430352>393630333336347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343837393932313539313238352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>313630343335377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343130303731373731303437322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>313833383332327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203432392>3339347=7=2<202274223:20313730373233393430362>313736323237337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353332313735383136343934352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>333833373538337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203530322>3037367=7=2<202274223:20313730373233393430362>323032383730317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343333343132303238373337342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>343037333037367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353133383639353535343130322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>363037313837377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343139323839383730383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>363330363836337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>33363037343532393833323537352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>3833323532327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037392<2022706?776572223:203437302>3234377=7=2<202274223:20313730373233393430362>373137333039357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323233373232353830303533342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430362>383534303835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203432372>3532327=7=2<202274223:20313730373233393430362>363935393534387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3532313434343238343932392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>303535393330397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343330383838353739383237322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>303737353531367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>333833323933363338303130342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>323830313139377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203437392>3338387=7=2<202274223:20313730373233393430372>323239373031357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333632353039363032393030372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>333030393336327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203432392>3633327=7=2<202274223:20313730373233393430372>323038333032357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35333135343435323833353635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>3530333533337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333132303430303835383334352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>353234343031347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>343537353839343634303339392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>373239343938397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343433353036323535343738332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>373437373835387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35333533333232393736333935372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>393532383134367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037392<2022706?776572223:203530312>3032357=7=2<202274223:20313730373233393430372>373630343039367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323538343138313133323839352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430372>3937313137377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203432382>3537377=7=2<202274223:20313730373233393430372>373534313236337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35313730323536343535313931342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>3137363232337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313232373934373533333133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>3139343635357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>31333937313939373131313839352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>3339393833357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203530352>3636317=7=2<202274223:20313730373233393430382>323736323030387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3432303437393830383732332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>343138303530337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433302>3430337=7=2<202274223:20313730373233393430382>323638373032337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239322>383937383235303334333935322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>363235323439347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323538343138313133323839352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>363431353135377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>3631363439353538353731352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>38353033397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037392<2022706?776572223:203439342>3638327=7=2<202274223:20313730373233393430382>373934343235357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343233363333393034333330322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430382>383634393237357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433302>3430337=7=2<202274223:20313730373233393430382>373835343136317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383037333436373838393531352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>303733383237377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323134323630333033333334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>303838333935367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383335373435323030303735352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>3239373236347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343130373032353430333332372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>333131383033337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>373535313134323634323534362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>3532313932397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203530362>3139347=7=2<202274223:20313730373233393430392>333538303937367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343232303536383438313037392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>353335313937377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3634327=7=2<202274223:20313730373233393430392>333430343932357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>353236363938353138303937372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>373438303139327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34313534333333393538333631352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>373538363737377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>31383434323933333237343232342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>393731363030357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203530352>3133347=7=2<202274223:20313730373233393430392>3837313737347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343236373838303637323838342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393430392>393832303639337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203432392>3334367=7=2<202274223:20313730373233393430392>383532363830327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353337383537353331313033322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>313934393933377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333831343336313136333638362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>323035353331347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>303934313835333139363331362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>343139343137347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203437352>3131347=7=2<202274223:20313730373233393431302>333833363433397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343332373831313632343638342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>3432383931367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3634327=7=2<202274223:20313730373233393431302>333634353837337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>343834303436323932393032392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>363434333635387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343231313130363232343536352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>363532333836377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35333135343435323833353635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>3836383837347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343339373230383339363136382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431302>3837353737377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35323233393131353334323234362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>3039323230377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038312<2022706?776572223:203530372>3031357=7=2<202274223:20313730373233393431302>3931333430327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343439353030303238393832342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>3039393135337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3932397=7=2<202274223:20313730373233393431302>393130333838357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>37313138383837363336353636352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>3331363232357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333938373138333834353039322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>333232363436367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239322>373932313338363730353631332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>353431363334387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038312<2022706?776572223:203437392>3938337=7=2<202274223:20313730373233393431312>3433373231357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3430343339343936383639312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>353436313635357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3932397=7=2<202274223:20313730373233393431312>343332343738377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>38303435313638373830393332342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>3736363731347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>333330363136333837333537382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>373639373532337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3434323234343433393431332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>393930303936337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038312<2022706?776572223:203530372>3330327=7=2<202274223:20313730373233393431312>393536303731347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343434353236323436303035362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431312>3939333133337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3634327=7=2<202274223:20313730373233393431312>393530343838337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3335373732373330383833332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>323133363137337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343732393137363833343437372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>3231363538337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>313736353038313538303935362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>343338373339387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>3435343534373630353832382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>343339393536347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239342>35373832333031353933323230362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>363632373936337=z[7;227461736;223:2022747261696>222<202272617465223:203239352>343732353331303030303738332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>3636333331347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3634327=7=2<202274223:20313730373233393431322>343638393138367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203437322>3730347=7=2<202274223:20313730373233393431322>343734383434377=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353137363536383731363330362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>383837343634337=z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343338323137313131373837372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431322>383837343938397=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34383833303737303938333435342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>313130383439397=z[7;227461736;223:2022747261696>222<202272617465223:203239352>343532333339323639373336342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>313130383831367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2038312<2022706?776572223:203530382>3036387=7=2<202274223:20313730373233393431322>3939353433397=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433302>3932397=7=2<202274223:20313730373233393431322>393839363133357=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343430303336323837323333362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>333334333934357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239322>38303332393235373431373438362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>3333363430327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>343430363637313834343838322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>353537373737347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433312>3734387=7=2<202274223:20313730373233393431332>353036373135337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>38363634393035353137353232342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>3536303939387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203439352>3530337=7=2<202274223:20313730373233393431332>353134323734387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333439383932303631393734332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>3738323330327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353233333338303237393833362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431332>373834343133387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34333738323831363830363137342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>303035363934347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35323432383439303836313234362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>3030373735327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343437363830383233323139362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>323239303738337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433312>3734387=7=2<202274223:20313730373233393431342>3032313537327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>323234303432363837303133382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>3233323833337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038312<2022706?776572223:203530392>3432337=7=2<202274223:20313730373233393431342>3033303032317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34323333313834393137333837372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>343532353537387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239332>34393833363630393536363432372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>3435373738367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>34343732393137363833343437372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>3637353934337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433312>3436317=7=2<202274223:20313730373233393431342>3533363738397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>35323038313330343236333735342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431342>363831313136337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038312<2022706?776572223:203530352>3432317=7=2<202274223:20313730373233393431342>3534363938327=z[0:z
real	0m22.442s
user	0m23.874s
sys	0m21.704s
---
bf16
====
[7;227461736;223:2022747261696>222<202272617465223:203136362>333034363435323134363235342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3034333336387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;3639342>303632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2035362<2022706?776572223:2039362>3830347=7=2<202274223:20313730373233393431362>363638363039347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203137372>33333734313237323132323539322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>303633393136347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;3639332>353632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2034332<2022706?776572223:2038302>3738377=7=2<202274223:20313730373233393431362>363730373338377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203238302>373932383734383738333730392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3132333534367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203236382>353430343834333237343137342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>313436323432367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383338363938343035343730342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>313937373232377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>372<202274656=7065726174757265223:2036362<2022706?776572223:203332372>3831357=7=2<202274223:20313730373233393431372>3138393932367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239302>393133313430373731373636342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3232313932387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>38382<202274656=7065726174757265223:2035302<2022706?776572223:203239312>3233397=7=2<202274223:20313730373233393431372>313934373130337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37383731313937393934323930342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3237323031357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383831363934323733393132352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>323936313733387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>303436393430353238333439332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>333436333837317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383736393136333430313137372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3337303333337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383836343732333631353031382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3432303535337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3039323535383837393230312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>34343436397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393135313434313137313338312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>343934373032367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383532303733353633323634382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>353138383533327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3931383936373433363331372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>353638383639387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373932383439383730343734392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>353934303730377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39343338323134313132343830352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>3634333031317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363932373139363539322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>363638323333347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393431393039343139333030352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>37313731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037322<2022706?776572223:203435302>3936317=7=2<202274223:20313730373233393431372>373039363238367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373032323734393131363535362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>373432333934377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2035372<2022706?776572223:203430302>3336327=7=2<202274223:20313730373233393431372>373135333831317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39303934303933323239383432332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>373931343037367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353031363237353236343135352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>383136363435347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393339303431343737353435342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>383635353533317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363036373235313534393832372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>383930383037367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383736393136333430313137372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>393339373136367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37383830373437393539303439352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431372>393634393833357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393132323736363932333639362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>303133383731377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38383734323739393734373536342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>3033393133367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383638333136343436383235352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>303838303333327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38323831393031393834323830352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>313133333032377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373139333330373535303331352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>313632323336357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373232313934343534323439332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>313837353032397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373437303135343936343630312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>323336353433347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373033313034313730313731322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>323631383139367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035382<2022706?776572223:203430332>337=7=2<202274223:20313730373233393431382>323436393631387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383936393834363935353931352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>333130363937337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037342<2022706?776572223:203435342>3138377=7=2<202274223:20313730373233393431382>323434363631337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3833323936363536333934332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>333336303934317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393339303431343737353435342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>333834393531387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363335333839343336313537372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>343130323533337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3933393939373435313937352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>343539303933337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353131313831353438373832342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>343834343138327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393337313239353437313532312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>353333323433377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343533383538333337303432362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>353538353930327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>343035363133343738343634362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>363038363536347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353937313730353137363031332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>363332373531377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38313039393635353532353131372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>363832383635397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353131313831353438373832342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>373036393133357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393133323332343934343731392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>373537303136327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37383731313937393934323930342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>373831303939337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035392<2022706?776572223:203430342>3634367=7=2<202274223:20313730373233393431382>373730333539357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383830373338363734383530322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>383331313831387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037352<2022706?776572223:203435352>3234387=7=2<202274223:20313730373233393431382>373637313137337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383132393036383631363834312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>383535333539337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363335333839343336313537372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>3930353433387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383332303131323738353432372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>393239353237337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363733363039333338383138362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431382>393739363037337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38333737343330383331373637352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>303033363930377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353937313730353137363031332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>303533373635337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373330393431303338303838352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3037373834347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383236323739363935323631372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3132373934357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373738373139313435373334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>313531393938387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3732373932323031383531372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>323032323335327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37373437303534303434393831352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>323236313833347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39313033363531303636323537362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>323736333836377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373034303538363236303334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>333030343538377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203430322>3532387=7=2<202274223:20313730373233393431392>323933303535337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39313830313135393732393132342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>333530353438357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037352<2022706?776572223:203435352>3030397=7=2<202274223:20313730373233393431392>323930323730387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383533393834333938343837382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>333734373230367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39303934303933323239383432332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>343234383033377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353937313730353137363031332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>343438383833337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393434373737343136343535322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>343938393438387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363335333839343336313537372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>353233303339337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383836343732333631353031382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3537333132317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38323533323434353239303130342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3539383332317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383438323531393636363137362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>363437323935377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383534393339383235333234342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>363732343838327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39323337343637323337353832362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>373231343431357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383339363533373333393133312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>373436363536327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373830343334393936313739382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>373935363331327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373838323734393531383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>383230383130337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37353137383932343938393036332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3836393933387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203435382>34387=7=2<202274223:20313730373233393431392>38333936387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37313136393434393430313430372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3839353030397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203430342>3634367=7=2<202274223:20313730373233393431392>383433353436347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373838323734393531383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>3934343230377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3639383333313938323936332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393431392>393639343137337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383935303733333036373438342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>303138333639377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383432353139373536313336372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3034333538367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393330343337393834363632342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>303932353233337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383833363035343930343932342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3131373733397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393136303939393337373031382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>313636363730337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373539363037373138313333372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3139313839347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383439323037333536353534372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>323430383339327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373939353335323333303137352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>323636303836387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3930303830373534373131312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>333134393837327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383636343035343237303839322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>333430323434337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39303734393737373431363136342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>333839313339327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203435392>38337=7=2<202274223:20313730373233393432302>333537393432387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383338363938343035343730342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>343134343038347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203430362>3531387=7=2<202274223:20313730373233393432302>3336333438327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383339363533373333393133312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>343633333838327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37373437303534303434393831352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3438383637387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373232313934343534323439332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>353337363636367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37333834323331323733313532332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>353632393439347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383535383935323538333131312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>363132393830317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383731313833303232353632322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3633373131317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383734303439363533363539312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>363837313433337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383233343133393836363234352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>373131323739347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383832363439383739313236362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3736313330397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353937313730353137363031332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>373835343338387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393038343533353435343936322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>383335343538337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343036303930363835303530352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>383539363033367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393336313733353931313838342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>393039353935357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037362<2022706?776572223:203436302>3036397=7=2<202274223:20313730373233393432302>383736393638317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363733363039333338383138362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3933333736327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430362>3831317=7=2<202274223:20313730373233393432302>383836313834377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383439323037333536353534372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432302>3938333838347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383133383632303234313231382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>3030383032367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393230383739313332383330382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>303538303333377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383839333339323837383739382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>303832313739387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343135363434303932343632352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>313332323035327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383438323531393636363137362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>313536333434327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383339363533373333393133312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>323036333832337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383734303439363533363539312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>323330343939377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3732373932323031383531372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>323830353832377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383832363439383739313236362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>333034363535387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37353237343430313930303736352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>333534383932377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373838323734393531383037362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>333738383131317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383634343934343331393536322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>343239303634357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203435392>38337=7=2<202274223:20313730373233393432312>3339303337347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383935303733333036373438342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>343532393632347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430362>3237397=7=2<202274223:20313730373233393432312>3430313234337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353330323839373738303133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>353033333336327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383533393834333938343837382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>353237323032387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363036373235313534393832372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>353737353233357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373938353830313632373836322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>3630323535387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383638333136343436383235352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>363531363839357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343633343132303531393234362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>363736373237387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393033363734373530333536362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>3732353833377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343533383538333337303432362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>373530383931347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383734303439363533363539312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>373939393936397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373139333330373535303331352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>383235303837337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373738373139313435373334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>383734313632377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373433313936363034323937352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>3839393337397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393134313838333032373238312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>3934383331347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203436302>3036397=7=2<202274223:20313730373233393432312>393132353131337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353937313730353137363031332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432312>393733353433367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430342>3933337=7=2<202274223:20313730373233393432312>393230323939387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373735363630333231303831392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>303232353833377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383434343330343638333635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>303437383132377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38313836333739323835323439342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>303936383438377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383636343035343237303839322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>313231393731347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393133323332343934343731392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3137313031317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383538373631353934313732342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3139363132397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3839313235303630323839332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>323435313732337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373231333835363031303939332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>323730323937337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3930303830373534373131312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>333139333239357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3835373830363134323733352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>333434343536327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38393630323839393830393336342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3339333438317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383536383530363937343437392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3431383632387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383939383531383235303031382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>343637363337357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203435392>3534327=7=2<202274223:20313730373233393432322>343237373338347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353031363237353236343135352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3439323738397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430372>3035317=7=2<202274223:20313730373233393432322>343337303730387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363932373139363539322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>353431383739327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343036303930363835303530352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>353637303332387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38393838393631303930343535352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>363137323432387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363932373139363539322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>363431313931357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38323135303335343439343036342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>363931343236337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3833323936363536333934332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>373135333630347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383933313631393432353135372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>373635353837387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353330323839373738303133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>373839353232347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393636373637323335333135372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>383339373335337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383830373338363734383530322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>383633363737337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>39303237313930303937383832362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>393133383834367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383338363938343035343730342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>3933373834337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3839313235303630323839332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432322>393838303533337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203436302>3839317=7=2<202274223:20313730373233393432322>3934363430367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343135363434303932343632352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>303132303034397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430362>3237397=7=2<202274223:20313730373233393432322>393537303836337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>3932313833343939303331392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3036323238397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38353330323839373738303133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>303836323437347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373738373139313435373334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>313336343438397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>373939353335323333303137352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3136303432377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38373937383330383139333936352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>323130363135347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>37333834323331323733313532332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>32333437317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383932323036323639363238312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>323834373635327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363036373235313534393832372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>333038383731337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393033363734373530333536362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>33353839327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383836343732333631353031382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3338333032347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>383932323036323639363238312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3433333037327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38313139353137303533393339342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>343537313935357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>393337313239353437313532312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>353037323135337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203436312>3431387=7=2<202274223:20313730373233393432332>3436303438337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38333737343330383331373637352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3533313336317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313631312>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203430372>3537377=7=2<202274223:20313730373233393432332>343732313138347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239352>353730363839343933363034352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>35383137397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38363036373235313534393832372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>363036373532327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>36383430313633343233363730342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>363536313130387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203239362>38343334373531303931373634342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432332>3638303931387=z[0:z
real	0m8.961s
user	0m8.979s
sys	0m8.885s
---
tf32
====
[7;227461736;223:2022747261696>222<202272617465223:203131382>37393633343536363639393639342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432352>393037333937337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;3639342>303632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2035352<2022706?776572223:203130392>3139367=7=2<202274223:20313730373233393432352>353031383438357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203132302>313637303236333030313134362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432352>393138353838347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;3639332>353632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2034332<2022706?776572223:2039312>3930337=7=2<202274223:20313730373233393432352>353139363238337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134352>34383638353633343333313737332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>303630333033377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39352<202274656=7065726174757265223:2036352<2022706?776572223:203334392>3932397=7=2<202274223:20313730373233393432362>303436303837337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203133342>35303033303735303031363830382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>303832353231377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035302<2022706?776572223:203330362>3237377=7=2<202274223:20313730373233393432362>303539323739347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>32313930303136383530303935372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>323038383532357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>34343233313630383938373133382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>3233313833387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363938383239343935303430352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>333536383239347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363736353337373933393536342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>333739383330387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303239303530343437373633342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>353034373936377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>35303332323633373930363630362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>353237393934367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393235393637333334323132382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>3635323737367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037322<2022706?776572223:203436382>31377=7=2<202274223:20313730373233393432362>353630303139357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353432353237373534343631342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>363736393835377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035372<2022706?776572223:203431362>32337=7=2<202274223:20313730373233393432362>353732383937347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393136333738393333393334382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>3830303833367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353136313733353438353231362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>3832353038397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36343635383633353638323732372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>393438383537337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353636343836393334343034372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432362>393733313031367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363039363135343034363032352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>303936383639327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037342<2022706?776572223:203437322>3730347=7=2<202274223:20313730373233393432372>3037323239387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36313536383931323431363131322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>31323131367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035392<2022706?776572223:203432332>3433367=7=2<202274223:20313730373233393432372>303835383431377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363634333136313530303437332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>323435303638387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36323039353735313131353039382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>323639333939367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>35353531323935303930353738352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>3339333138327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353437333139353238363635342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>343137343135397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373836393437363332343532332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>353431313738357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353636343836393334343034372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>353635343235367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363532373436333737333731332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>363930313939317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037352<2022706?776572223:203436352>39377=7=2<202274223:20313730373233393432372>353935343930327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36343635383633353638323732372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>373133343535377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203432342>3830377=7=2<202274223:20313730373233393432372>3630333632367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363831353031373439363431352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>383338333035327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353136313733353438353231362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>383631353432327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373033313434373839393533342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432372>393836323736317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>343836303133303736333936392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>3030393732337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393231313733313138363136342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>313334323536347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037352<2022706?776572223:203437392>3130317=7=2<202274223:20313730373233393432382>3131323630357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36343434333033313930363231372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>313537373437337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432352>3839357=7=2<202274223:20313730373233393432382>313233363739367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363836323934343139383336332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>323832343138337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36323639343437363739363835322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>33303539347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393930363932323730373631372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>343330333930347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363136383033373236323638352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>343533393435347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393238333634343533363034362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>353738333637327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363139313939383438393338372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>363031393435397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353430313331383738393433382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>373237343739377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203437392>3932317=7=2<202274223:20313730373233393432382>3636333431387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>323931343435393231363233362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>373530333136397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203432332>32367=7=2<202274223:20313730373233393432382>363737343630347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363736373039313130333437322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>383735363736347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373331383236333237393032332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432382>383938353134337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303737303030393532303232362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>303233363430367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303737303030393532303232362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>3034363437367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303931333836373036333235372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>313731363030367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303236363533303033373233332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>313934343339367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433322>3630317=7=2<202274223:20313730373233393432392>3139313138347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303838393839303631323739362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>333139353538397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037362<2022706?776572223:203437382>3836327=7=2<202274223:20313730373233393432392>313738303338387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303732323035373632343338362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>333432343639327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363830313332373737353431332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>343637363332357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393532333336303732363230352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>3439303434367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>34373734303739323133393635322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>363135393031377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>35393635333435303135303139342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>363338353930367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303030323831363239343837372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>3736343838387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438302>3730337=7=2<202274223:20313730373233393432392>363930313830357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303630323137393233373734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>3738363535347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433332>3839347=7=2<202274223:20313730373233393432392>373033313036367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393634333232313731393735382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>3931323935357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373039383537393638373835322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393432392>393334353931387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303336323432383236323636362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>303630393335337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363937313531333932343334332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>303832353633327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303137303633333034383638312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>323038393032367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313137373631333132313135342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>323330353136327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433362>3035327=7=2<202274223:20313730373233393433302>323237353538317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303630323137393233373734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>333536383633337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438332>3133397=7=2<202274223:20313730373233393433302>323139313533327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303537383230333739323334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>333738353437327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393039313837373134383738342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>353034393230377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303739333938353538343131322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>353236353037367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363437393533393233343733322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>3635323932387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363537353338383632313637382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>363734353036327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373137343437353239333230362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>383032313730337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203437352>3131317=7=2<202274223:20313730373233393433302>373531363630387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363836323934343139383336332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>383232353939367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203433312>3439387=7=2<202274223:20313730373233393433302>373537313138327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393231313733313138363136342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>39353032357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393432373437333332323634352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433302>393730363633387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>353334353535323635343839342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>303938333831337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303032363738393838343934362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>313138363432337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373033313434373839393533342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>323436333434367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373033383634303330313232392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>323636363030387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303537383230333739323334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>333934333033387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438332>3130357=7=2<202274223:20313730373233393433312>323730343139317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313336393433343331323230392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>343134353439347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433332>3635367=7=2<202274223:20313730373233393433312>323736353439387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373032313835383133383833352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>353432333530337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393537313330343839313734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>353632353839347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393235393637333334323132382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>363931333539337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373138393639363831303434372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>373130353335357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303836353931343233393635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>383339333233337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037372<2022706?776572223:203438322>3033327=7=2<202274223:20313730373233393433312>373839393534377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303737303030393532303232362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>383538343932347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433352>3234327=7=2<202274223:20313730373233393433312>373935363331367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>3638383238323035303637372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433312>393837333835357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36383538383531313239303437342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>303036353436337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363338333639313038333731352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>313335343730367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363833393637363138333237382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>313534363032337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363732323234303433313237382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>323833343732357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393330373631353830373235332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>333032353932357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>343930333136303237393438312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3433313634357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037382<2022706?776572223:203438312>3736317=7=2<202274223:20313730373233393433322>333131363031397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313033333735303437353237322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3435303534367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433342>3731317=7=2<202274223:20313730373233393433322>333235373436387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36363933343833343833303637362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3537393732377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303732323035373632343338362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>353938353835387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303431303337373833393231382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3732383835397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303936313832303139363131352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3734363534317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313334353435363339323730362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>383736383232327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037382<2022706?776572223:203439392>3134387=7=2<202274223:20313730373233393433322>38323531337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393833353030333332383831342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433322>3839343530377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433362>3334357=7=2<202274223:20313730373233393433322>383337373531327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303931333836373036333235372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>303234383634347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313033333735303437353237322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>303432353333327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313939323838373336303438392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>313732383133347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303639383038313739323433352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>313930343930357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363839393539393533313030342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>333230373839387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303737303030393532303232362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>333338343433337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313438393332353036393537352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>3436383734327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203439392>3936377=7=2<202274223:20313730373233393433332>333732373137397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303239303530343437373633342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>343836343236367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433332>3635367=7=2<202274223:20313730373233393433332>333735313739387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>35363735373235313436353434322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>3631363932327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313038313730343338313239372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>363334343533337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>353031373335373339383538322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>3736373230347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36303733303833363931353231352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>37383235317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313737373037303737343338322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>3931353135387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037392<2022706?776572223:203530302>3439337=7=2<202274223:20313730373233393433332>383935353533347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303132323638353031383232382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433332>393330343731327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203433362>3538337=7=2<202274223:20313730373233393433332>383938343138377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313436353334363736333435382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>303633323030377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393537313330343839313734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>303738353130357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303836353931343233393635362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>3231313136327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303630323137393233373734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>323236343638337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373135313333303334353330322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>333539313131387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313137373631333132313135342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>333734343231387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373138393639363831303434372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>353037303632327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203439382>3836327=7=2<202274223:20313730373233393433342>343138343832387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313232353536373935343939372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>353232333733377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433362>3334357=7=2<202274223:20313730373233393433342>3432313335347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>393534383437353439323037312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>3635353837327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373139383433393736343336362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>363730343339377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>383339353930373636333635342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>383036313430347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373630353834373539343639322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>383138353237357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393534373333323737303332372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>393534313333337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2037392<2022706?776572223:203530312>3236347=7=2<202274223:20313730373233393433342>393338383734377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363937383730353734363237372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433342>3936363439337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433342>3939377=7=2<202274223:20313730373233393433342>393433363131397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313536313236303435313837382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>3130323139337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303537383230333739323334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>313134353431357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313837323938383438333739332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>3235303134357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303635303133303336303436352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>323632343938397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313732393131323338333634372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>333938303938377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313137373631333132313135342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>343130343438387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37323230383731303231303436372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>3534363035337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203439392>3936377=7=2<202274223:20313730373233393433352>343535373536347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303739333938353538343131322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>3535383430387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433342>3731317=7=2<202274223:20313730373233393433352>3436313635387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>36353532323230323338373833342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>363936313834367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303137303633333034383638312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>37303634367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>303132353433333538313535332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>383434383434367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303737303030393532303232362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>3835343431367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36353638383832383934383736382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433352>393932383538327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313137373631333132313135342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>303032333636387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313732393131323338333634372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>313430383035377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203530302>3230367=7=2<202274223:20313730373233393433362>3030333736387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373036323631353437363034352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>313530333232347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433362>3538337=7=2<202274223:20313730373233393433362>303130383336347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373132373335323330393831322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>3238383833387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303537383230333739323334362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>323938333530337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313234393534353438373839362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>3433363739337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373033383634303330313232392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>343436333130337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313934343932373537373831352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>353834373434377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203530332>3133347=7=2<202274223:20313730373233393433362>353232333133387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373136303932313737363030342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>353934323535377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433362>3334357=7=2<202274223:20313730373233393433362>353238303034327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>39323531383631353136373439352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>373334373033337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313336393433343331323230392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>3734323239347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>34323632373536333438383231362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>383832393438397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313232353536373935343939372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433362>383930323435347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373136303932313737363030342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>303330383939357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313538353233393036373239382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>303338313934377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303635303133303336303436352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>313738383635327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203530332>3133347=7=2<202274223:20313730373233393433372>303339343934357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313033333735303437353237322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>313836313436337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433322>3331357=7=2<202274223:20313730373233393433372>303436383732367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36383230353031373332303732372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>333237303235347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36393537313330343839313734342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>333334313838357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313332313437383535303532322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>343734393830367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>35343635313633343134363732382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>343832333833337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>33323831373136373730313937322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>363233333231337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203530322>3839357=7=2<202274223:20313730373233393433372>353536343531387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313038313730343338313239372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>363330333533327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433352>3234327=7=2<202274223:20313730373233393433372>353634393730357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>30303839383035383534313330372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>373733333135377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303137303633333034383638312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>373738343031397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>323433373737323732393036332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>393231373437327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303838393839303631323739362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433372>393236333834327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373132373335323330393831322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>3036393730377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373033313434373839393533342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>303734333438327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313830313035303038353734322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>3231373635387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2038302<2022706?776572223:203530302>3733317=7=2<202274223:20313730373233393433382>303732323939357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>373139323039343738303234382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>323232323933317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433372>3130387=7=2<202274223:20313730373233393433382>303831323535377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313732393131323338333634372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>333635363835327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313130353638313435303238332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>333730333136377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37323038383830373835333830382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>353133363336347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313533373238313931333738382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>353138323635357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134372>32333735343432343835373732352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>363633303732367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2037392<2022706?776572223:203530322>3630387=7=2<202274223:20313730373233393433382>363235343436337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313130353638313435303238332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>363636323136347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203433382>3231317=7=2<202274223:20313730373233393433382>363432363737357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303838393839303631323739362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>383132323034317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>36373535373931363130323739372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>383134323735337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37323539323431303734343330332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>393630313530357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37303834313933373934333832382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433382>393632323239377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>37313737373037303737343338322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433392>313038313033387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:203134382>363130343231313130363730372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393433392>313130323937377=z[0:z
real	0m15.096s
user	0m16.675s
sys	0m14.620s
---
fp32
====
[7;227461736;223:2022747261696>222<202272617465223:2031382>3934353334333834333932353230382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434312>393537393638327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;3639342>303632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2035362<2022706?776572223:203131302>3235337=7=2<202274223:20313730373233393434302>363039313737347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203330342>3832337=7=2<202274223:20313730373233393434312>313234393236367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203331322>3238347=7=2<202274223:20313730373233393434312>3634393535387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031382>3935383939333332323439313237382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434312>393730353431327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;3639332>353632352<2038313932302>305=2<20226<6?6164223:20302<202274656=7065726174757265223:2034352<2022706?776572223:2039332>3238367=7=2<202274223:20313730373233393434302>363230393230327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393434312>313335313632367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393434312>363630313537347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373636323331373632313034322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434332>3130363734367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036372<2022706?776572223:203331332>3837357=7=2<202274223:20313730373233393434322>313631373534387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203332342>3335317=7=2<202274223:20313730373233393434322>363734363135397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363434333239323232313335332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434332>313138353639347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203239322>3438357=7=2<202274223:20313730373233393434322>313736313635337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393434322>363838393639317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373731303830363939373632312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434342>3235353036367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203331322>3238347=7=2<202274223:20313730373233393434332>3231373030357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203332362>3038317=7=2<202274223:20313730373233393434332>373339313635357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136353031303333303839303637342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434342>323636333838327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393434332>323230343835347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238342>3733367=7=2<202274223:20313730373233393434332>373433393834357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373739303935343633323730392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434352>343033333739327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203331312>3538347=7=2<202274223:20313730373233393434342>3236353139357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036372<2022706?776572223:203331322>3532347=7=2<202274223:20313730373233393434342>373935393938387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203332352>3730317=7=2<202274223:20313730373233393434352>333138373231387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136353838363436353637343539332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434352>34313432317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203239342>3336367=7=2<202274223:20313730373233393434342>323639383639337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203239342>3635347=7=2<202274223:20313730373233393434342>373938353530367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238312>3739347=7=2<202274223:20313730373233393434352>333231353435347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373438363836383835373838372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434362>353531363335377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203331312>3538347=7=2<202274223:20313730373233393434352>383431333138317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203332332>3034387=7=2<202274223:20313730373233393434362>333635333631327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343339373038343230313034352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434362>353632303639327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203239342>3934317=7=2<202274223:20313730373233393434352>383434313233317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238312>3739347=7=2<202274223:20313730373233393434362>3336383135337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373831383436393534343631312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434372>3639393936337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203332352>3436317=7=2<202274223:20313730373233393434362>383930303730377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036362<2022706?776572223:203331302>3339397=7=2<202274223:20313730373233393434372>3433313030347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136353637353338393531353833372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434372>3731363830397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39362<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393434362>383936313732387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3331317=7=2<202274223:20313730373233393434372>343333333531337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137393534343630363330303330382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434382>383438303230337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036362<2022706?776572223:203332332>3935337=7=2<202274223:20313730373233393434372>3935343130357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203331302>3339397=7=2<202274223:20313730373233393434382>343735353838387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363530393739343037363434362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434382>3836343537377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393434372>393536393130387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035352<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393434382>353034363639377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383935343335373337343532322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393434392>393936303933337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036352<2022706?776572223:203332322>3437327=7=2<202274223:20313730373233393434382>393930333139337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203332312>3933377=7=2<202274223:20313730373233393434392>353032363037337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343436303739363331353930342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435302>303132333132327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393434392>303138323037367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393434392>353333303332327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383733313032393431363839352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435312>3134343238337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203331332>3130377=7=2<202274223:20313730373233393435302>303431393836377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203332312>3933377=7=2<202274223:20313730373233393435302>35353731367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203331322>3831327=7=2<202274223:20313730373233393435312>3036393331387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363439393034313936363734332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435312>3136303131387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238332>3134327=7=2<202274223:20313730373233393435302>303539303036327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238312>3739347=7=2<202274223:20313730373233393435302>353730383636367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239352>31387=7=2<202274223:20313730373233393435312>3038353136367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373837373838383439363939372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435322>323932353034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203332302>3334377=7=2<202274223:20313730373233393435312>353835333032387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203331342>3533367=7=2<202274223:20313730373233393435322>303937383633377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333735323032323839353739322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435322>333037393331347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238312>3530377=7=2<202274223:20313730373233393435312>353937303832367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393435322>313130393839337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373833373736393539343437362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435332>3434313137387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036352<2022706?776572223:203331322>3532347=7=2<202274223:20313730373233393435322>3631323237367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331372>3933337=7=2<202274223:20313730373233393435332>3132343331377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323739363435363333343039362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435332>3435353734317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238302>31367=7=2<202274223:20313730373233393435322>363235343331337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203237392>3633337=7=2<202274223:20313730373233393435332>313337383837357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383431353938363332303436362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435342>353839323334387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036342<2022706?776572223:203331332>3130377=7=2<202274223:20313730373233393435332>363339343033367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331372>3131317=7=2<202274223:20313730373233393435342>313533363033337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363137343134363232383439362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435342>3630333639397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239322>3438357=7=2<202274223:20313730373233393435332>363531323234397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238302>3434377=7=2<202274223:20313730373233393435342>313937303830367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373838313633333038393435322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435352>373337343230337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203330392>3837317=7=2<202274223:20313730373233393435342>363636303638337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331342>3639377=7=2<202274223:20313730373233393435352>313834323038327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331362>3538337=7=2<202274223:20313730373233393435352>363936353232357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343338393132303231363437332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435352>373532343235377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393435342>373039323034347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393435352>323233343334327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393435352>373337343736337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>313738333736313038313835352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435362>383835343238377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331332>3633357=7=2<202274223:20313730373233393435362>323131313035337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331362>3832337=7=2<202274223:20313730373233393435362>3732333137347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363331373037303835353131362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435362>393030323932397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203239332>3539337=7=2<202274223:20313730373233393435362>3235323430397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3530377=7=2<202274223:20313730373233393435362>373732333937337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373532323735353834333733362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435382>303333353330357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036342<2022706?776572223:203331352>3232357=7=2<202274223:20313730373233393435372>323337333937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331312>3735367=7=2<202274223:20313730373233393435372>373439343935337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363032343437363433393238372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435382>303438323637387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3336357=7=2<202274223:20313730373233393435372>3238343730317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035342<2022706?776572223:203238322>3835357=7=2<202274223:20313730373233393435372>373939393731337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373739383533313132333838342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435392>3138313732377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203331372>3633387=7=2<202274223:20313730373233393435382>323836323437357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331302>3634357=7=2<202274223:20313730373233393435382>383030333438387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363330393130373931393536312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393435392>3139363139397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3336357=7=2<202274223:20313730373233393435382>333132303332377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3338387=7=2<202274223:20313730373233393435382>383236343538327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137393231373537323030313837352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436302>333239373238347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331352>3539337=7=2<202274223:20313730373233393435392>333133333638337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330392>3034387=7=2<202274223:20313730373233393435392>383236343231377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333431333538323137323933362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436302>333433393632327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393435392>333633343631377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3038337=7=2<202274223:20313730373233393435392>383736313935377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383637393138363137303536362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436312>343737383134347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331392>3831327=7=2<202274223:20313730373233393436302>333339353230327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330382>3035367=7=2<202274223:20313730373233393436302>383532323231357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330372>32347=7=2<202274223:20313730373233393436312>333634393933337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3135383831393933343230343131382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436312>3439323032377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3134337=7=2<202274223:20313730373233393436302>333839363234367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393436302>393038343837387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>34337=7=2<202274223:20313730373233393436312>3432313930377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373537303630353336373132362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436322>3632363133397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203332302>3138357=7=2<202274223:20313730373233393436312>383737363438387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330362>3130377=7=2<202274223:20313730373233393436322>3431313535357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363032363036383534383034332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436322>363430303339327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3236327=7=2<202274223:20313730373233393436312>393334333933327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393436322>343436373934357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373635303630303939313639342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436332>373734343134387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203331372>3139327=7=2<202274223:20313730373233393436322>3932363535357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330362>3939327=7=2<202274223:20313730373233393436332>343338383335317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323636393035343635393234342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436332>373837383730327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3338387=7=2<202274223:20313730373233393436322>393539333530387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3134327=7=2<202274223:20313730373233393436332>3439343430347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353630343938343033313539372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436342>3932323736397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331382>3436317=7=2<202274223:20313730373233393436332>3935323733377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3832337=7=2<202274223:20313730373233393436342>343635313934357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343232393834313931353033382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436342>393335363737387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393436342>303036333434367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3835357=7=2<202274223:20313730373233393436342>353138343839367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373838313633333038393435322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436362>303730393435357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203330352>3238347=7=2<202274223:20313730373233393436342>393930383137357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203332302>3334377=7=2<202274223:20313730373233393436352>353034323535387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3939367=7=2<202274223:20313730373233393436362>303136347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363139393232363136363533382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436362>303833353234357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393436352>303330363234397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393436352>353434303135367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393436362>303536343634327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373832343834393932363830332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436372>3231393038317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331382>3137337=7=2<202274223:20313730373233393436362>353239343234377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330352>3932387=7=2<202274223:20313730373233393436372>303431383131377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323131313639323234393237362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436372>323331343233397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>3831327=7=2<202274223:20313730373233393436362>353639353433347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393436372>3038313531357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373736323634333031313238372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436382>333637323331387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203332302>3035327=7=2<202274223:20313730373233393436372>353534373831377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3436387=7=2<202274223:20313730373233393436382>303936323634317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333339333637343236373339332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436382>333739313833337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393436372>353935313636327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238302>39387=7=2<202274223:20313730373233393436382>313235393232347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373736323030353834383438362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436392>3531353337337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330352>3238347=7=2<202274223:20313730373233393436382>3630383335327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3933337=7=2<202274223:20313730373233393436392>313331323337377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343338313135363233383532332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393436392>353236393039347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239332>3330367=7=2<202274223:20313730373233393436382>363339373530377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3039397=7=2<202274223:20313730373233393436392>313534313334387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373237313534393736333330342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437302>3636333530317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3232317=7=2<202274223:20313730373233393436392>363435323037347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331362>3238387=7=2<202274223:20313730373233393437302>313636303432387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343032323738343038313235382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437302>363734363335347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393436392>3636363132387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>39327=7=2<202274223:20313730373233393437302>313738333339357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373439343834333733323032342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437312>383131373033327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330362>3939327=7=2<202274223:20313730373233393437302>363738323233367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3438317=7=2<202274223:20313730373233393437312>3230303539387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331362>3238387=7=2<202274223:20313730373233393437312>373134393037367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343238313630373037323630332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437312>383233333332387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393437302>363931353232387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3633337=7=2<202274223:20313730373233393437312>323132363739397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239332>3539337=7=2<202274223:20313730373233393437312>373236333039387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363434363230343639303433322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437322>3936303033337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330352>3932387=7=2<202274223:20313730373233393437322>323237303437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3131317=7=2<202274223:20313730373233393437322>373831333636337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343336393231303238343030342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437322>393731313130367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393437322>323431363434367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393437322>373834393135327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373737343630353536363234352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437342>313038313030327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330362>3130377=7=2<202274223:20313730373233393437332>3239393631377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330392>3430397=7=2<202274223:20313730373233393437332>383236343938357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363332343633353730353139312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437342>313138383837377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393437332>333034383630367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238302>3638367=7=2<202274223:20313730373233393437332>383335393836317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373731353539313734333032372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437352>323536343432357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203331352>35327=7=2<202274223:20313730373233393437342>3333393134347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>3539327=7=2<202274223:20313730373233393437342>383630333234347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136303737383133353233303135362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437352>323636393639347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3931357=7=2<202274223:20313730373233393437342>333531373830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3234367=7=2<202274223:20313730373233393437342>383639313030387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373731343339353535343433372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437362>343034363033327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331342>3030317=7=2<202274223:20313730373233393437352>3432303932357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331312>3735367=7=2<202274223:20313730373233393437352>393431393435387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333434393431363530373135322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437362>343134373236357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3332387=7=2<202274223:20313730373233393437352>343138333136387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393437352>393339313233327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383438333737393533303531372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437372>353533303033357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203330352>3131327=7=2<202274223:20313730373233393437362>3436333637377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331322>33367=7=2<202274223:20313730373233393437372>303130393530337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3239367=7=2<202274223:20313730373233393437372>353333363633337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333638343333333739343838372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437372>353632343737367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>37337=7=2<202274223:20313730373233393437362>343630383330327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3637357=7=2<202274223:20313730373233393437372>303036353133367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3439317=7=2<202274223:20313730373233393437372>353237383439377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373537303630353336373132362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437382>373031323931387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331332>3137377=7=2<202274223:20313730373233393437382>303530363031377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331302>3634357=7=2<202274223:20313730373233393437382>353639383135397=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333139343539373438363837352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437382>3731303331397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3930347=7=2<202274223:20313730373233393437382>303433303631337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>36317=7=2<202274223:20313730373233393437382>353633363934327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373032383332343735303032322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437392>383439343538327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330392>3430397=7=2<202274223:20313730373233393437392>303838313130347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3436317=7=2<202274223:20313730373233393437392>363334323637367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343037383532393938303937362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393437392>3835383035357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3134337=7=2<202274223:20313730373233393437392>303832353233367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3935377=7=2<202274223:20313730373233393437392>363234323031337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383439393733303934333730362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438302>393937343435337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203330382>3539327=7=2<202274223:20313730373233393438302>313438393032377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3939367=7=2<202274223:20313730373233393438302>363634313436327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136353831383736313439343536332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438312>303035373132337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393438302>313338333036347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3833387=7=2<202274223:20313730373233393438302>363530363030347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373737333936383530353432342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438322>3134353631347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203331322>3238347=7=2<202274223:20313730373233393438312>3137363135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331322>31327=7=2<202274223:20313730373233393438312>363839393432367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343832333136373032353536372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438322>313533343034357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3733367=7=2<202274223:20313730373233393438312>313633373434327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3230337=7=2<202274223:20313730373233393438312>363736333031327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373439343834333733323032342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438332>323933383337357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330312>3334337=7=2<202274223:20313730373233393438322>323132353135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3232317=7=2<202274223:20313730373233393438322>373338393133357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>38387=7=2<202274223:20313730373233393438332>323536303431387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333937383938333935383939342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438332>333031313432357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3631317=7=2<202274223:20313730373233393438322>313933393130317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238302>31367=7=2<202274223:20313730373233393438322>373333303335387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239312>3138367=7=2<202274223:20313730373233393438332>323530323838357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>313738313430383330353634382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438342>3434313937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>3233327=7=2<202274223:20313730373233393438332>3737353138347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331322>33367=7=2<202274223:20313730373233393438342>333137373531347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323734303731373839323930322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438342>343439303135347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3236387=7=2<202274223:20313730373233393438332>373638323139377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>3831327=7=2<202274223:20313730373233393438342>333134383035337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>313736373933303830313936382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438352>353930333732387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330372>3435377=7=2<202274223:20313730373233393438342>383431363031317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331332>3837357=7=2<202274223:20313730373233393438352>333930363237367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>313632373132383438373933392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438352>353936393233347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3438357=7=2<202274223:20313730373233393438342>383338343738387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>3831327=7=2<202274223:20313730373233393438352>333631383632347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373739393332383635323735332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438362>3733383532317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331322>33367=7=2<202274223:20313730373233393438352>393032363834377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330352>3932387=7=2<202274223:20313730373233393438362>343137333432327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363139323036303430323936332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438362>373434383033347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3438357=7=2<202274223:20313730373233393438352>383832303530357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>3537337=7=2<202274223:20313730373233393438362>3339363234357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373734353839353638353036342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438372>383836353037357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203331352>3232357=7=2<202274223:20313730373233393438362>393531373032347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330332>3430357=7=2<202274223:20313730373233393438372>343633393330387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363139353634333237383035312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438372>383932363534347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3737337=7=2<202274223:20313730373233393438362>393038303736357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239332>3036377=7=2<202274223:20313730373233393438372>343233303730327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137383130303935333537343130322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438392>3033343630397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203239332>3939327=7=2<202274223:20313730373233393438372>393736303034317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330332>3131317=7=2<202274223:20313730373233393438382>343930373530337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330322>3430377=7=2<202274223:20313730373233393438392>3030333730387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313539303138393634323338362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393438392>3034303532317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3039397=7=2<202274223:20313730373233393438372>393335323230327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3535317=7=2<202274223:20313730373233393438382>343439323136347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>39327=7=2<202274223:20313730373233393438382>393631353638387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373338373138333439303830342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439302>313833303139397=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331332>3633357=7=2<202274223:20313730373233393438392>353331333035387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330322>3035357=7=2<202274223:20313730373233393439302>303433363730347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313938343239393637393338322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439302>313838343430387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393438392>343735373332367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3332387=7=2<202274223:20313730373233393438392>393838343331377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373637323932383630353835312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439312>333331313739347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331362>3034387=7=2<202274223:20313730373233393439302>353536393837387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330312>32387=7=2<202274223:20313730373233393439312>303639303738327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363332373432323737303237342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439312>3333363232327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3336357=7=2<202274223:20313730373233393439302>35303435377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3037387=7=2<202274223:20313730373233393439312>3034323032377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373633333435363932363536362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439322>343739333738357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203331342>3639377=7=2<202274223:20313730373233393439312>353831393934387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330322>31367=7=2<202274223:20313730373233393439322>313138303535387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136343234353736393632363034322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439322>343833393730347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3434387=7=2<202274223:20313730373233393439312>353534313832387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3230337=7=2<202274223:20313730373233393439322>303637373234327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373133313939333637333339352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439332>363237363537327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203330322>3136347=7=2<202274223:20313730373233393439322>363331323234367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331332>3936337=7=2<202274223:20313730373233393439332>3135333930367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363238373231303138383030372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439332>3633313737327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3032337=7=2<202274223:20313730373233393439322>353830313830347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3532397=7=2<202274223:20313730373233393439332>303933373835337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3030337=7=2<202274223:20313730373233393439332>363238353839347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373735353036363830333636392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439342>373735373934357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330332>3531317=7=2<202274223:20313730373233393439332>3636363030347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331352>33317=7=2<202274223:20313730373233393439342>313830313935367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330332>3531317=7=2<202274223:20313730373233393439342>3732323538377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363236323532363037323437362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439342>373739363530327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3839377=7=2<202274223:20313730373233393439342>313432313731347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3735387=7=2<202274223:20313730373233393439342>3635343436377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373032303335303236333837372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439352>393233393635377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331322>3930337=7=2<202274223:20313730373233393439352>3233343739327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331342>3738347=7=2<202274223:20313730373233393439352>3734383639347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363230313231363636353933312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439352>393237343937347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3832317=7=2<202274223:20313730373233393439352>3136383033357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3333377=7=2<202274223:20313730373233393439352>363830313439387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373438323838313432333330352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439372>303732313037367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203330322>3938357=7=2<202274223:20313730373233393439362>3236323431387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331322>3930337=7=2<202274223:20313730373233393439362>373736333439357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136303338383035353133333434322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439372>303735343435347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3934377=7=2<202274223:20313730373233393439362>313933353032327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3738337=7=2<202274223:20313730373233393439362>37303635327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373637343932323136383131332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439382>323230333437347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203330332>3232347=7=2<202274223:20313730373233393439372>3331333730337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331332>3637367=7=2<202274223:20313730373233393439372>383238333531357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363135373432363633343532372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439382>323233333330357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3933347=7=2<202274223:20313730373233393439372>323139373931327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3930367=7=2<202274223:20313730373233393439372>373332323833347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373633333435363932363536362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439392>333638353138367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203330302>3537377=7=2<202274223:20313730373233393439382>333430353131337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331302>3439357=7=2<202274223:20313730373233393439382>3835333130377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331312>3032387=7=2<202274223:20313730373233393439392>333636313731367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333139383537383938313934382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393439392>333731313136367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3239367=7=2<202274223:20313730373233393439382>323435363837327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3939347=7=2<202274223:20313730373233393439382>373539323834357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239332>3930367=7=2<202274223:20313730373233393439392>323731363135337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363039313336383031393036322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530302>353136383132337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330362>37347=7=2<202274223:20313730373233393439392>393036393039377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330382>3632317=7=2<202274223:20313730373233393530302>343232343435387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323539373339313936313538332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530302>353139303035387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3334367=7=2<202274223:20313730373233393439392>3738333934317=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3530347=7=2<202274223:20313730373233393530302>323937323834387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373632353838313733393632332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530312>363635303136377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330382>3632317=7=2<202274223:20313730373233393530302>3933343934347=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330392>3637347=7=2<202274223:20313730373233393530312>3434383538377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323335343533393032373732382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530312>363636383335357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239342>3433387=7=2<202274223:20313730373233393530302>383230303932327=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3436337=7=2<202274223:20313730373233393530312>333536323939327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363135353135373931333135382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530322>383133323138367=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330382>3033397=7=2<202274223:20313730373233393530312>393630383032387=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330392>3134377=7=2<202274223:20313730373233393530322>343734353431377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363234353430363831373432332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530322>383134363536377=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239312>3830317=7=2<202274223:20313730373233393530312>383731343032357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3436337=7=2<202274223:20313730373233393530322>333833343933327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353535333135373638343238382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530332>3936313436327=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323331303734363533303830332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530332>393632343930387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203331312>3032387=7=2<202274223:20313730373233393530322>393836383135357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239312>3435397=7=2<202274223:20313730373233393530322>383936333733337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331302>3230387=7=2<202274223:20313730373233393530332>353030343438327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3731377=7=2<202274223:20313730373233393530332>343038363338357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3534387=7=2<202274223:20313730373233393530332>393437333734337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353834303139393433363434372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530352>313039373432367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333437373238373734383633382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530352>313130353437387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330302>3537377=7=2<202274223:20313730373233393530342>3033343637367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3533357=7=2<202274223:20313730373233393530342>343635353137357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331302>3230387=7=2<202274223:20313730373233393530342>353436383234377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3739337=7=2<202274223:20313730373233393530342>3937373631347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330372>3531337=7=2<202274223:20313730373233393530352>3035393132357=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373535373330393038363136342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530362>323538323731327=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313938383238303637313535342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530362>323539303332357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331302>3439357=7=2<202274223:20313730373233393530352>353732363137357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3939377=7=2<202274223:20313730373233393530352>343931363439367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203330372>3236367=7=2<202274223:20313730373233393530362>3038343831317=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3630347=7=2<202274223:20313730373233393530362>303033383937327=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373731353539313734333032372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530372>34303635327=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363235343936313731323735382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530372>343037343030387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>3033397=7=2<202274223:20313730373233393530362>363139363039347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>31367=7=2<202274223:20313730373233393530362>353137323235337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331332>3936337=7=2<202274223:20313730373233393530372>313338363238377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3239367=7=2<202274223:20313730373233393530372>303239333037317=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373733363732343635343137352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530382>353534373932367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363238373231303138383030372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530382>353535363633337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036302<2022706?776572223:203330382>3033397=7=2<202274223:20313730373233393530372>363531363133327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3331387=7=2<202274223:20313730373233393530372>353432373937387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036302<2022706?776572223:203331312>3236387=7=2<202274223:20313730373233393530382>313833353432357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3437387=7=2<202274223:20313730373233393530382>303535383632377=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373733373132333339323832342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530392>373033303031337=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333331343034333035383834332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393530392>373033383439367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203330372>387=7=2<202274223:20313730373233393530382>363936393239327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3731377=7=2<202274223:20313730373233393530382>3536383737357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330342>3537327=7=2<202274223:20313730373233393530392>323132303839357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3838327=7=2<202274223:20313730373233393530392>303830383730367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3239367=7=2<202274223:20313730373233393530392>353935343031337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353438313339383538383936382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531302>383531343032387=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136303935333237383339393236322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531302>383532333338387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203238382>3230337=7=2<202274223:20313730373233393530392>373234323932387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3230357=7=2<202274223:20313730373233393531302>313039333732317=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330352>3130357=7=2<202274223:20313730373233393531302>323430353935337=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3430327=7=2<202274223:20313730373233393531302>3632323635357=z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331352>3731347=7=2<202274223:20313730373233393531302>3735353436387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137343930373334353136303535352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531322>303034323333347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313034343833313738343638362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531322>3030353034327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330392>39327=7=2<202274223:20313730373233393531312>323730353639387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238392>3539317=7=2<202274223:20313730373233393531312>313337353434347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331302>3439357=7=2<202274223:20313730373233393531312>373930373339337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3334367=7=2<202274223:20313730373233393531312>363439353531397=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373636303536383631323334312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531332>313532353333387=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136303532373336373633303431352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531332>3135333530327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3535357=7=2<202274223:20313730373233393531322>333035333535387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3931397=7=2<202274223:20313730373233393531322>313633353033327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330392>3338377=7=2<202274223:20313730373233393531322>383139313034327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3630347=7=2<202274223:20313730373233393531322>363738313033327=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373531303739333530303139372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531342>333030373335377=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323332363637313035313938322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531342>333031363437377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203330392>3338377=7=2<202274223:20313730373233393531332>333331333930397=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3839377=7=2<202274223:20313730373233393531332>3139303239357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330392>3134377=7=2<202274223:20313730373233393531332>383434363237347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3432327=7=2<202274223:20313730373233393531332>373033373639327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3636317=7=2<202274223:20313730373233393531342>323136323230317=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137323635393330303133313131362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531352>3434393235347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>313632363635303733333834322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531352>3434393931387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330362>3937397=7=2<202274223:20313730373233393531342>3337383036337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3933397=7=2<202274223:20313730373233393531342>37333031337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>38367=7=2<202274223:20313730373233393531342>3839323432357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3934377=7=2<202274223:20313730373233393531352>323432343730357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3032387=7=2<202274223:20313730373233393531352>3431313634347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363338323431323835393937362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531362>353937363036347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323235353031303931353131382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531362>353938333939347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330382>3332377=7=2<202274223:20313730373233393531352>393235383034347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238342>3236317=7=2<202274223:20313730373233393531352>373631323934317=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331342>3230327=7=2<202274223:20313730373233393531362>343339343734337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3335367=7=2<202274223:20313730373233393531362>323733373634317=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137333931383737383135343832382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531372>373436303538357=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323331343732373635383631382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531372>373436373537377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331312>3739357=7=2<202274223:20313730373233393531362>393732343237387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3831367=7=2<202274223:20313730373233393531362>373930313436367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330362>37347=7=2<202274223:20313730373233393531372>343835363138387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3031327=7=2<202274223:20313730373233393531372>3330353939317=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363938343436353135383331362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531382>3839343335397=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363232373838393735363738352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393531382>3839353234387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036312<2022706?776572223:203331332>3936337=7=2<202274223:20313730373233393531372>393937393236327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238372>3139327=7=2<202274223:20313730373233393531372>383138333133347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330342>3537327=7=2<202274223:20313730373233393531382>353132323738337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238342>3539357=7=2<202274223:20313730373233393531382>333330393038337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3636317=7=2<202274223:20313730373233393531382>3834343133327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373632383237333839373435352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532302>3034323639347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363233313837303837383830382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532302>303433353935367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331322>3930337=7=2<202274223:20313730373233393531392>303234393130347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3733367=7=2<202274223:20313730373233393531392>333536363933337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330352>3633327=7=2<202274223:20313730373233393531392>353632323534327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203239302>3639367=7=2<202274223:20313730373233393531392>383732363735347=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353436353435323139363237382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532312>313930393831317=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313633373935393639323533342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532312>313931383131387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203239392>3232397=7=2<202274223:20313730373233393532302>3038313930347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238312>3333377=7=2<202274223:20313730373233393532302>333834383135357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331352>33317=7=2<202274223:20313730373233393532302>353936333831327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3139337=7=2<202274223:20313730373233393532302>383938373631377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203330332>3232347=7=2<202274223:20313730373233393532312>313038343636347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363033313536353337383738362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532322>333339343635347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136303834313832333238343832332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532322>333430333138327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036312<2022706?776572223:203331382>3533397=7=2<202274223:20313730373233393532312>363232343033317=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3237377=7=2<202274223:20313730373233393532312>3431383839317=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330322>3938357=7=2<202274223:20313730373233393532322>3136333132357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3739317=7=2<202274223:20313730373233393532312>393434373238397=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363532313935383034303334332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532332>3438373830357=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313932343538343939353333332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532332>3438383638357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331352>3834347=7=2<202274223:20313730373233393532322>363832373038357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203239302>3136347=7=2<202274223:20313730373233393532322>3435373834327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3433317=7=2<202274223:20313730373233393532332>313934393130337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3836327=7=2<202274223:20313730373233393532322>393730363432387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239312>32327=7=2<202274223:20313730373233393532332>3438323830327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353137343433353138383337342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532342>363336323237367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363134383636383836383431362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532342>363337313133337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203330332>3735387=7=2<202274223:20313730373233393532332>373039303331337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3739317=7=2<202274223:20313730373233393532332>393936323138377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331362>3038337=7=2<202274223:20313730373233393532342>323231343936387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238302>3332337=7=2<202274223:20313730373233393532342>3533393733327=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373531393833353430363133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532352>373834353735377=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323538313436363939303434362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532352>373835333338327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3034357=7=2<202274223:20313730373233393532342>373539383432327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203239322>3237377=7=2<202274223:20313730373233393532352>303531393034327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3433317=7=2<202274223:20313730373233393532352>323733323030337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>31327=7=2<202274223:20313730373233393532352>353635323637387=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137343934333232323439323837352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532362>393333303831367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313233313932313835383138352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532362>393333393037357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3537327=7=2<202274223:20313730373233393532352>373836353531327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2035332<2022706?776572223:203239312>32327=7=2<202274223:20313730373233393532362>303737373131367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3433317=7=2<202274223:20313730373233393532362>323938353835347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238312>3035317=7=2<202274223:20313730373233393532362>353930373837327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3433317=7=2<202274223:20313730373233393532362>383132323534327=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137353531373237383036393439322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532382>303831353233347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323639363932333633303836352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532382>3038323233397=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3034357=7=2<202274223:20313730373233393532372>333534313336327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203237392>3436337=7=2<202274223:20313730373233393532372>313333383931337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331372>3139317=7=2<202274223:20313730373233393532372>3836363735357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238392>3837387=7=2<202274223:20313730373233393532372>363436353432337=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373536343038363339373436312<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532392>3232393839347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363236383839363130353932372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393532392>323330363731367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3537327=7=2<202274223:20313730373233393532382>3338323335347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237382>3430367=7=2<202274223:20313730373233393532382>3136303730387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331362>3635387=7=2<202274223:20313730373233393532382>383938333032367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239342>3134347=7=2<202274223:20313730373233393532382>3637393433367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203237382>31367=7=2<202274223:20313730373233393532392>313933343536367=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363135353135373931333135382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533302>333738313933347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>313632383935393930303731332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533302>333739303534337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036322<2022706?776572223:203330342>3238347=7=2<202274223:20313730373233393532392>343130333438327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203239322>3033387=7=2<202274223:20313730373233393532392>373333373037377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330362>3435337=7=2<202274223:20313730373233393532392>393438333936327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239322>3237377=7=2<202274223:20313730373233393533302>323438303434337=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373635313339383339373533332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533312>353236343935357=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363139323835363539363236392<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533312>353237333738387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331362>3839377=7=2<202274223:20313730373233393533302>343632323939337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203237392>3436337=7=2<202274223:20313730373233393533302>373631393539337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330362>37347=7=2<202274223:20313730373233393533302>393736383730357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238382>3832317=7=2<202274223:20313730373233393533312>323734313235367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331352>3032337=7=2<202274223:20313730373233393533312>343839303731347=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373531303739333530303139372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533322>363734383330347=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136313830393133373636313439332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533322>363735373135377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203330392>3134377=7=2<202274223:20313730373233393533322>303032313337377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238322>3339347=7=2<202274223:20313730373233393533312>373838303934387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331352>35357=7=2<202274223:20313730373233393533322>3534323435347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238382>3837347=7=2<202274223:20313730373233393533322>333239393130337=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137343931313333313532343138322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533332>383233313436387=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333539363733363834343730332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533332>383233383331387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331322>3337367=7=2<202274223:20313730373233393533332>303535383734387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238332>3733367=7=2<202274223:20313730373233393533322>383433303737347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331322>3631357=7=2<202274223:20313730373233393533332>353638323739337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3133367=7=2<202274223:20313730373233393533332>333535323232377=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373635383537353037393932322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533342>393731343732357=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333430353631393030353735382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533342>3937323236327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203331322>3337367=7=2<202274223:20313730373233393533342>303832313835337=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238362>3133367=7=2<202274223:20313730373233393533332>383639363035387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331352>3032337=7=2<202274223:20313730373233393533342>353935313830377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238342>3739337=7=2<202274223:20313730373233393533342>333831363230327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3839377=7=2<202274223:20313730373233393533342>3839353530387=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137373235393538373733333135342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533362>313139393930337=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333737393839353031373530352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533362>3132303836337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036322<2022706?776572223:203331302>3230387=7=2<202274223:20313730373233393533352>313039303938347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3035357=7=2<202274223:20313730373233393533352>34313035397=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331332>3936337=7=2<202274223:20313730373233393533352>3632373531327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238372>3532397=7=2<202274223:20313730373233393533352>393234393338377=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363437303132363733363237332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533372>323638333332377=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136333132363931323332333731352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533372>3236393133397=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203330392>3134377=7=2<202274223:20313730373233393533362>313431353038367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238342>3739337=7=2<202274223:20313730373233393533362>343433303535347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331362>3635387=7=2<202274223:20313730373233393533362>363533383237347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238362>3934377=7=2<202274223:20313730373233393533362>393633383636377=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331352>3834347=7=2<202274223:20313730373233393533372>313639313130337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137343839353338363037393631362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533382>343136383339317=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363139393232363136363533382<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533382>343137353533327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331302>3733347=7=2<202274223:20313730373233393533372>363833333439367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238352>3331387=7=2<202274223:20313730373233393533372>343736343539357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331372>3433317=7=2<202274223:20313730373233393533382>313936373234327=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3437387=7=2<202274223:20313730373233393533372>393935393432367=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373530363638303039393835322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533392>35363631397=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3136323539333431303731363331352<2022756>697473223:202254666<6?7073222<202274223:20313730373233393533392>353636393433327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203331302>3230387=7=2<202274223:20313730373233393533382>37303930367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3037397=7=2<202274223:20313730373233393533382>353038353235367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331342>3230327=7=2<202274223:20313730373233393533392>3232313937377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238372>3437387=7=2<202274223:20313730373233393533392>303234313030357=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238342>3032337=7=2<202274223:20313730373233393533392>353430303734337=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>31373534383933373137393532362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534302>3731343439357=z[7;227461736;223:2022747261696>222<202272617465223:2031392>3135393239373530373330333638342<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534302>373135353239347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331312>3032387=7=2<202274223:20313730373233393533392>373334363736387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238372>3437387=7=2<202274223:20313730373233393534302>3035323430367=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331372>3731387=7=2<202274223:20313730373233393534302>323437303833347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238342>3838327=7=2<202274223:20313730373233393534302>353636333833367=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363039313336383031393036322<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534312>383632383937367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363032343437363433393238372<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534312>3836333834387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203330382>3332377=7=2<202274223:20313730373233393534302>3736303338347=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238352>3630347=7=2<202274223:20313730373233393534312>303830353031337=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331342>3132327=7=2<202274223:20313730373233393534312>323733313031367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238322>3339347=7=2<202274223:20313730373233393534312>3539333133347=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203332302>3033387=7=2<202274223:20313730373233393534312>3738363934377=z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363634353535363839363133332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534332>3031313536317=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363134383636383836383431362<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534332>303132343132387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203330372>387=7=2<202274223:20313730373233393534322>333032373632357=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238392>3334367=7=2<202274223:20313730373233393534322>313036343837387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203331362>3839377=7=2<202274223:20313730373233393534322>383136363733387=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203238382>3533357=7=2<202274223:20313730373233393534322>3634383432327=z[0:z[0:z[7;227461736;223:2022747261696>222<202272617465223:2031392>3137363235343833303437323331332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534342>313539393132367=z[7;227461736;223:2022747261696>222<202272617465223:2031392>31363133333934303037383637332<2022756>697473223:202254666<6?7073222<202274223:20313730373233393534342>313630383039387=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20302>39392<202274656=7065726174757265223:2036332<2022706?776572223:203330382>3033397=7=2<202274223:20313730373233393534332>333335373831367=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035322<2022706?776572223:203238332>3738337=7=2<202274223:20313730373233393534332>313631383836327=z[0:z[0:z[7;227461736;223:20226=61696>222<202267707564617461223:207;2230223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2036332<2022706?776572223:203332302>3132367=7=2<202274223:20313730373233393534332>383438333833377=z[7;227461736;223:20226=61696>222<202267707564617461223:207;2231223:207;226=656=6?7279223:205;313938392>343337352<2038313932302>305=2<20226<6?6164223:20312>302<202274656=7065726174757265223:2035332<2022706?776572223:203239302>3430327=7=2<202274223:20313730373233393534332>363738323533377=z[0:z[0:z
real	1m45.138s
user	1m52.414s
sys	1m44.574s
---
resnet50
========
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

real	7m39.036s
user	26m38.497s
sys	3m25.387s
---
convnext_large-fp32
===================
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

real	52m57.528s
user	73m30.090s
sys	49m19.248s
---
convnext_large-fp16
===================
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/Tmp/slurm.4115007.0/base/venv/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
slurmstepd: error: *** JOB 4115007 ON cn-g024 CANCELLED AT 2024-02-06T13:24:45 DUE TO TIME LIMIT ***

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Tue Feb  6 13:24:45 2024
Driver Version                            : 535.104.12
CUDA Version                              : 12.2

Attached GPUs                             : 2
GPU 00000000:81:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 15486
            GPU Utilization               : 0 %
            Memory Utilization            : 0 %
            Max memory usage              : 574 MiB
            Time                          : 103278 ms
            Is Running                    : 0
        Process ID                        : 16229
            GPU Utilization               : 1 %
            Memory Utilization            : 0 %
            Max memory usage              : 574 MiB
            Time                          : 3360 ms
            Is Running                    : 0
        Process ID                        : 16325
            GPU Utilization               : 0 %
            Memory Utilization            : 0 %
            Max memory usage              : 574 MiB
            Time                          : 102656 ms
            Is Running                    : 0
        Process ID                        : 17030
            GPU Utilization               : 1 %
            Memory Utilization            : 0 %
            Max memory usage              : 574 MiB
            Time                          : 3452 ms
            Is Running                    : 0
        Process ID                        : 17273
            GPU Utilization               : 2 %
            Memory Utilization            : 0 %
            Max memory usage              : 4712 MiB
            Time                          : 44212 ms
            Is Running                    : 0
        Process ID                        : 17725
            GPU Utilization               : 76 %
            Memory Utilization            : 56 %
            Max memory usage              : 27548 MiB
            Time                          : 307134 ms
            Is Running                    : 0
        Process ID                        : 18964
            GPU Utilization               : 90 %
            Memory Utilization            : 21 %
            Max memory usage              : 910 MiB
            Time                          : 21263 ms
            Is Running                    : 0
        Process ID                        : 19092
            GPU Utilization               : 83 %
            Memory Utilization            : 19 %
            Max memory usage              : 910 MiB
            Time                          : 7814 ms
            Is Running                    : 0
        Process ID                        : 19180
            GPU Utilization               : 96 %
            Memory Utilization            : 26 %
            Max memory usage              : 1288 MiB
            Time                          : 13983 ms
            Is Running                    : 0
        Process ID                        : 19282
            GPU Utilization               : 99 %
            Memory Utilization            : 4 %
            Max memory usage              : 1288 MiB
            Time                          : 104066 ms
            Is Running                    : 0
        Process ID                        : 19702
            GPU Utilization               : 35 %
            Memory Utilization            : 15 %
            Max memory usage              : 3852 MiB
            Time                          : 453580 ms
            Is Running                    : 0
        Process ID                        : 9877
            GPU Utilization               : 97 %
            Memory Utilization            : 16 %
            Max memory usage              : 48704 MiB
            Time                          : 3176123 ms
            Is Running                    : 0
        Process ID                        : 9956
            GPU Utilization               : 74 %
            Memory Utilization            : 37 %
            Max memory usage              : 26584 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:C1:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 17726
            GPU Utilization               : 76 %
            Memory Utilization            : 56 %
            Max memory usage              : 27548 MiB
            Time                          : 307508 ms
            Is Running                    : 0
        Process ID                        : 18965
            GPU Utilization               : 91 %
            Memory Utilization            : 21 %
            Max memory usage              : 910 MiB
            Time                          : 21299 ms
            Is Running                    : 0
        Process ID                        : 19093
            GPU Utilization               : 77 %
            Memory Utilization            : 18 %
            Max memory usage              : 910 MiB
            Time                          : 7734 ms
            Is Running                    : 0
        Process ID                        : 19181
            GPU Utilization               : 94 %
            Memory Utilization            : 26 %
            Max memory usage              : 1288 MiB
            Time                          : 14005 ms
            Is Running                    : 0
        Process ID                        : 19283
            GPU Utilization               : 99 %
            Memory Utilization            : 4 %
            Max memory usage              : 1288 MiB
            Time                          : 103897 ms
            Is Running                    : 0
        Process ID                        : 19703
            GPU Utilization               : 35 %
            Memory Utilization            : 15 %
            Max memory usage              : 3852 MiB
            Time                          : 457533 ms
            Is Running                    : 0
        Process ID                        : 9878
            GPU Utilization               : 97 %
            Memory Utilization            : 17 %
            Max memory usage              : 48704 MiB
            Time                          : 3154313 ms
            Is Running                    : 0
        Process ID                        : 9957
            GPU Utilization               : 74 %
            Memory Utilization            : 37 %
            Max memory usage              : 26584 MiB
            Time                          : 0 ms
            Is Running                    : 1

Tue Feb  6 13:24:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:81:00.0 Off |                    0 |
| N/A   66C    P0             400W / 500W |  26594MiB / 81920MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:C1:00.0 Off |                    0 |
| N/A   53C    P0             176W / 500W |  26594MiB / 81920MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      9956      C   python                                    26584MiB |
|    1   N/A  N/A      9957      C   python                                    26584MiB |
+---------------------------------------------------------------------------------------+
