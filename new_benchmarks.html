<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Creating a new benchmark &mdash; milabench  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=5929fcd5"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Reference" href="reference.html" />
    <link rel="prev" title="Using milabench (DEVELOPERS)" href="dev-usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            milabench
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="usage.html">Install and use</a></li>
<li class="toctree-l1"><a class="reference internal" href="docker.html">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev-usage.html">Using milabench (DEVELOPERS)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Creating a new benchmark</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#benchfile-py">benchfile.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#requirements-in">requirements.in</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prepare-py">prepare.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#main-py">main.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#voirfile-py">voirfile.py</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#development">Development</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integrating-in-base-yaml">Integrating in base.yaml</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="sizer.html">Scaling</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">milabench</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Creating a new benchmark</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/new_benchmarks.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="creating-a-new-benchmark">
<h1>Creating a new benchmark<a class="headerlink" href="#creating-a-new-benchmark" title="Link to this heading"></a></h1>
<p>To define a new benchmark (let’s assume it is called <code class="docutils literal notranslate"><span class="pre">ornatebench</span></code>), make a copy of <code class="docutils literal notranslate"><span class="pre">benchmarks/_template</span></code> using <code class="docutils literal notranslate"><span class="pre">cp-template</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp-template<span class="w"> </span>benchmarks/_template/<span class="w"> </span>benchmarks/ornatebench
</pre></div>
</div>
<p>You should see a directory with the following structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ornatebench</span>
<span class="o">|-</span> <span class="n">README</span><span class="o">.</span><span class="n">md</span>          <span class="c1"># Document the benchmark here</span>
<span class="o">|-</span> <span class="n">benchfile</span><span class="o">.</span><span class="n">py</span>       <span class="c1"># Benchmark definition file</span>
<span class="o">|-</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span>            <span class="c1"># Executed by milabench run</span>
<span class="o">|-</span> <span class="n">prepare</span><span class="o">.</span><span class="n">py</span>         <span class="c1"># Executed by milabench prepare (EXECUTABLE)</span>
<span class="o">|-</span> <span class="n">requirements</span><span class="o">.</span><span class="ow">in</span>    <span class="c1"># Python requirements to install from pip</span>
<span class="o">|-</span> <span class="n">voirfile</span><span class="o">.</span><span class="n">py</span>        <span class="c1"># Probes and extra instruments</span>
</pre></div>
</div>
<p>Some of these files may be unnecessary depending on the benchmark.</p>
<p>First of all, if you want to verify that everything works, you can use the <code class="docutils literal notranslate"><span class="pre">dev.yaml</span></code> benchmark config that comes with the template:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># You can also use --config</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MILABENCH_CONFIG</span><span class="o">=</span>benchmarks/ornatebench/dev.yaml

milabench<span class="w"> </span>install
milabench<span class="w"> </span>prepare
milabench<span class="w"> </span>run
</pre></div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<section id="benchfile-py">
<h3>benchfile.py<a class="headerlink" href="#benchfile-py" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">benchfile.py</span></code> defines what to do on <code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">install/prepare/run</span></code>. It is run from the benchmark directory directly, in the <em>current</em> virtual environment, but it can create <em>new processes</em> in the virtual environment of the benchmark.</p>
<p>By default it will dispatch to <code class="docutils literal notranslate"><span class="pre">requirements.in</span></code> for install requirements, <code class="docutils literal notranslate"><span class="pre">prepare.py</span></code> for prep work and downloading datasets, and <code class="docutils literal notranslate"><span class="pre">main.py</span></code> for running the actual benchmark. If that is suitable you may not need to change it at all.</p>
</section>
<section id="requirements-in">
<h3>requirements.in<a class="headerlink" href="#requirements-in" title="Link to this heading"></a></h3>
<p>Write all of the benchmark’s requirements in this file. Use <code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">install</span> <span class="pre">--config</span> <span class="pre">benchmarks/ornatebench/dev.yaml</span></code> to install them during development (add <code class="docutils literal notranslate"><span class="pre">--force</span></code> if you made changes and want to reinstall.)</p>
</section>
<section id="prepare-py">
<h3>prepare.py<a class="headerlink" href="#prepare-py" title="Link to this heading"></a></h3>
<p>This script is executed in the venv for the benchmark when you run <code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">prepare</span></code>.</p>
<p>The purpose of <code class="docutils literal notranslate"><span class="pre">prepare.py</span></code> is to download and/or generate everything that is required by the main script, so that the main script does not need to use the network and can start training right away. In particular, it must:</p>
<ul class="simple">
<li><p>Download any datasets required by the main script into <code class="docutils literal notranslate"><span class="pre">$MILABENCH_DATA_DIR</span></code>.</p></li>
<li><p>Preprocess the data, if this must be done prior to training.</p></li>
<li><p>Generate synthetic data into <code class="docutils literal notranslate"><span class="pre">$MILABENCH_DATA_DIR</span></code> (if needed).</p></li>
<li><p>Download and cache pretrained model weights (if needed).
* Weights should ideally go somewhere under <code class="docutils literal notranslate"><span class="pre">$XDG_CACHE_HOME</span></code> (which milabench sets to <code class="docutils literal notranslate"><span class="pre">$MILABENCH_BASE/cache</span></code>).
* Note that most frameworks already cache weights in subdirectories of <code class="docutils literal notranslate"><span class="pre">$XDG_CACHE_HOME</span></code>, so it is usually sufficient to import the framework, load the model, and then quit without training it.</p></li>
</ul>
<p>If no preparation is needed, this file should be removed.</p>
</section>
<section id="main-py">
<h3>main.py<a class="headerlink" href="#main-py" title="Link to this heading"></a></h3>
<p>This is the main script that will be benchmarked when you run <code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">run</span></code>. It is run as <code class="docutils literal notranslate"><span class="pre">voir</span> <span class="pre">main.py</span> <span class="pre">ARGV...</span></code></p>
<p>The template <code class="docutils literal notranslate"><span class="pre">main.py</span></code> demonstrates a simple loop that you can adapt to any script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">voir</span><span class="o">.</span><span class="n">iterate</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">report_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="n">give</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Wrap the training loop with <code class="docutils literal notranslate"><span class="pre">voir.iterate</span></code>.
* <code class="docutils literal notranslate"><span class="pre">report_batch=True</span></code> triggers the computation of the number of training samples per second.
* Set <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> to the batch_size. milabench can also figure it out automatically if you are iterating over the input batches (it will use the first number in the tensor’s shape).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">give(loss=loss.item())</span></code> will forward the value of the loss to milabench. Make sure the value is a plain Python <code class="docutils literal notranslate"><span class="pre">float</span></code>.</p></li>
</ul>
<p>If the script takes command line arguments, you can parse them however you like, for example with <code class="docutils literal notranslate"><span class="pre">argparse.ArgumentParser</span></code>. Then, you can add an <code class="docutils literal notranslate"><span class="pre">argv</span></code> section in <code class="docutils literal notranslate"><span class="pre">dev.yaml</span></code>, just like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">trivial</span><span class="p">:</span>
<span class="w">  </span><span class="nt">inherits</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">_defaults</span>
<span class="w">  </span><span class="nt">definition</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.</span>

<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>

<span class="w">  </span><span class="c1"># Pass arguments to main.py below</span>
<span class="w">  </span><span class="nt">argv</span><span class="p">:</span>
<span class="w">    </span><span class="nt">--batch-size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">argv</span></code> can also be an array if you need to pass positional arguments, but I recommend using named parameters only.</p>
</section>
<section id="voirfile-py">
<h3>voirfile.py<a class="headerlink" href="#voirfile-py" title="Link to this heading"></a></h3>
<p>The voirfile contains instrumentation for the main script. You can usually just leave it as it is. By default, it will:</p>
<ul class="simple">
<li><p>Compute the train “rate” (number of samples per second) using events from <code class="docutils literal notranslate"><span class="pre">voir.iterate</span></code>.</p></li>
<li><p>Forcefully stop the program after a certain number of rate measurements.</p></li>
<li><p>Monitor GPU usage.</p></li>
</ul>
</section>
</section>
<section id="development">
<h2>Development<a class="headerlink" href="#development" title="Link to this heading"></a></h2>
<p>To develop the benchmark, first run <code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">dev</span> <span class="pre">--config</span> <span class="pre">benchmarks/BENCHNAME/dev.yaml</span></code>. This will activate the benchmark’s virtual environment and put you into a shell.</p>
<p>Then, try and run <code class="docutils literal notranslate"><span class="pre">voir</span> <span class="pre">--dash</span> <span class="pre">main.py</span></code>. This should show you a little dashboard and display losses, train rate calculations and one or more progress bars.</p>
<p>From there, you can develop as you would any other Python program.</p>
</section>
<section id="integrating-in-base-yaml">
<h2>Integrating in base.yaml<a class="headerlink" href="#integrating-in-base-yaml" title="Link to this heading"></a></h2>
<p>You can copy-paste the contents of <code class="docutils literal notranslate"><span class="pre">dev.yaml</span></code> into <code class="docutils literal notranslate"><span class="pre">config/base.yaml</span></code>, you will only need to change:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">definition</span></code> should be the relative path to the <code class="docutils literal notranslate"><span class="pre">benchfile.py</span></code>.</p></li>
<li><p>Remove <code class="docutils literal notranslate"><span class="pre">install_variant:</span> <span class="pre">unpinned</span></code></p></li>
<li><p>If the benchmark’s requirements are compatible with those of other benchmarks, you can set <code class="docutils literal notranslate"><span class="pre">install_group</span></code> to the same <code class="docutils literal notranslate"><span class="pre">install_group</span></code> as them. For example, <code class="docutils literal notranslate"><span class="pre">install_group:</span> <span class="pre">torch</span></code>.</p></li>
</ul>
<p>Then, run the following commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">pin</span> <span class="pre">--select</span> <span class="pre">NAME_OR_INSTALL_GROUP</span> <span class="pre">--variant</span> <span class="pre">cuda</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">milabench</span> <span class="pre">pin</span> <span class="pre">--select</span> <span class="pre">NAME_OR_INSTALL_GROUP</span> <span class="pre">--variant</span> <span class="pre">rocm</span></code></p></li>
</ul>
<p>This will create <code class="docutils literal notranslate"><span class="pre">requirements.&lt;arch&gt;.txt</span></code> for these two architectures. These files must be checked in under version control.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">--variant</span> <span class="pre">unpinned</span></code> means installing directly from <code class="docutils literal notranslate"><span class="pre">requirements.in</span></code>. This can be useful during development, but less stable over time since various dependencies may break.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dev-usage.html" class="btn btn-neutral float-left" title="Using milabench (DEVELOPERS)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="reference.html" class="btn btn-neutral float-right" title="Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Mila IDT.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>