/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
Added key: store_based_barrier_key:1 to store for rank: 2
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 4, total 8, device cuda:4.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 7, total 8, device cuda:7.
Training in distributed mode with multiple processes, 1 device per process.Process 6, total 8, device cuda:6.
Training in distributed mode with multiple processes, 1 device per process.Process 3, total 8, device cuda:3.
Training in distributed mode with multiple processes, 1 device per process.Process 2, total 8, device cuda:2.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 8, device cuda:1.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 5, total 8, device cuda:5.
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 8, device cuda:0.
Model resnet152 created, param count:60192808
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.8) calculated from base learning rate (0.1) and global batch size (2048) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/2 (  0%)]  Loss: 6.941 (6.94)  Time: 2.419s,  846.60/s  (2.419s,  846.60/s)  LR: 1.000e-05  Data: 1.269 (1.269)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [   1/2 (100%)]  Loss: 6.942 (6.94)  Time: 0.525s, 3901.81/s  (1.472s, 1391.32/s)  LR: 1.000e-05  Data: 0.005 (0.637)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.286 (1.286)  Loss:  6.9080 (6.9080)  Acc@1:  0.2441 ( 0.2441)  Acc@5:  0.7812 ( 0.7812)
Test: [   2/2]  Time: 0.437 (0.634)  Loss:  6.8999 (6.9076)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.6541)
Current checkpoints:
 ('/milabench/envs/extra/timm/redizilo.2023-05-08_17:54:51.224604/resnet152-multi.0/20230508-183039-resnet152-224/checkpoint-0.pth.tar', 0.12112403100775193)

Train: 1 [   0/2 (  0%)]  Loss: 6.945 (6.95)  Time: 1.629s, 1257.39/s  (1.629s, 1257.39/s)  LR: 1.600e-01  Data: 1.123 (1.123)
Train: 1 [   1/2 (100%)]  Loss: 6.921 (6.93)  Time: 0.512s, 4003.06/s  (1.070s, 1913.68/s)  LR: 1.600e-01  Data: 0.000 (0.561)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.165 (1.165)  Loss:  6.8983 (6.8983)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   2/2]  Time: 0.025 (0.456)  Loss:  6.8586 (6.8980)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 0.9448)
Current checkpoints:
 ('/milabench/envs/extra/timm/redizilo.2023-05-08_17:54:51.224604/resnet152-multi.0/20230508-183039-resnet152-224/checkpoint-1.pth.tar', 0.2180232558139535)

Train: 2 [   0/2 (  0%)]  Loss: 6.869 (6.87)  Time: 1.641s, 1247.94/s  (1.641s, 1247.94/s)  LR: 3.200e-01  Data: 1.131 (1.131)
Train: 2 [   1/2 (100%)]  Loss: 6.884 (6.88)  Time: 0.512s, 4003.12/s  (1.076s, 1902.73/s)  LR: 3.200e-01  Data: 0.000 (0.566)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.167 (1.167)  Loss:  6.8702 (6.8702)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [   2/2]  Time: 0.023 (0.456)  Loss:  6.7472 (6.8684)  Acc@1: 15.6250 ( 0.1938)  Acc@5: 28.1250 ( 1.1628)
Train: 3 [   0/2 (  0%)]  Loss: 6.835 (6.84)  Time: 1.624s, 1261.00/s  (1.624s, 1261.00/s)  LR: 4.800e-01  Data: 1.114 (1.114)
Train: 3 [   1/2 (100%)]  Loss: 6.929 (6.88)  Time: 0.513s, 3995.50/s  (1.068s, 1916.99/s)  LR: 4.800e-01  Data: 0.000 (0.557)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.165 (1.165)  Loss:  6.8397 (6.8397)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.4395 ( 0.4395)
Test: [   2/2]  Time: 0.023 (0.455)  Loss:  6.6047 (6.8365)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 1.1386)
Train: 4 [   0/2 (  0%)]  Loss: 6.853 (6.85)  Time: 1.617s, 1266.64/s  (1.617s, 1266.64/s)  LR: 6.400e-01  Data: 1.104 (1.104)
Train: 4 [   1/2 (100%)]  Loss: 6.980 (6.92)  Time: 0.515s, 3976.38/s  (1.066s, 1921.28/s)  LR: 6.400e-01  Data: 0.001 (0.552)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.171 (1.171)  Loss:  6.8234 (6.8234)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.4395 ( 0.4395)
Test: [   2/2]  Time: 0.028 (0.459)  Loss:  6.5302 (6.8192)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 1.0174)
Train: 5 [   0/2 (  0%)]  Loss: 6.876 (6.88)  Time: 1.605s, 1275.84/s  (1.605s, 1275.84/s)  LR: 7.995e-01  Data: 1.096 (1.096)
Train: 5 [   1/2 (100%)]  Loss: 6.985 (6.93)  Time: 0.517s, 3963.21/s  (1.061s, 1930.28/s)  LR: 7.995e-01  Data: 0.000 (0.548)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.159 (1.159)  Loss:  6.8264 (6.8264)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [   2/2]  Time: 0.023 (0.453)  Loss:  6.5199 (6.8185)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 1.0659)
Train: 6 [   0/2 (  0%)]  Loss: 6.851 (6.85)  Time: 1.612s, 1270.58/s  (1.612s, 1270.58/s)  LR: 7.992e-01  Data: 1.100 (1.100)
Train: 6 [   1/2 (100%)]  Loss: 7.011 (6.93)  Time: 0.514s, 3988.02/s  (1.063s, 1927.17/s)  LR: 7.992e-01  Data: 0.000 (0.550)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.153 (1.153)  Loss:  6.8320 (6.8320)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   2/2]  Time: 0.023 (0.451)  Loss:  6.4845 (6.8222)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.0901)
Current checkpoints:
 ('/milabench/envs/extra/timm/redizilo.2023-05-08_17:54:51.224604/resnet152-multi.0/20230508-183039-resnet152-224/checkpoint-6.pth.tar', 0.24224806201550386)

Train: 7 [   0/2 (  0%)]  Loss: 6.872 (6.87)  Time: 1.586s, 1291.11/s  (1.586s, 1291.11/s)  LR: 7.989e-01  Data: 1.078 (1.078)
Train: 7 [   1/2 (100%)]  Loss: 6.979 (6.93)  Time: 0.513s, 3990.23/s  (1.050s, 1950.96/s)  LR: 7.989e-01  Data: 0.000 (0.539)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.157 (1.157)  Loss:  6.8373 (6.8373)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.5137 ( 1.5137)
Test: [   2/2]  Time: 0.024 (0.453)  Loss:  6.4746 (6.8254)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.1870)
Train: 8 [   0/2 (  0%)]  Loss: 6.862 (6.86)  Time: 1.606s, 1275.12/s  (1.606s, 1275.12/s)  LR: 7.986e-01  Data: 1.092 (1.092)
Train: 8 [   1/2 (100%)]  Loss: 6.980 (6.92)  Time: 0.518s, 3950.96/s  (1.062s, 1928.00/s)  LR: 7.986e-01  Data: 0.001 (0.546)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.172 (1.172)  Loss:  6.8324 (6.8324)  Acc@1:  0.4883 ( 0.4883)  Acc@5:  1.3672 ( 1.3672)
Test: [   2/2]  Time: 0.023 (0.457)  Loss:  6.5575 (6.8263)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.0659)
Train: 9 [   0/2 (  0%)]  Loss: 6.873 (6.87)  Time: 1.603s, 1277.66/s  (1.603s, 1277.66/s)  LR: 7.982e-01  Data: 1.088 (1.088)
Train: 9 [   1/2 (100%)]  Loss: 6.948 (6.91)  Time: 0.518s, 3957.35/s  (1.060s, 1931.67/s)  LR: 7.982e-01  Data: 0.000 (0.544)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.182 (1.182)  Loss:  6.8268 (6.8268)  Acc@1:  0.7324 ( 0.7324)  Acc@5:  1.9043 ( 1.9043)
Test: [   2/2]  Time: 0.021 (0.460)  Loss:  6.7054 (6.8252)  Acc@1:  0.0000 ( 0.3634)  Acc@5:  0.0000 ( 1.1143)
Current checkpoints:
 ('/milabench/envs/extra/timm/redizilo.2023-05-08_17:54:51.224604/resnet152-multi.0/20230508-183039-resnet152-224/checkpoint-9.pth.tar', 0.3633720930232558)

Train: 10 [   0/2 (  0%)]  Loss: 6.848 (6.85)  Time: 1.618s, 1265.55/s  (1.618s, 1265.55/s)  LR: 7.978e-01  Data: 1.105 (1.105)
Train: 10 [   1/2 (100%)]  Loss: 6.942 (6.90)  Time: 0.513s, 3992.48/s  (1.066s, 1921.89/s)  LR: 7.978e-01  Data: 0.001 (0.553)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.171 (1.171)  Loss:  6.8181 (6.8181)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  2.2461 ( 2.2461)
Test: [   2/2]  Time: 0.022 (0.457)  Loss:  6.6597 (6.8233)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.1143)
Train: 11 [   0/2 (  0%)]  Loss: 6.850 (6.85)  Time: 1.631s, 1255.33/s  (1.631s, 1255.33/s)  LR: 7.973e-01  Data: 1.124 (1.124)
Train: 11 [   1/2 (100%)]  Loss: 6.954 (6.90)  Time: 0.513s, 3991.65/s  (1.072s, 1909.98/s)  LR: 7.973e-01  Data: 0.000 (0.562)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.187 (1.187)  Loss:  6.8270 (6.8270)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.5137 ( 1.5137)
Test: [   2/2]  Time: 0.024 (0.463)  Loss:  6.5345 (6.8267)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.1386)
Train: 12 [   0/2 (  0%)]  Loss: 6.862 (6.86)  Time: 1.590s, 1287.91/s  (1.590s, 1287.91/s)  LR: 7.968e-01  Data: 1.084 (1.084)
Train: 12 [   1/2 (100%)]  Loss: 6.924 (6.89)  Time: 0.511s, 4005.11/s  (1.051s, 1949.07/s)  LR: 7.968e-01  Data: 0.000 (0.542)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.169 (1.169)  Loss:  6.8298 (6.8298)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.7090 ( 1.7090)
Test: [   2/2]  Time: 0.021 (0.456)  Loss:  6.5118 (6.8260)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.1143)
Train: 13 [   0/2 (  0%)]  Loss: 6.858 (6.86)  Time: 1.578s, 1297.53/s  (1.578s, 1297.53/s)  LR: 7.963e-01  Data: 1.072 (1.072)
Train: 13 [   1/2 (100%)]  Loss: 6.921 (6.89)  Time: 0.511s, 4005.23/s  (1.045s, 1960.08/s)  LR: 7.963e-01  Data: 0.000 (0.536)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.165 (1.165)  Loss:  6.8331 (6.8331)  Acc@1:  0.5859 ( 0.5859)  Acc@5:  1.2207 ( 1.2207)
Test: [   2/2]  Time: 0.022 (0.455)  Loss:  6.5116 (6.8269)  Acc@1:  0.0000 ( 0.2907)  Acc@5:  0.0000 ( 1.0417)
Train: 14 [   0/2 (  0%)]  Loss: 6.834 (6.83)  Time: 1.599s, 1281.09/s  (1.599s, 1281.09/s)  LR: 7.957e-01  Data: 1.092 (1.092)
Train: 14 [   1/2 (100%)]  Loss: 6.949 (6.89)  Time: 0.512s, 3997.97/s  (1.055s, 1940.41/s)  LR: 7.957e-01  Data: 0.000 (0.546)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.161 (1.161)  Loss:  6.8416 (6.8416)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.5137 ( 1.5137)
Test: [   2/2]  Time: 0.022 (0.454)  Loss:  6.5283 (6.8310)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 0.8479)
Train: 15 [   0/2 (  0%)]  Loss: 6.837 (6.84)  Time: 1.605s, 1275.82/s  (1.605s, 1275.82/s)  LR: 7.951e-01  Data: 1.098 (1.098)
Train: 15 [   1/2 (100%)]  Loss: 6.924 (6.88)  Time: 0.512s, 3999.68/s  (1.059s, 1934.56/s)  LR: 7.951e-01  Data: 0.000 (0.549)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.169 (1.169)  Loss:  6.8333 (6.8333)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.8066 ( 1.8066)
Test: [   2/2]  Time: 0.022 (0.456)  Loss:  6.5437 (6.8305)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 0.9205)
Train: 16 [   0/2 (  0%)]  Loss: 6.841 (6.84)  Time: 1.598s, 1281.48/s  (1.598s, 1281.48/s)  LR: 7.944e-01  Data: 1.092 (1.092)
Train: 16 [   1/2 (100%)]  Loss: 6.913 (6.88)  Time: 0.512s, 3998.34/s  (1.055s, 1940.90/s)  LR: 7.944e-01  Data: 0.000 (0.546)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.169 (1.169)  Loss:  6.8336 (6.8336)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.5137 ( 1.5137)
Test: [   2/2]  Time: 0.023 (0.457)  Loss:  6.3699 (6.8400)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.0174)
Train: 17 [   0/2 (  0%)]  Loss: 6.829 (6.83)  Time: 1.590s, 1287.78/s  (1.590s, 1287.78/s)  LR: 7.937e-01  Data: 1.083 (1.083)
Train: 17 [   1/2 (100%)]  Loss: 6.900 (6.86)  Time: 0.513s, 3994.92/s  (1.051s, 1947.71/s)  LR: 7.937e-01  Data: 0.000 (0.541)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.166 (1.166)  Loss:  6.8329 (6.8329)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.6113 ( 1.6113)
Test: [   2/2]  Time: 0.022 (0.455)  Loss:  6.2889 (6.8440)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 0.9690)
Train: 18 [   0/2 (  0%)]  Loss: 6.832 (6.83)  Time: 1.566s, 1307.46/s  (1.566s, 1307.46/s)  LR: 7.929e-01  Data: 1.057 (1.057)
Train: 18 [   1/2 (100%)]  Loss: 6.896 (6.86)  Time: 0.513s, 3988.41/s  (1.040s, 1969.34/s)  LR: 7.929e-01  Data: 0.001 (0.529)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.173 (1.173)  Loss:  6.8454 (6.8454)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.023 (0.458)  Loss:  6.4810 (6.8491)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9205)
Train: 19 [   0/2 (  0%)]  Loss: 6.841 (6.84)  Time: 1.576s, 1299.84/s  (1.576s, 1299.84/s)  LR: 7.921e-01  Data: 1.067 (1.067)
Train: 19 [   1/2 (100%)]  Loss: 6.881 (6.86)  Time: 0.514s, 3980.76/s  (1.045s, 1959.76/s)  LR: 7.921e-01  Data: 0.001 (0.534)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.172 (1.172)  Loss:  6.8331 (6.8331)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  0.6836 ( 0.6836)
Test: [   2/2]  Time: 0.021 (0.457)  Loss:  6.5111 (6.8299)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.0901)
Train: 20 [   0/2 (  0%)]  Loss: 6.838 (6.84)  Time: 1.562s, 1311.05/s  (1.562s, 1311.05/s)  LR: 7.913e-01  Data: 1.055 (1.055)
Train: 20 [   1/2 (100%)]  Loss: 6.871 (6.85)  Time: 0.512s, 3996.60/s  (1.037s, 1974.41/s)  LR: 7.913e-01  Data: 0.000 (0.527)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.171 (1.171)  Loss:  6.8176 (6.8176)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.0742 ( 1.0742)
Test: [   2/2]  Time: 0.021 (0.457)  Loss:  6.4670 (6.8187)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.1143)
Train: 21 [   0/2 (  0%)]  Loss: 6.822 (6.82)  Time: 1.546s, 1324.71/s  (1.546s, 1324.71/s)  LR: 7.904e-01  Data: 1.039 (1.039)
Train: 21 [   1/2 (100%)]  Loss: 6.867 (6.84)  Time: 0.513s, 3994.75/s  (1.029s, 1989.64/s)  LR: 7.904e-01  Data: 0.000 (0.520)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.170 (1.170)  Loss:  6.8129 (6.8129)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.4395 ( 0.4395)
Test: [   2/2]  Time: 0.025 (0.458)  Loss:  6.4233 (6.8155)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 0.9932)
Train: 22 [   0/2 (  0%)]  Loss: 6.821 (6.82)  Time: 1.580s, 1296.41/s  (1.580s, 1296.41/s)  LR: 7.894e-01  Data: 1.072 (1.072)
Train: 22 [   1/2 (100%)]  Loss: 6.869 (6.85)  Time: 0.513s, 3993.41/s  (1.046s, 1957.38/s)  LR: 7.894e-01  Data: 0.000 (0.536)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.168 (1.168)  Loss:  6.8152 (6.8152)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0977 ( 0.0977)
Test: [   2/2]  Time: 0.022 (0.456)  Loss:  6.3782 (6.8130)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  0.0000 ( 0.9448)
Train: 23 [   0/2 (  0%)]  Loss: 6.825 (6.82)  Time: 1.566s, 1308.13/s  (1.566s, 1308.13/s)  LR: 7.885e-01  Data: 1.058 (1.058)
Train: 23 [   1/2 (100%)]  Loss: 6.860 (6.84)  Time: 0.514s, 3984.86/s  (1.040s, 1969.67/s)  LR: 7.885e-01  Data: 0.000 (0.529)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.167 (1.167)  Loss:  6.8125 (6.8125)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  0.4395 ( 0.4395)
Test: [   2/2]  Time: 0.021 (0.456)  Loss:  6.4359 (6.8068)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.0174)
Train: 24 [   0/2 (  0%)]  Loss: 6.822 (6.82)  Time: 1.565s, 1308.64/s  (1.565s, 1308.64/s)  LR: 7.874e-01  Data: 1.057 (1.057)
Train: 24 [   1/2 (100%)]  Loss: 6.859 (6.84)  Time: 0.512s, 3998.65/s  (1.039s, 1971.92/s)  LR: 7.874e-01  Data: 0.000 (0.529)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.153 (1.153)  Loss:  6.8049 (6.8049)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.9766 ( 0.9766)
Test: [   2/2]  Time: 0.021 (0.451)  Loss:  6.5040 (6.8020)  Acc@1:  0.0000 ( 0.2665)  Acc@5: 15.6250 ( 1.1870)
Train: 25 [   0/2 (  0%)]  Loss: 6.825 (6.82)  Time: 1.571s, 1303.92/s  (1.571s, 1303.92/s)  LR: 7.864e-01  Data: 1.064 (1.064)
Train: 25 [   1/2 (100%)]  Loss: 6.848 (6.84)  Time: 0.514s, 3986.53/s  (1.042s, 1965.10/s)  LR: 7.864e-01  Data: 0.000 (0.532)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.159 (1.159)  Loss:  6.7987 (6.7987)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.021 (0.453)  Loss:  6.4968 (6.7998)  Acc@1:  0.0000 ( 0.2665)  Acc@5: 18.7500 ( 1.1628)
Train: 26 [   0/2 (  0%)]  Loss: 6.815 (6.81)  Time: 1.608s, 1273.90/s  (1.608s, 1273.90/s)  LR: 7.853e-01  Data: 1.099 (1.099)
Train: 26 [   1/2 (100%)]  Loss: 6.859 (6.84)  Time: 0.512s, 3999.12/s  (1.060s, 1932.29/s)  LR: 7.853e-01  Data: 0.000 (0.550)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.164 (1.164)  Loss:  6.7979 (6.7979)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.2695 ( 1.2695)
Test: [   2/2]  Time: 0.022 (0.455)  Loss:  6.4839 (6.7987)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1143)
Train: 27 [   0/2 (  0%)]  Loss: 6.816 (6.82)  Time: 1.613s, 1269.88/s  (1.613s, 1269.88/s)  LR: 7.841e-01  Data: 1.105 (1.105)
Train: 27 [   1/2 (100%)]  Loss: 6.857 (6.84)  Time: 0.512s, 3999.03/s  (1.062s, 1927.64/s)  LR: 7.841e-01  Data: 0.001 (0.553)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.164 (1.164)  Loss:  6.7906 (6.7906)  Acc@1:  0.4883 ( 0.4883)  Acc@5:  1.3672 ( 1.3672)
Test: [   2/2]  Time: 0.021 (0.455)  Loss:  6.4690 (6.7947)  Acc@1:  0.0000 ( 0.3149)  Acc@5:  0.0000 ( 1.1870)
Train: 28 [   0/2 (  0%)]  Loss: 6.817 (6.82)  Time: 1.612s, 1270.70/s  (1.612s, 1270.70/s)  LR: 7.829e-01  Data: 1.104 (1.104)
Train: 28 [   1/2 (100%)]  Loss: 6.850 (6.83)  Time: 0.513s, 3995.67/s  (1.062s, 1928.20/s)  LR: 7.829e-01  Data: 0.000 (0.552)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.181 (1.181)  Loss:  6.7898 (6.7898)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  0.9766 ( 0.9766)
Test: [   2/2]  Time: 0.020 (0.460)  Loss:  6.4019 (6.7924)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.0901)
Train: 29 [   0/2 (  0%)]  Loss: 6.821 (6.82)  Time: 1.630s, 1256.52/s  (1.630s, 1256.52/s)  LR: 7.817e-01  Data: 1.122 (1.122)
Train: 29 [   1/2 (100%)]  Loss: 6.838 (6.83)  Time: 0.511s, 4004.27/s  (1.071s, 1912.81/s)  LR: 7.817e-01  Data: 0.000 (0.561)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.163 (1.163)  Loss:  6.7895 (6.7895)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.3184 ( 1.3184)
Test: [   2/2]  Time: 0.023 (0.454)  Loss:  6.3658 (6.7909)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  3.1250 ( 1.1870)
Train: 30 [   0/2 (  0%)]  Loss: 6.817 (6.82)  Time: 1.624s, 1260.98/s  (1.624s, 1260.98/s)  LR: 7.804e-01  Data: 1.116 (1.116)
Train: 30 [   1/2 (100%)]  Loss: 6.840 (6.83)  Time: 0.512s, 4000.55/s  (1.068s, 1917.55/s)  LR: 7.804e-01  Data: 0.000 (0.558)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.166 (1.166)  Loss:  6.7944 (6.7944)  Acc@1:  0.4883 ( 0.4883)  Acc@5:  1.4160 ( 1.4160)
Test: [   2/2]  Time: 0.021 (0.455)  Loss:  6.3046 (6.7915)  Acc@1:  0.0000 ( 0.2422)  Acc@5: 21.8750 ( 1.1870)
Train: 31 [   0/2 (  0%)]  Loss: 6.824 (6.82)  Time: 1.616s, 1267.55/s  (1.616s, 1267.55/s)  LR: 7.791e-01  Data: 1.108 (1.108)
Train: 31 [   1/2 (100%)]  Loss: 6.842 (6.83)  Time: 0.512s, 4000.74/s  (1.064s, 1925.15/s)  LR: 7.791e-01  Data: 0.000 (0.554)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.162 (1.162)  Loss:  6.7961 (6.7961)  Acc@1:  0.4883 ( 0.4883)  Acc@5:  1.2695 ( 1.2695)
Test: [   2/2]  Time: 0.021 (0.454)  Loss:  6.2848 (6.7908)  Acc@1:  0.0000 ( 0.2422)  Acc@5: 28.1250 ( 1.3081)
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158556 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158557 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158558 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158559 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158560 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158561 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158562 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 158563 closing signal SIGTERM
Traceback (most recent call last):
  File "/milabench/envs/venv/torch/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/milabench/envs/venv/torch/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 158546 got signal: 15
