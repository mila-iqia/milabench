Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 6
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Added key: store_based_barrier_key:1 to store for rank: 5
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 6, total 8, device cuda:6.
Training in distributed mode with multiple processes, 1 device per process.Process 7, total 8, device cuda:7.
Training in distributed mode with multiple processes, 1 device per process.Process 5, total 8, device cuda:5.
Training in distributed mode with multiple processes, 1 device per process.Process 3, total 8, device cuda:3.
Training in distributed mode with multiple processes, 1 device per process.Process 2, total 8, device cuda:2.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 8, device cuda:1.
Training in distributed mode with multiple processes, 1 device per process.Process 4, total 8, device cuda:4.
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 8, device cuda:0.
Model davit_large created, param count:196811752
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.04) calculated from base learning rate (0.01) and global batch size (1024) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 300. LR stepped per epoch.
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Train: 0 [   0/4 (  0%)]  Loss: 7.221 (7.22)  Time: 10.879s,   94.13/s  (10.879s,   94.13/s)  LR: 1.000e-05  Data: 1.390 (1.390)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [   3/4 (100%)]  Loss: 7.247 (7.23)  Time: 0.419s, 2445.10/s  (3.387s,  302.34/s)  LR: 1.000e-05  Data: 0.000 (0.359)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 1.289 (1.289)  Loss:  7.2865 (7.2865)  Acc@1:  0.2930 ( 0.2930)  Acc@5:  0.4883 ( 0.4883)
Test: [   4/4]  Time: 1.687 (0.688)  Loss:  7.0634 (7.2443)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  0.0000 ( 0.5087)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large-multi.0/20230510-141208-davit_large-224/checkpoint-0.pth.tar', 0.09689922480620156)

Train: 1 [   0/4 (  0%)]  Loss: 7.233 (7.23)  Time: 1.152s,  888.70/s  (1.152s,  888.70/s)  LR: 8.008e-03  Data: 0.568 (0.568)
Train: 1 [   3/4 (100%)]  Loss: 7.003 (7.12)  Time: 0.428s, 2394.56/s  (0.612s, 1673.63/s)  LR: 8.008e-03  Data: 0.000 (0.148)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.755 (0.755)  Loss:  6.9699 (6.9699)  Acc@1:  0.1953 ( 0.1953)  Acc@5:  0.8789 ( 0.8789)
Test: [   4/4]  Time: 0.023 (0.247)  Loss:  6.7330 (6.9359)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  0.0000 ( 0.6056)
Train: 2 [   0/4 (  0%)]  Loss: 6.937 (6.94)  Time: 0.981s, 1044.28/s  (0.981s, 1044.28/s)  LR: 1.601e-02  Data: 0.543 (0.543)
Train: 2 [   3/4 (100%)]  Loss: 6.958 (6.95)  Time: 0.429s, 2387.70/s  (0.569s, 1799.10/s)  LR: 1.601e-02  Data: 0.001 (0.142)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.683 (0.683)  Loss:  6.8535 (6.8535)  Acc@1:  0.1953 ( 0.1953)  Acc@5:  1.3672 ( 1.3672)
Test: [   4/4]  Time: 0.023 (0.235)  Loss:  6.4871 (6.8475)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.4050)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large-multi.0/20230510-141208-davit_large-224/checkpoint-2.pth.tar', 0.1937984496124031)

Train: 3 [   0/4 (  0%)]  Loss: 6.871 (6.87)  Time: 0.988s, 1036.95/s  (0.988s, 1036.95/s)  LR: 2.400e-02  Data: 0.533 (0.533)
Train: 3 [   3/4 (100%)]  Loss: 6.967 (6.92)  Time: 0.428s, 2393.83/s  (0.571s, 1791.88/s)  LR: 2.400e-02  Data: 0.000 (0.140)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.676 (0.676)  Loss:  6.7997 (6.7997)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.3672 ( 1.3672)
Test: [   4/4]  Time: 0.023 (0.232)  Loss:  6.2585 (6.8252)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1143)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large-multi.0/20230510-141208-davit_large-224/checkpoint-3.pth.tar', 0.26647286821705424)

Train: 4 [   0/4 (  0%)]  Loss: 6.841 (6.84)  Time: 0.969s, 1057.25/s  (0.969s, 1057.25/s)  LR: 3.200e-02  Data: 0.523 (0.523)
Train: 4 [   3/4 (100%)]  Loss: 6.984 (6.91)  Time: 0.424s, 2415.79/s  (0.565s, 1811.61/s)  LR: 3.200e-02  Data: 0.000 (0.137)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.672 (0.672)  Loss:  6.7865 (6.7865)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.6836 ( 0.6836)
Test: [   4/4]  Time: 0.023 (0.231)  Loss:  6.3134 (6.8127)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.9690)
Train: 5 [   0/4 (  0%)]  Loss: 6.847 (6.85)  Time: 0.952s, 1076.12/s  (0.952s, 1076.12/s)  LR: 3.997e-02  Data: 0.518 (0.518)
Train: 5 [   3/4 (100%)]  Loss: 6.946 (6.90)  Time: 0.426s, 2405.13/s  (0.563s, 1820.13/s)  LR: 3.997e-02  Data: 0.001 (0.137)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.666 (0.666)  Loss:  6.7953 (6.7953)  Acc@1:  1.0742 ( 1.0742)  Acc@5:  1.5625 ( 1.5625)
Test: [   4/4]  Time: 0.024 (0.230)  Loss:  6.4709 (6.7968)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1628)
Train: 6 [   0/4 (  0%)]  Loss: 6.824 (6.82)  Time: 0.973s, 1052.66/s  (0.973s, 1052.66/s)  LR: 3.996e-02  Data: 0.520 (0.520)
Train: 6 [   3/4 (100%)]  Loss: 6.955 (6.89)  Time: 0.428s, 2390.03/s  (0.567s, 1806.78/s)  LR: 3.996e-02  Data: 0.001 (0.137)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.673 (0.673)  Loss:  6.7895 (6.7895)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.9766 ( 0.9766)
Test: [   4/4]  Time: 0.024 (0.232)  Loss:  6.4542 (6.7912)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.1870)
Train: 7 [   0/4 (  0%)]  Loss: 6.811 (6.81)  Time: 0.970s, 1056.06/s  (0.970s, 1056.06/s)  LR: 3.995e-02  Data: 0.518 (0.518)
Train: 7 [   3/4 (100%)]  Loss: 6.926 (6.87)  Time: 0.424s, 2413.72/s  (0.565s, 1811.97/s)  LR: 3.995e-02  Data: 0.001 (0.136)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.672 (0.672)  Loss:  6.7706 (6.7706)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [   4/4]  Time: 0.023 (0.231)  Loss:  6.4055 (6.7885)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1143)
Train: 8 [   0/4 (  0%)]  Loss: 6.808 (6.81)  Time: 0.950s, 1077.70/s  (0.950s, 1077.70/s)  LR: 3.993e-02  Data: 0.516 (0.516)
Train: 8 [   3/4 (100%)]  Loss: 6.940 (6.87)  Time: 0.428s, 2391.38/s  (0.564s, 1816.97/s)  LR: 3.993e-02  Data: 0.000 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.665 (0.665)  Loss:  6.7714 (6.7714)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [   4/4]  Time: 0.025 (0.231)  Loss:  6.3386 (6.7864)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.1870)
Train: 9 [   0/4 (  0%)]  Loss: 6.825 (6.82)  Time: 0.970s, 1056.19/s  (0.970s, 1056.19/s)  LR: 3.991e-02  Data: 0.517 (0.517)
Train: 9 [   3/4 (100%)]  Loss: 6.923 (6.87)  Time: 0.423s, 2420.52/s  (0.566s, 1807.68/s)  LR: 3.991e-02  Data: 0.000 (0.136)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.670 (0.670)  Loss:  6.7854 (6.7854)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.023 (0.231)  Loss:  6.5016 (6.7865)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2355)
Train: 10 [   0/4 (  0%)]  Loss: 6.823 (6.82)  Time: 0.971s, 1054.27/s  (0.971s, 1054.27/s)  LR: 3.989e-02  Data: 0.514 (0.514)
Train: 10 [   3/4 (100%)]  Loss: 6.930 (6.88)  Time: 0.422s, 2427.93/s  (0.566s, 1809.91/s)  LR: 3.989e-02  Data: 0.000 (0.136)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.690 (0.690)  Loss:  6.7778 (6.7778)  Acc@1:  0.6836 ( 0.6836)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.022 (0.237)  Loss:  6.5581 (6.7850)  Acc@1:  0.0000 ( 0.3634)  Acc@5:  0.0000 ( 1.4050)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large-multi.0/20230510-141208-davit_large-224/checkpoint-10.pth.tar', 0.3633720930232558)

Train: 11 [   0/4 (  0%)]  Loss: 6.806 (6.81)  Time: 0.950s, 1078.45/s  (0.950s, 1078.45/s)  LR: 3.987e-02  Data: 0.515 (0.515)
Train: 11 [   3/4 (100%)]  Loss: 6.922 (6.86)  Time: 0.422s, 2425.57/s  (0.561s, 1824.19/s)  LR: 3.987e-02  Data: 0.000 (0.137)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.686 (0.686)  Loss:  6.7690 (6.7690)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  2.3438 ( 2.3438)
Test: [   4/4]  Time: 0.024 (0.235)  Loss:  6.3850 (6.7844)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.3081)
Train: 12 [   0/4 (  0%)]  Loss: 6.840 (6.84)  Time: 0.979s, 1045.49/s  (0.979s, 1045.49/s)  LR: 3.984e-02  Data: 0.523 (0.523)
Train: 12 [   3/4 (100%)]  Loss: 6.905 (6.87)  Time: 0.423s, 2423.01/s  (0.568s, 1801.27/s)  LR: 3.984e-02  Data: 0.001 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.673 (0.673)  Loss:  6.7703 (6.7703)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [   4/4]  Time: 0.023 (0.232)  Loss:  6.3025 (6.7840)  Acc@1:  0.0000 ( 0.2665)  Acc@5: 28.1250 ( 1.2597)
Train: 13 [   0/4 (  0%)]  Loss: 6.826 (6.83)  Time: 0.970s, 1056.08/s  (0.970s, 1056.08/s)  LR: 3.981e-02  Data: 0.515 (0.515)
Train: 13 [   3/4 (100%)]  Loss: 6.900 (6.86)  Time: 0.423s, 2422.76/s  (0.565s, 1811.72/s)  LR: 3.981e-02  Data: 0.000 (0.136)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.675 (0.675)  Loss:  6.7716 (6.7716)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.023 (0.233)  Loss:  6.3871 (6.7838)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.2355)
Train: 14 [   0/4 (  0%)]  Loss: 6.816 (6.82)  Time: 0.961s, 1065.03/s  (0.961s, 1065.03/s)  LR: 3.979e-02  Data: 0.524 (0.524)
Train: 14 [   3/4 (100%)]  Loss: 6.924 (6.87)  Time: 0.422s, 2424.66/s  (0.563s, 1819.62/s)  LR: 3.979e-02  Data: 0.000 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.698 (0.698)  Loss:  6.7702 (6.7702)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  2.0508 ( 2.0508)
Test: [   4/4]  Time: 0.023 (0.237)  Loss:  6.3754 (6.7836)  Acc@1:  0.0000 ( 0.3634)  Acc@5:  0.0000 ( 1.2597)
Train: 15 [   0/4 (  0%)]  Loss: 6.809 (6.81)  Time: 0.983s, 1041.98/s  (0.983s, 1041.98/s)  LR: 3.975e-02  Data: 0.526 (0.526)
Train: 15 [   3/4 (100%)]  Loss: 6.920 (6.86)  Time: 0.421s, 2429.68/s  (0.569s, 1799.37/s)  LR: 3.975e-02  Data: 0.000 (0.139)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.685 (0.685)  Loss:  6.7690 (6.7690)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   4/4]  Time: 0.023 (0.238)  Loss:  6.4504 (6.7841)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.1870)
Train: 16 [   0/4 (  0%)]  Loss: 6.811 (6.81)  Time: 0.954s, 1073.40/s  (0.954s, 1073.40/s)  LR: 3.972e-02  Data: 0.511 (0.511)
Train: 16 [   3/4 (100%)]  Loss: 6.915 (6.86)  Time: 0.422s, 2427.10/s  (0.564s, 1815.51/s)  LR: 3.972e-02  Data: 0.000 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.670 (0.670)  Loss:  6.7658 (6.7658)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   4/4]  Time: 0.025 (0.234)  Loss:  6.4825 (6.7839)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2355)
Train: 17 [   0/4 (  0%)]  Loss: 6.809 (6.81)  Time: 0.955s, 1072.32/s  (0.955s, 1072.32/s)  LR: 3.968e-02  Data: 0.519 (0.519)
Train: 17 [   3/4 (100%)]  Loss: 6.930 (6.87)  Time: 0.424s, 2414.79/s  (0.560s, 1828.92/s)  LR: 3.968e-02  Data: 0.001 (0.135)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.655 (0.655)  Loss:  6.7788 (6.7788)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.023 (0.227)  Loss:  6.3754 (6.7837)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.3324)
Train: 18 [   0/4 (  0%)]  Loss: 6.811 (6.81)  Time: 0.963s, 1062.88/s  (0.963s, 1062.88/s)  LR: 3.965e-02  Data: 0.524 (0.524)
Train: 18 [   3/4 (100%)]  Loss: 6.926 (6.87)  Time: 0.420s, 2438.63/s  (0.561s, 1826.41/s)  LR: 3.965e-02  Data: 0.000 (0.136)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.662 (0.662)  Loss:  6.7651 (6.7651)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.024 (0.228)  Loss:  6.4139 (6.7834)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2597)
Train: 19 [   0/4 (  0%)]  Loss: 6.812 (6.81)  Time: 0.951s, 1076.55/s  (0.951s, 1076.55/s)  LR: 3.961e-02  Data: 0.514 (0.514)
Train: 19 [   3/4 (100%)]  Loss: 6.896 (6.85)  Time: 0.421s, 2430.60/s  (0.557s, 1837.58/s)  LR: 3.961e-02  Data: 0.001 (0.133)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.650 (0.650)  Loss:  6.7771 (6.7771)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.6836 ( 0.6836)
Test: [   4/4]  Time: 0.022 (0.225)  Loss:  6.4200 (6.7841)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2839)
Train: 20 [   0/4 (  0%)]  Loss: 6.822 (6.82)  Time: 0.961s, 1065.81/s  (0.961s, 1065.81/s)  LR: 3.956e-02  Data: 0.513 (0.513)
Train: 20 [   3/4 (100%)]  Loss: 6.896 (6.86)  Time: 0.421s, 2431.37/s  (0.561s, 1825.61/s)  LR: 3.956e-02  Data: 0.001 (0.133)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.666 (0.666)  Loss:  6.7738 (6.7738)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.8555 ( 1.8555)
Test: [   4/4]  Time: 0.025 (0.229)  Loss:  6.4243 (6.7841)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1870)
Train: 21 [   0/4 (  0%)]  Loss: 6.811 (6.81)  Time: 0.949s, 1079.44/s  (0.949s, 1079.44/s)  LR: 3.952e-02  Data: 0.513 (0.513)
Train: 21 [   3/4 (100%)]  Loss: 6.920 (6.87)  Time: 0.425s, 2412.03/s  (0.559s, 1832.84/s)  LR: 3.952e-02  Data: 0.001 (0.133)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.658 (0.658)  Loss:  6.7662 (6.7662)  Acc@1:  1.0742 ( 1.0742)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.023 (0.227)  Loss:  6.4001 (6.7838)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2355)
Train: 22 [   0/4 (  0%)]  Loss: 6.817 (6.82)  Time: 0.976s, 1048.87/s  (0.976s, 1048.87/s)  LR: 3.947e-02  Data: 0.532 (0.532)
Train: 22 [   3/4 (100%)]  Loss: 6.890 (6.85)  Time: 0.420s, 2436.58/s  (0.564s, 1816.18/s)  LR: 3.947e-02  Data: 0.000 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.656 (0.656)  Loss:  6.7563 (6.7563)  Acc@1:  1.0742 ( 1.0742)  Acc@5:  1.3672 ( 1.3672)
Test: [   4/4]  Time: 0.023 (0.227)  Loss:  6.3876 (6.7833)  Acc@1:  0.0000 ( 0.2665)  Acc@5: 21.8750 ( 1.3081)
Train: 23 [   0/4 (  0%)]  Loss: 6.810 (6.81)  Time: 0.960s, 1066.42/s  (0.960s, 1066.42/s)  LR: 3.942e-02  Data: 0.513 (0.513)
Train: 23 [   3/4 (100%)]  Loss: 6.905 (6.86)  Time: 0.423s, 2420.60/s  (0.561s, 1823.73/s)  LR: 3.942e-02  Data: 0.001 (0.133)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.657 (0.657)  Loss:  6.7675 (6.7675)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.2695 ( 1.2695)
Test: [   4/4]  Time: 0.022 (0.227)  Loss:  6.4658 (6.7833)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2597)
Train: 24 [   0/4 (  0%)]  Loss: 6.792 (6.79)  Time: 0.989s, 1035.84/s  (0.989s, 1035.84/s)  LR: 3.937e-02  Data: 0.536 (0.536)
Train: 24 [   3/4 (100%)]  Loss: 6.914 (6.85)  Time: 0.420s, 2439.97/s  (0.566s, 1807.62/s)  LR: 3.937e-02  Data: 0.000 (0.138)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.656 (0.656)  Loss:  6.7706 (6.7706)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [   4/4]  Time: 0.023 (0.227)  Loss:  6.3823 (6.7833)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2355)
Train: 25 [   0/4 (  0%)]  Loss: 6.798 (6.80)  Time: 0.962s, 1064.63/s  (0.962s, 1064.63/s)  LR: 3.932e-02  Data: 0.514 (0.514)
Train: 25 [   3/4 (100%)]  Loss: 6.897 (6.85)  Time: 0.424s, 2414.12/s  (0.562s, 1822.85/s)  LR: 3.932e-02  Data: 0.001 (0.134)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.661 (0.661)  Loss:  6.7766 (6.7766)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.024 (0.228)  Loss:  6.3209 (6.7835)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.2355)
Train: 26 [   0/4 (  0%)]  Loss: 6.814 (6.81)  Time: 0.947s, 1081.05/s  (0.947s, 1081.05/s)  LR: 3.926e-02  Data: 0.515 (0.515)
Train: 26 [   3/4 (100%)]  Loss: 6.909 (6.86)  Time: 0.424s, 2413.64/s  (0.558s, 1835.71/s)  LR: 3.926e-02  Data: 0.001 (0.134)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.654 (0.654)  Loss:  6.7853 (6.7853)  Acc@1:  1.0742 ( 1.0742)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.022 (0.226)  Loss:  6.4700 (6.7833)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2597)
Train: 27 [   0/4 (  0%)]  Loss: 6.819 (6.82)  Time: 0.947s, 1081.82/s  (0.947s, 1081.82/s)  LR: 3.921e-02  Data: 0.514 (0.514)
Train: 27 [   3/4 (100%)]  Loss: 6.899 (6.86)  Time: 0.424s, 2413.63/s  (0.558s, 1834.57/s)  LR: 3.921e-02  Data: 0.001 (0.134)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.650 (0.650)  Loss:  6.7790 (6.7790)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   4/4]  Time: 0.024 (0.226)  Loss:  6.4633 (6.7830)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2112)
Train: 28 [   0/4 (  0%)]  Loss: 6.812 (6.81)  Time: 0.955s, 1072.77/s  (0.955s, 1072.77/s)  LR: 3.915e-02  Data: 0.502 (0.502)
Train: 28 [   3/4 (100%)]  Loss: 6.906 (6.86)  Time: 0.424s, 2414.72/s  (0.560s, 1829.36/s)  LR: 3.915e-02  Data: 0.000 (0.130)
Distributing BatchNorm running means and vars
Test: [   0/4]  Time: 0.649 (0.649)  Loss:  6.7633 (6.7633)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.0742 ( 1.0742)
Test: [   4/4]  Time: 0.022 (0.225)  Loss:  6.4064 (6.7832)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.2839)
Train: 29 [   0/4 (  0%)]  Loss: 6.814 (6.81)  Time: 0.946s, 1082.47/s  (0.946s, 1082.47/s)  LR: 3.908e-02  Data: 0.504 (0.504)
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716571 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716573 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716574 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716576 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716577 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716578 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716579 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1716580 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/mila/d/delaunap/scratch/milabench/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1716458 got signal: 15
