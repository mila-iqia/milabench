Training with a single process on 1 device (cuda:0).
Model focalnet_base_lrf created, param count:88749768
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
	crop_mode: center
Learning rate (0.05) calculated from base learning rate (0.1) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.004 (7.00)  Time: 17.612s,    7.27/s  (17.612s,    7.27/s)  LR: 1.000e-05  Data: 0.778 (0.778)
Train: 0 [  31/32 (100%)]  Loss: 7.005 (7.00)  Time: 0.313s,  408.34/s  (0.902s,  141.83/s)  LR: 1.000e-05  Data: 0.000 (0.031)
Test: [   0/32]  Time: 0.867 (0.867)  Loss:  6.9616 (6.9616)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 2.011 (0.193)  Loss:  6.8639 (6.9459)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D0/20230510-141508-focalnet_base_lrf-224/checkpoint-0.pth.tar', 0.14534883720930233)

Train: 1 [   0/32 (  0%)]  Loss: 7.021 (7.02)  Time: 1.168s,  109.58/s  (1.168s,  109.58/s)  LR: 1.001e-02  Data: 0.520 (0.520)
Train: 1 [  31/32 (100%)]  Loss: 7.108 (7.05)  Time: 0.313s,  409.23/s  (0.346s,  369.61/s)  LR: 1.001e-02  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.574 (0.574)  Loss:  6.8922 (6.8922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.9391 (6.9699)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D0/20230510-141508-focalnet_base_lrf-224/checkpoint-1.pth.tar', 0.26647286821705424)

Train: 2 [   0/32 (  0%)]  Loss: 6.996 (7.00)  Time: 0.836s,  153.09/s  (0.836s,  153.09/s)  LR: 2.001e-02  Data: 0.524 (0.524)
Train: 2 [  31/32 (100%)]  Loss: 7.261 (7.13)  Time: 0.313s,  409.43/s  (0.335s,  382.22/s)  LR: 2.001e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.565 (0.565)  Loss:  6.9301 (6.9301)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.7455 (7.1150)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  0.0000 ( 0.8479)
Train: 3 [   0/32 (  0%)]  Loss: 7.089 (7.09)  Time: 0.833s,  153.60/s  (0.833s,  153.60/s)  LR: 3.000e-02  Data: 0.520 (0.520)
Train: 3 [  31/32 (100%)]  Loss: 7.240 (7.23)  Time: 0.312s,  410.16/s  (0.335s,  382.22/s)  LR: 3.000e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.561 (0.561)  Loss:  7.1711 (7.1711)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.031 (0.123)  Loss:  6.2448 (7.1748)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 25.0000 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 7.216 (7.22)  Time: 0.858s,  149.23/s  (0.858s,  149.23/s)  LR: 4.000e-02  Data: 0.544 (0.544)
Train: 4 [  31/32 (100%)]  Loss: 7.404 (7.33)  Time: 0.312s,  410.91/s  (0.336s,  380.43/s)  LR: 4.000e-02  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.565 (0.565)  Loss:  6.9697 (6.9697)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.8207 (7.2576)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  3.1250 ( 0.7025)
Train: 5 [   0/32 (  0%)]  Loss: 7.211 (7.21)  Time: 0.866s,  147.73/s  (0.866s,  147.73/s)  LR: 4.997e-02  Data: 0.552 (0.552)
Train: 5 [  31/32 (100%)]  Loss: 7.660 (7.46)  Time: 0.313s,  409.55/s  (0.336s,  381.13/s)  LR: 4.997e-02  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.569 (0.569)  Loss:  6.9418 (6.9418)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.7286 (6.9690)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.9448)
Train: 6 [   0/32 (  0%)]  Loss: 7.380 (7.38)  Time: 0.859s,  149.02/s  (0.859s,  149.02/s)  LR: 4.995e-02  Data: 0.545 (0.545)
