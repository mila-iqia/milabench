Training with a single process on 1 device (cuda:0).
Model davit_large created, param count:196811752
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.005) calculated from base learning rate (0.01) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.224 (7.22)  Time: 7.955s,   16.09/s  (7.955s,   16.09/s)  LR: 1.000e-05  Data: 0.783 (0.783)
Train: 0 [  31/32 (100%)]  Loss: 7.311 (7.24)  Time: 0.414s,  309.00/s  (0.680s,  188.26/s)  LR: 1.000e-05  Data: 0.000 (0.031)
Test: [   0/32]  Time: 0.882 (0.882)  Loss:  7.1173 (7.1173)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.908 (0.196)  Loss:  7.0504 (7.2335)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  0.0000 ( 0.5329)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D4/20230510-140901-davit_large-224/checkpoint-0.pth.tar', 0.09689922480620156)

Train: 1 [   0/32 (  0%)]  Loss: 7.414 (7.41)  Time: 1.302s,   98.31/s  (1.302s,   98.31/s)  LR: 1.008e-03  Data: 0.528 (0.528)
Train: 1 [  31/32 (100%)]  Loss: 6.997 (7.05)  Time: 0.416s,  307.80/s  (0.448s,  285.97/s)  LR: 1.008e-03  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.602 (0.602)  Loss:  6.8691 (6.8691)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.040 (0.162)  Loss:  6.7074 (6.8682)  Acc@1:  0.0000 ( 0.2907)  Acc@5:  3.1250 ( 1.2839)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D4/20230510-140901-davit_large-224/checkpoint-1.pth.tar', 0.29069767441860467)

Train: 2 [   0/32 (  0%)]  Loss: 6.886 (6.89)  Time: 1.037s,  123.47/s  (1.037s,  123.47/s)  LR: 2.006e-03  Data: 0.602 (0.602)
Train: 2 [  31/32 (100%)]  Loss: 6.948 (6.95)  Time: 0.414s,  309.39/s  (0.440s,  290.97/s)  LR: 2.006e-03  Data: 0.000 (0.025)
Test: [   0/32]  Time: 0.612 (0.612)  Loss:  6.7543 (6.7543)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  2.3438 ( 2.3438)
Test: [  32/32]  Time: 0.041 (0.162)  Loss:  6.4865 (6.8226)  Acc@1:  3.1250 ( 0.2422)  Acc@5:  6.2500 ( 1.0417)
Train: 3 [   0/32 (  0%)]  Loss: 6.860 (6.86)  Time: 0.965s,  132.69/s  (0.965s,  132.69/s)  LR: 3.004e-03  Data: 0.533 (0.533)
Train: 3 [  31/32 (100%)]  Loss: 7.002 (6.96)  Time: 0.414s,  309.20/s  (0.437s,  292.94/s)  LR: 3.004e-03  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.619 (0.619)  Loss:  6.8251 (6.8251)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.041 (0.162)  Loss:  6.3623 (6.8163)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  6.2500 ( 1.0417)
Train: 4 [   0/32 (  0%)]  Loss: 6.859 (6.86)  Time: 0.976s,  131.19/s  (0.976s,  131.19/s)  LR: 4.002e-03  Data: 0.539 (0.539)
