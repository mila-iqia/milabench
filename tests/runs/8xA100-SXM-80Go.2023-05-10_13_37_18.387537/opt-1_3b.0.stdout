[05/10/23 14:18:22] INFO     [3/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 3                                                     
                             Local process index: 3                                               
                             Device: cuda:3                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [4/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 4                                                     
                             Local process index: 4                                               
                             Device: cuda:4                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [1/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 1                                                     
                             Local process index: 1                                               
                             Device: cuda:1                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [6/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 6                                                     
                             Local process index: 6                                               
                             Device: cuda:6                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [5/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 5                                                     
                             Local process index: 5                                               
                             Device: cuda:5                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [2/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 2                                                     
                             Local process index: 2                                               
                             Device: cuda:2                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:22] INFO     [7/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 7                                                     
                             Local process index: 7                                               
                             Device: cuda:7                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:23] INFO     [0/8] __main__ - Distributed environment: MULTI_GPU     logging.py:47
                             Backend: nccl                                                        
                             Num processes: 8                                                     
                             Process index: 0                                                     
                             Local process index: 0                                               
                             Device: cuda:0                                                       
                                                                                                  
                             Mixed precision type: fp16                                           
                                                                                                  
[05/10/23 14:18:30] WARNING  [0/8] datasets.builder - Found cached dataset wikitext builder.py:817
                             (/Tmp/slurm.3188069.0/milabench_dev/results/cache/hugg               
                             ingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db               
                             52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646               
                             a126)                                                                
                    WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-1b4e61c4f1ef8b68                      
                             _*_of_00008.arrow                                                    
                    WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-329331b01bd0468e                      
                             _*_of_00008.arrow                                                    
                    WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-c60b625823abbe78                      
                             _*_of_00008.arrow                                                    
                    WARNING  [0/8] __main__ - The tokenizer picked seems to have a   logging.py:47
                             very large `model_max_length`                                        
                             (1000000000000000019884624838656). Picking 1024                      
                             instead. You can change that default value by passing                
                             --block_size xxx.                                                    
                    WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-87a845baeb5292f0                      
                             _*_of_00008.arrow                                                    
                    WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-23b43495f74434f7                      
                             _*_of_00008.arrow                                                    
[05/10/23 14:18:31] WARNING  [0/8] datasets.arrow_dataset - Loading cached   arrow_dataset.py:3110
                             processed dataset at                                                 
                             /Tmp/slurm.3188069.0/milabench_dev/results/cach                      
                             e/huggingface/datasets/wikitext/wikitext-103-v1                      
                             /1.0.0/a241db52902eaf2c6aa732210bead40c090019a4                      
                             99ceb13bcbfa3f8ab646a126/cache-7649aa6a833efa01                      
                             _*_of_00008.arrow                                                    
[05/10/23 14:18:53] INFO     [0/8] __main__ - ***** Running training *****           logging.py:47
                    INFO     [0/8] __main__ -   Num examples = 115910                logging.py:47
                    INFO     [0/8] __main__ -   Num Epochs = 1                       logging.py:47
                    INFO     [0/8] __main__ -   Instantaneous batch size per device  logging.py:47
                             = 1                                                                  
                    INFO     [0/8] __main__ -   Total train batch size (w. parallel, logging.py:47
                             distributed & accumulation) = 8                                      
                    INFO     [0/8] __main__ -   Gradient Accumulation steps = 1      logging.py:47
                    INFO     [0/8] __main__ -   Total optimization steps = 100       logging.py:47
[05/10/23 14:18:57] INFO     [0/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [6/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [5/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [2/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [7/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [1/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [4/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
[05/10/23 14:18:57] INFO     [3/8] torch.nn.parallel.distributed - Reducer     distributed.py:1140
                             buckets have been rebuilt in this iteration.                         
