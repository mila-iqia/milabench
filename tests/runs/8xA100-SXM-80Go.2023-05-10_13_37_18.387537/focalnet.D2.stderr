Training with a single process on 1 device (cuda:0).
Model focalnet_base_lrf created, param count:88749768
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
	crop_mode: center
Learning rate (0.05) calculated from base learning rate (0.1) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.004 (7.00)  Time: 17.303s,    7.40/s  (17.303s,    7.40/s)  LR: 1.000e-05  Data: 0.839 (0.839)
Train: 0 [  31/32 (100%)]  Loss: 7.005 (7.00)  Time: 0.315s,  406.05/s  (0.889s,  143.95/s)  LR: 1.000e-05  Data: 0.001 (0.033)
Test: [   0/32]  Time: 0.917 (0.917)  Loss:  6.9615 (6.9615)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 2.305 (0.205)  Loss:  6.8640 (6.9459)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D2/20230510-141509-focalnet_base_lrf-224/checkpoint-0.pth.tar', 0.14534883720930233)

Train: 1 [   0/32 (  0%)]  Loss: 7.021 (7.02)  Time: 1.816s,   70.48/s  (1.816s,   70.48/s)  LR: 1.001e-02  Data: 0.551 (0.551)
Train: 1 [  31/32 (100%)]  Loss: 7.108 (7.05)  Time: 0.314s,  407.02/s  (0.368s,  347.84/s)  LR: 1.001e-02  Data: 0.001 (0.023)
Test: [   0/32]  Time: 0.582 (0.582)  Loss:  6.8922 (6.8922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.127)  Loss:  6.9391 (6.9700)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D2/20230510-141509-focalnet_base_lrf-224/checkpoint-1.pth.tar', 0.26647286821705424)

Train: 2 [   0/32 (  0%)]  Loss: 6.996 (7.00)  Time: 0.853s,  149.98/s  (0.853s,  149.98/s)  LR: 2.001e-02  Data: 0.539 (0.539)
Train: 2 [  31/32 (100%)]  Loss: 7.261 (7.13)  Time: 0.315s,  406.35/s  (0.338s,  379.17/s)  LR: 2.001e-02  Data: 0.001 (0.023)
Test: [   0/32]  Time: 0.581 (0.581)  Loss:  6.9301 (6.9301)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.7457 (7.1150)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  0.0000 ( 0.8479)
Train: 3 [   0/32 (  0%)]  Loss: 7.089 (7.09)  Time: 0.847s,  151.11/s  (0.847s,  151.11/s)  LR: 3.000e-02  Data: 0.533 (0.533)
Train: 3 [  31/32 (100%)]  Loss: 7.240 (7.23)  Time: 0.315s,  406.64/s  (0.337s,  379.91/s)  LR: 3.000e-02  Data: 0.001 (0.023)
Test: [   0/32]  Time: 0.577 (0.577)  Loss:  7.1713 (7.1713)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.031 (0.125)  Loss:  6.2462 (7.1749)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 25.0000 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 7.216 (7.22)  Time: 0.880s,  145.38/s  (0.880s,  145.38/s)  LR: 4.000e-02  Data: 0.566 (0.566)
Train: 4 [  31/32 (100%)]  Loss: 7.404 (7.33)  Time: 0.314s,  407.12/s  (0.337s,  379.32/s)  LR: 4.000e-02  Data: 0.000 (0.024)
Test: [   0/32]  Time: 0.569 (0.569)  Loss:  6.9702 (6.9702)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.8202 (7.2577)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  3.1250 ( 0.7025)
Train: 5 [   0/32 (  0%)]  Loss: 7.212 (7.21)  Time: 0.855s,  149.69/s  (0.855s,  149.69/s)  LR: 4.997e-02  Data: 0.541 (0.541)
Train: 5 [  31/32 (100%)]  Loss: 7.497 (7.45)  Time: 0.314s,  407.85/s  (0.337s,  379.87/s)  LR: 4.997e-02  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.583 (0.583)  Loss:  7.3900 (7.3900)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.031 (0.124)  Loss:  6.9167 (7.2936)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.7994)
Train: 6 [   0/32 (  0%)]  Loss: 7.311 (7.31)  Time: 0.851s,  150.47/s  (0.851s,  150.47/s)  LR: 4.995e-02  Data: 0.537 (0.537)
