Training with a single process on 1 device (cuda:0).
Model focalnet_base_lrf created, param count:88749768
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
	crop_mode: center
Learning rate (0.05) calculated from base learning rate (0.1) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.004 (7.00)  Time: 17.009s,    7.53/s  (17.009s,    7.53/s)  LR: 1.000e-05  Data: 0.743 (0.743)
Train: 0 [  31/32 (100%)]  Loss: 7.005 (7.00)  Time: 0.312s,  409.80/s  (0.877s,  145.96/s)  LR: 1.000e-05  Data: 0.000 (0.030)
Test: [   0/32]  Time: 0.798 (0.798)  Loss:  6.9616 (6.9616)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 2.053 (0.191)  Loss:  6.8639 (6.9459)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D3/20230510-141509-focalnet_base_lrf-224/checkpoint-0.pth.tar', 0.14534883720930233)

Train: 1 [   0/32 (  0%)]  Loss: 7.021 (7.02)  Time: 1.409s,   90.84/s  (1.409s,   90.84/s)  LR: 1.001e-02  Data: 0.512 (0.512)
Train: 1 [  31/32 (100%)]  Loss: 7.108 (7.05)  Time: 0.311s,  410.93/s  (0.352s,  363.23/s)  LR: 1.001e-02  Data: 0.001 (0.022)
Test: [   0/32]  Time: 0.570 (0.570)  Loss:  6.8922 (6.8922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.9391 (6.9699)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D3/20230510-141509-focalnet_base_lrf-224/checkpoint-1.pth.tar', 0.26647286821705424)

Train: 2 [   0/32 (  0%)]  Loss: 6.996 (7.00)  Time: 0.827s,  154.72/s  (0.827s,  154.72/s)  LR: 2.001e-02  Data: 0.514 (0.514)
Train: 2 [  31/32 (100%)]  Loss: 7.261 (7.13)  Time: 0.311s,  411.84/s  (0.333s,  384.25/s)  LR: 2.001e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.566 (0.566)  Loss:  6.9301 (6.9301)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.7455 (7.1150)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  0.0000 ( 0.8479)
Train: 3 [   0/32 (  0%)]  Loss: 7.089 (7.09)  Time: 0.820s,  156.08/s  (0.820s,  156.08/s)  LR: 3.000e-02  Data: 0.508 (0.508)
Train: 3 [  31/32 (100%)]  Loss: 7.240 (7.23)  Time: 0.311s,  410.98/s  (0.333s,  384.37/s)  LR: 3.000e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.578 (0.578)  Loss:  7.1711 (7.1711)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.2448 (7.1748)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 25.0000 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 7.216 (7.22)  Time: 0.833s,  153.60/s  (0.833s,  153.60/s)  LR: 4.000e-02  Data: 0.522 (0.522)
Train: 4 [  31/32 (100%)]  Loss: 7.404 (7.33)  Time: 0.312s,  409.74/s  (0.334s,  382.92/s)  LR: 4.000e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.591 (0.591)  Loss:  6.9697 (6.9697)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.8207 (7.2576)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  3.1250 ( 0.7025)
Train: 5 [   0/32 (  0%)]  Loss: 7.211 (7.21)  Time: 0.829s,  154.36/s  (0.829s,  154.36/s)  LR: 4.997e-02  Data: 0.517 (0.517)
Train: 5 [  31/32 (100%)]  Loss: 7.660 (7.46)  Time: 0.311s,  411.40/s  (0.334s,  383.62/s)  LR: 4.997e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.561 (0.561)  Loss:  6.9418 (6.9418)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.7286 (6.9690)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.9448)
Train: 6 [   0/32 (  0%)]  Loss: 7.380 (7.38)  Time: 0.842s,  152.01/s  (0.842s,  152.01/s)  LR: 4.995e-02  Data: 0.530 (0.530)
