Training with a single process on 1 device (cuda:0).
Model focalnet_base_lrf created, param count:88749768
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
	crop_mode: center
Learning rate (0.05) calculated from base learning rate (0.1) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.004 (7.00)  Time: 18.095s,    7.07/s  (18.095s,    7.07/s)  LR: 1.000e-05  Data: 0.792 (0.792)
Train: 0 [  31/32 (100%)]  Loss: 7.005 (7.00)  Time: 0.317s,  403.51/s  (0.924s,  138.51/s)  LR: 1.000e-05  Data: 0.000 (0.031)
Test: [   0/32]  Time: 0.853 (0.853)  Loss:  6.9616 (6.9616)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 2.346 (0.210)  Loss:  6.8638 (6.9459)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D4/20230510-141508-focalnet_base_lrf-224/checkpoint-0.pth.tar', 0.14534883720930233)

Train: 1 [   0/32 (  0%)]  Loss: 7.021 (7.02)  Time: 1.798s,   71.19/s  (1.798s,   71.19/s)  LR: 1.001e-02  Data: 0.545 (0.545)
Train: 1 [  31/32 (100%)]  Loss: 7.108 (7.05)  Time: 0.313s,  408.82/s  (0.367s,  349.01/s)  LR: 1.001e-02  Data: 0.001 (0.024)
Test: [   0/32]  Time: 0.608 (0.608)  Loss:  6.8922 (6.8922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.132)  Loss:  6.9392 (6.9699)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D4/20230510-141508-focalnet_base_lrf-224/checkpoint-1.pth.tar', 0.26647286821705424)

Train: 2 [   0/32 (  0%)]  Loss: 6.996 (7.00)  Time: 0.835s,  153.35/s  (0.835s,  153.35/s)  LR: 2.001e-02  Data: 0.521 (0.521)
Train: 2 [  31/32 (100%)]  Loss: 7.261 (7.13)  Time: 0.314s,  407.69/s  (0.335s,  381.56/s)  LR: 2.001e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.598 (0.598)  Loss:  6.9301 (6.9301)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)
Test: [  32/32]  Time: 0.030 (0.129)  Loss:  6.7457 (7.1150)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  0.0000 ( 0.8479)
Train: 3 [   0/32 (  0%)]  Loss: 7.089 (7.09)  Time: 0.882s,  145.12/s  (0.882s,  145.12/s)  LR: 3.000e-02  Data: 0.569 (0.569)
Train: 3 [  31/32 (100%)]  Loss: 7.240 (7.23)  Time: 0.313s,  409.48/s  (0.338s,  379.23/s)  LR: 3.000e-02  Data: 0.001 (0.024)
Test: [   0/32]  Time: 0.595 (0.595)  Loss:  7.1714 (7.1714)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.030 (0.129)  Loss:  6.2466 (7.1750)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 25.0000 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 7.216 (7.22)  Time: 0.910s,  140.74/s  (0.910s,  140.74/s)  LR: 4.000e-02  Data: 0.594 (0.594)
Train: 4 [  31/32 (100%)]  Loss: 7.404 (7.33)  Time: 0.316s,  404.44/s  (0.340s,  376.55/s)  LR: 4.000e-02  Data: 0.001 (0.026)
Test: [   0/32]  Time: 0.598 (0.598)  Loss:  6.9702 (6.9702)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.030 (0.128)  Loss:  6.8201 (7.2577)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  3.1250 ( 0.7025)
Train: 5 [   0/32 (  0%)]  Loss: 7.212 (7.21)  Time: 0.838s,  152.78/s  (0.838s,  152.78/s)  LR: 4.997e-02  Data: 0.524 (0.524)
Train: 5 [  31/32 (100%)]  Loss: 7.644 (7.46)  Time: 0.314s,  407.90/s  (0.338s,  379.19/s)  LR: 4.997e-02  Data: 0.001 (0.024)
Test: [   0/32]  Time: 0.597 (0.597)  Loss:  7.0919 (7.0919)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.128)  Loss:  7.1080 (7.1438)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.7025)
Train: 6 [   0/32 (  0%)]  Loss: 7.365 (7.37)  Time: 0.841s,  152.19/s  (0.841s,  152.19/s)  LR: 4.995e-02  Data: 0.528 (0.528)
