Training with a single process on 1 device (cuda:0).
Model focalnet_base_lrf created, param count:88749768
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
	crop_mode: center
Learning rate (0.05) calculated from base learning rate (0.1) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.004 (7.00)  Time: 17.449s,    7.34/s  (17.449s,    7.34/s)  LR: 1.000e-05  Data: 0.777 (0.777)
Train: 0 [  31/32 (100%)]  Loss: 7.005 (7.00)  Time: 0.312s,  410.26/s  (0.880s,  145.50/s)  LR: 1.000e-05  Data: 0.000 (0.030)
Test: [   0/32]  Time: 0.737 (0.737)  Loss:  6.9615 (6.9615)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 1.642 (0.183)  Loss:  6.8639 (6.9459)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  3.1250 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D6/20230510-141508-focalnet_base_lrf-224/checkpoint-0.pth.tar', 0.14534883720930233)

Train: 1 [   0/32 (  0%)]  Loss: 7.021 (7.02)  Time: 1.492s,   85.76/s  (1.492s,   85.76/s)  LR: 1.001e-02  Data: 0.525 (0.525)
Train: 1 [  31/32 (100%)]  Loss: 7.108 (7.05)  Time: 0.313s,  408.79/s  (0.355s,  360.11/s)  LR: 1.001e-02  Data: 0.001 (0.021)
Test: [   0/32]  Time: 0.563 (0.563)  Loss:  6.8922 (6.8922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.9391 (6.9699)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/focalnet.D6/20230510-141508-focalnet_base_lrf-224/checkpoint-1.pth.tar', 0.26647286821705424)

Train: 2 [   0/32 (  0%)]  Loss: 6.996 (7.00)  Time: 0.833s,  153.58/s  (0.833s,  153.58/s)  LR: 2.001e-02  Data: 0.520 (0.520)
Train: 2 [  31/32 (100%)]  Loss: 7.261 (7.13)  Time: 0.314s,  408.22/s  (0.335s,  382.38/s)  LR: 2.001e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.563 (0.563)  Loss:  6.9302 (6.9302)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)
Test: [  32/32]  Time: 0.030 (0.126)  Loss:  6.7455 (7.1150)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  0.0000 ( 0.8479)
Train: 3 [   0/32 (  0%)]  Loss: 7.089 (7.09)  Time: 0.858s,  149.19/s  (0.858s,  149.19/s)  LR: 3.000e-02  Data: 0.545 (0.545)
Train: 3 [  31/32 (100%)]  Loss: 7.239 (7.23)  Time: 0.312s,  410.25/s  (0.335s,  381.64/s)  LR: 3.000e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.579 (0.579)  Loss:  7.1710 (7.1710)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.2428 (7.1747)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 25.0000 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 7.215 (7.22)  Time: 0.870s,  147.16/s  (0.870s,  147.16/s)  LR: 4.000e-02  Data: 0.557 (0.557)
Train: 4 [  31/32 (100%)]  Loss: 7.405 (7.33)  Time: 0.313s,  409.01/s  (0.337s,  379.83/s)  LR: 4.000e-02  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.576 (0.576)  Loss:  6.9688 (6.9688)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.030 (0.124)  Loss:  6.8192 (7.2574)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  3.1250 ( 0.7025)
Train: 5 [   0/32 (  0%)]  Loss: 7.210 (7.21)  Time: 0.850s,  150.67/s  (0.850s,  150.67/s)  LR: 4.997e-02  Data: 0.534 (0.534)
Train: 5 [  31/32 (100%)]  Loss: 7.411 (7.45)  Time: 0.314s,  407.10/s  (0.336s,  381.29/s)  LR: 4.997e-02  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.573 (0.573)  Loss:  7.2761 (7.2761)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.030 (0.123)  Loss:  6.8474 (7.2048)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 0.9205)
Train: 6 [   0/32 (  0%)]  Loss: 7.199 (7.20)  Time: 0.866s,  147.87/s  (0.866s,  147.87/s)  LR: 4.995e-02  Data: 0.552 (0.552)
