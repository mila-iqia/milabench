Training with a single process on 1 device (cuda:0).
Model davit_large created, param count:196811752
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.005) calculated from base learning rate (0.01) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.224 (7.22)  Time: 8.241s,   15.53/s  (8.241s,   15.53/s)  LR: 1.000e-05  Data: 0.794 (0.794)
Train: 0 [  31/32 (100%)]  Loss: 7.311 (7.24)  Time: 0.414s,  309.50/s  (0.677s,  189.01/s)  LR: 1.000e-05  Data: 0.000 (0.031)
Test: [   0/32]  Time: 0.855 (0.855)  Loss:  7.1173 (7.1173)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 1.066 (0.199)  Loss:  7.0504 (7.2335)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  0.0000 ( 0.5329)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D2/20230510-140901-davit_large-224/checkpoint-0.pth.tar', 0.09689922480620156)

Train: 1 [   0/32 (  0%)]  Loss: 7.414 (7.41)  Time: 1.462s,   87.56/s  (1.462s,   87.56/s)  LR: 1.008e-03  Data: 0.529 (0.529)
Train: 1 [  31/32 (100%)]  Loss: 6.997 (7.05)  Time: 0.413s,  309.84/s  (0.450s,  284.47/s)  LR: 1.008e-03  Data: 0.001 (0.022)
Test: [   0/32]  Time: 0.603 (0.603)  Loss:  6.8691 (6.8691)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.040 (0.160)  Loss:  6.7074 (6.8682)  Acc@1:  0.0000 ( 0.2907)  Acc@5:  3.1250 ( 1.2839)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D2/20230510-140901-davit_large-224/checkpoint-1.pth.tar', 0.29069767441860467)

Train: 2 [   0/32 (  0%)]  Loss: 6.886 (6.89)  Time: 0.988s,  129.52/s  (0.988s,  129.52/s)  LR: 2.006e-03  Data: 0.554 (0.554)
Train: 2 [  31/32 (100%)]  Loss: 6.948 (6.95)  Time: 0.412s,  310.46/s  (0.435s,  294.18/s)  LR: 2.006e-03  Data: 0.001 (0.023)
Test: [   0/32]  Time: 0.604 (0.604)  Loss:  6.7543 (6.7543)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  2.3438 ( 2.3438)
Test: [  32/32]  Time: 0.040 (0.160)  Loss:  6.4865 (6.8226)  Acc@1:  3.1250 ( 0.2422)  Acc@5:  6.2500 ( 1.0417)
Train: 3 [   0/32 (  0%)]  Loss: 6.860 (6.86)  Time: 0.982s,  130.29/s  (0.982s,  130.29/s)  LR: 3.004e-03  Data: 0.543 (0.543)
Train: 3 [  31/32 (100%)]  Loss: 7.002 (6.96)  Time: 0.411s,  311.06/s  (0.434s,  294.61/s)  LR: 3.004e-03  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.624 (0.624)  Loss:  6.8251 (6.8251)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.041 (0.161)  Loss:  6.3623 (6.8163)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  6.2500 ( 1.0417)
Train: 4 [   0/32 (  0%)]  Loss: 6.859 (6.86)  Time: 0.979s,  130.74/s  (0.979s,  130.74/s)  LR: 4.002e-03  Data: 0.538 (0.538)
Train: 4 [  31/32 (100%)]  Loss: 7.079 (6.97)  Time: 0.409s,  312.99/s  (0.435s,  294.35/s)  LR: 4.002e-03  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.602 (0.602)  Loss:  6.8306 (6.8306)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  4.6875 ( 4.6875)
Test: [  32/32]  Time: 0.040 (0.161)  Loss:  6.7322 (6.8267)  Acc@1:  3.1250 ( 0.2665)  Acc@5:  3.1250 ( 1.1870)
Train: 5 [   0/32 (  0%)]  Loss: 6.830 (6.83)  Time: 0.968s,  132.21/s  (0.968s,  132.21/s)  LR: 4.997e-03  Data: 0.526 (0.526)
