  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 108.24it/s]
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 150.84it/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 159.63it/s]
100%|██████████| 3/3 [00:00<00:00, 155.92it/s]
100%|██████████| 3/3 [00:00<00:00, 172.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 165.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]loading configuration file config.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
100%|██████████| 3/3 [00:00<00:00, 161.57it/s]
Model config OPTConfig {
  "_name_or_path": "facebook/opt-6.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 16384,
  "hidden_size": 4096,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.27.4",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 4096
}

100%|██████████| 3/3 [00:00<00:00, 156.03it/s]
loading configuration file config.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-6.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 16384,
  "hidden_size": 4096,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.27.4",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 4096
}

loading file vocab.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/vocab.json
loading file merges.txt from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/merges.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/special_tokens_map.json
loading file tokenizer_config.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/tokenizer_config.json
loading configuration file config.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-6.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 16384,
  "hidden_size": 4096,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.27.4",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 4096
}

loading configuration file config.json from cache at /Tmp/slurm.3188069.0/milabench_dev/results/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-6.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 16384,
  "hidden_size": 4096,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.27.4",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 4096
}

Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1,
  "transformers_version": "4.27.4"
}

Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Creating extension directory /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118/utils...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...

Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...


Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...

Loading extension module utils...Loading extension module utils...No modifications detected for re-loaded extension module utils, skipping build step...


Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...

Loading extension module utils...Loading extension module utils...

No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /Tmp/slurm.3188069.0/milabench_dev/results/cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
