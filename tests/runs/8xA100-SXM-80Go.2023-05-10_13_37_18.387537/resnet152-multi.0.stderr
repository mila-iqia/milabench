Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 3, total 8, device cuda:3.
Training in distributed mode with multiple processes, 1 device per process.Process 4, total 8, device cuda:4.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 8, device cuda:1.
Training in distributed mode with multiple processes, 1 device per process.Process 6, total 8, device cuda:6.
Training in distributed mode with multiple processes, 1 device per process.Process 2, total 8, device cuda:2.
Training in distributed mode with multiple processes, 1 device per process.Process 5, total 8, device cuda:5.
Training in distributed mode with multiple processes, 1 device per process.Process 7, total 8, device cuda:7.
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 8, device cuda:0.
Model resnet152 created, param count:60192808
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.8) calculated from base learning rate (0.1) and global batch size (2048) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/2 (  0%)]  Loss: 6.941 (6.94)  Time: 16.749s,  122.28/s  (16.749s,  122.28/s)  LR: 1.000e-05  Data: 4.690 (4.690)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [   1/2 (100%)]  Loss: 6.942 (6.94)  Time: 1.082s, 1893.29/s  (8.915s,  229.72/s)  LR: 1.000e-05  Data: 0.024 (2.357)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.707 (1.707)  Loss:  6.9080 (6.9080)  Acc@1:  0.2441 ( 0.2441)  Acc@5:  0.7812 ( 0.7812)
Test: [   2/2]  Time: 1.276 (1.032)  Loss:  6.8999 (6.9076)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 0.6541)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/resnet152-multi.0/20230510-140535-resnet152-224/checkpoint-0.pth.tar', 0.12112403100775193)

Train: 1 [   0/2 (  0%)]  Loss: 6.945 (6.95)  Time: 1.796s, 1140.30/s  (1.796s, 1140.30/s)  LR: 1.600e-01  Data: 1.135 (1.135)
Train: 1 [   1/2 (100%)]  Loss: 6.921 (6.93)  Time: 0.373s, 5486.51/s  (1.085s, 1888.16/s)  LR: 1.600e-01  Data: 0.000 (0.568)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.118 (1.118)  Loss:  6.8983 (6.8983)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   2/2]  Time: 0.018 (0.416)  Loss:  6.8585 (6.8979)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 0.9932)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/resnet152-multi.0/20230510-140535-resnet152-224/checkpoint-1.pth.tar', 0.2180232558139535)

Train: 2 [   0/2 (  0%)]  Loss: 6.869 (6.87)  Time: 1.431s, 1430.99/s  (1.431s, 1430.99/s)  LR: 3.200e-01  Data: 1.062 (1.062)
Train: 2 [   1/2 (100%)]  Loss: 6.884 (6.88)  Time: 0.373s, 5489.98/s  (0.902s, 2270.24/s)  LR: 3.200e-01  Data: 0.000 (0.531)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.109 (1.109)  Loss:  6.8702 (6.8702)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [   2/2]  Time: 0.018 (0.412)  Loss:  6.7487 (6.8684)  Acc@1:  6.2500 ( 0.2180)  Acc@5: 28.1250 ( 1.1628)
Train: 3 [   0/2 (  0%)]  Loss: 6.836 (6.84)  Time: 1.451s, 1411.17/s  (1.451s, 1411.17/s)  LR: 4.800e-01  Data: 1.084 (1.084)
Train: 3 [   1/2 (100%)]  Loss: 6.930 (6.88)  Time: 0.379s, 5406.44/s  (0.915s, 2238.15/s)  LR: 4.800e-01  Data: 0.000 (0.542)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.121 (1.121)  Loss:  6.8391 (6.8391)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  0.4395 ( 0.4395)
Test: [   2/2]  Time: 0.020 (0.417)  Loss:  6.6033 (6.8367)  Acc@1:  9.3750 ( 0.2907)  Acc@5: 28.1250 ( 1.1386)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/resnet152-multi.0/20230510-140535-resnet152-224/checkpoint-3.pth.tar', 0.29069767441860467)

Train: 4 [   0/2 (  0%)]  Loss: 6.853 (6.85)  Time: 1.457s, 1406.08/s  (1.457s, 1406.08/s)  LR: 6.400e-01  Data: 1.087 (1.087)
Train: 4 [   1/2 (100%)]  Loss: 6.984 (6.92)  Time: 0.376s, 5444.82/s  (0.916s, 2234.99/s)  LR: 6.400e-01  Data: 0.001 (0.544)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.099 (1.099)  Loss:  6.8208 (6.8208)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.9766 ( 0.9766)
Test: [   2/2]  Time: 0.017 (0.409)  Loss:  6.5450 (6.8198)  Acc@1: 28.1250 ( 0.2180)  Acc@5: 28.1250 ( 1.1143)
Train: 5 [   0/2 (  0%)]  Loss: 6.884 (6.88)  Time: 1.482s, 1381.54/s  (1.482s, 1381.54/s)  LR: 7.995e-01  Data: 1.115 (1.115)
Train: 5 [   1/2 (100%)]  Loss: 6.981 (6.93)  Time: 0.375s, 5465.14/s  (0.929s, 2205.54/s)  LR: 7.995e-01  Data: 0.000 (0.558)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.106 (1.106)  Loss:  6.8242 (6.8242)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.6836 ( 0.6836)
Test: [   2/2]  Time: 0.020 (0.412)  Loss:  6.5234 (6.8192)  Acc@1:  0.0000 ( 0.2180)  Acc@5: 18.7500 ( 0.9205)
Train: 6 [   0/2 (  0%)]  Loss: 6.852 (6.85)  Time: 1.456s, 1406.22/s  (1.456s, 1406.22/s)  LR: 7.992e-01  Data: 1.087 (1.087)
Train: 6 [   1/2 (100%)]  Loss: 7.021 (6.94)  Time: 0.375s, 5460.57/s  (0.916s, 2236.49/s)  LR: 7.992e-01  Data: 0.001 (0.544)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.119 (1.119)  Loss:  6.8323 (6.8323)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8301 ( 0.8301)
Test: [   2/2]  Time: 0.021 (0.417)  Loss:  6.5049 (6.8240)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.0417)
Train: 7 [   0/2 (  0%)]  Loss: 6.876 (6.88)  Time: 1.603s, 1277.73/s  (1.603s, 1277.73/s)  LR: 7.989e-01  Data: 1.216 (1.216)
Train: 7 [   1/2 (100%)]  Loss: 6.984 (6.93)  Time: 0.376s, 5445.03/s  (0.989s, 2069.77/s)  LR: 7.989e-01  Data: 0.000 (0.608)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.096 (1.096)  Loss:  6.8381 (6.8381)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.019 (0.409)  Loss:  6.4825 (6.8268)  Acc@1:  0.0000 ( 0.1938)  Acc@5: 18.7500 ( 1.0174)
Train: 8 [   0/2 (  0%)]  Loss: 6.868 (6.87)  Time: 1.472s, 1391.77/s  (1.472s, 1391.77/s)  LR: 7.986e-01  Data: 1.104 (1.104)
Train: 8 [   1/2 (100%)]  Loss: 6.995 (6.93)  Time: 0.375s, 5460.74/s  (0.923s, 2218.20/s)  LR: 7.986e-01  Data: 0.000 (0.552)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.122 (1.122)  Loss:  6.8377 (6.8377)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.018 (0.417)  Loss:  6.6107 (6.8289)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.0659)
Train: 9 [   0/2 (  0%)]  Loss: 6.873 (6.87)  Time: 1.511s, 1355.10/s  (1.511s, 1355.10/s)  LR: 7.982e-01  Data: 1.120 (1.120)
Train: 9 [   1/2 (100%)]  Loss: 6.958 (6.92)  Time: 0.375s, 5461.73/s  (0.943s, 2171.45/s)  LR: 7.982e-01  Data: 0.000 (0.560)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.125 (1.125)  Loss:  6.8394 (6.8394)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  1.5137 ( 1.5137)
Test: [   2/2]  Time: 0.018 (0.418)  Loss:  6.6655 (6.8278)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.0174)
Train: 10 [   0/2 (  0%)]  Loss: 6.848 (6.85)  Time: 1.461s, 1401.91/s  (1.461s, 1401.91/s)  LR: 7.978e-01  Data: 1.094 (1.094)
Train: 10 [   1/2 (100%)]  Loss: 6.949 (6.90)  Time: 0.376s, 5442.39/s  (0.919s, 2229.52/s)  LR: 7.978e-01  Data: 0.001 (0.547)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.107 (1.107)  Loss:  6.8355 (6.8355)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.3672 ( 1.3672)
Test: [   2/2]  Time: 0.021 (0.413)  Loss:  6.6229 (6.8262)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 0.9448)
Train: 11 [   0/2 (  0%)]  Loss: 6.850 (6.85)  Time: 1.457s, 1405.82/s  (1.457s, 1405.82/s)  LR: 7.973e-01  Data: 1.088 (1.088)
Train: 11 [   1/2 (100%)]  Loss: 6.948 (6.90)  Time: 0.374s, 5470.59/s  (0.916s, 2236.83/s)  LR: 7.973e-01  Data: 0.001 (0.544)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.138 (1.138)  Loss:  6.8438 (6.8438)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.1719 ( 1.1719)
Test: [   2/2]  Time: 0.020 (0.423)  Loss:  6.5363 (6.8256)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 0.8963)
Train: 12 [   0/2 (  0%)]  Loss: 6.859 (6.86)  Time: 1.470s, 1393.18/s  (1.470s, 1393.18/s)  LR: 7.968e-01  Data: 1.102 (1.102)
Train: 12 [   1/2 (100%)]  Loss: 6.920 (6.89)  Time: 0.375s, 5468.07/s  (0.922s, 2220.59/s)  LR: 7.968e-01  Data: 0.000 (0.551)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.105 (1.105)  Loss:  6.8451 (6.8451)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.3672 ( 1.3672)
Test: [   2/2]  Time: 0.018 (0.411)  Loss:  6.5265 (6.8267)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.0901)
Train: 13 [   0/2 (  0%)]  Loss: 6.861 (6.86)  Time: 1.506s, 1360.05/s  (1.506s, 1360.05/s)  LR: 7.963e-01  Data: 1.114 (1.114)
Train: 13 [   1/2 (100%)]  Loss: 6.921 (6.89)  Time: 0.378s, 5422.68/s  (0.942s, 2174.68/s)  LR: 7.963e-01  Data: 0.000 (0.557)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.118 (1.118)  Loss:  6.8474 (6.8474)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  1.4648 ( 1.4648)
Test: [   2/2]  Time: 0.018 (0.416)  Loss:  6.5342 (6.8302)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.0901)
Train: 14 [   0/2 (  0%)]  Loss: 6.839 (6.84)  Time: 1.471s, 1392.40/s  (1.471s, 1392.40/s)  LR: 7.957e-01  Data: 1.099 (1.099)
Train: 14 [   1/2 (100%)]  Loss: 6.953 (6.90)  Time: 0.375s, 5465.27/s  (0.923s, 2219.37/s)  LR: 7.957e-01  Data: 0.000 (0.550)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.103 (1.103)  Loss:  6.8561 (6.8561)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.018 (0.410)  Loss:  6.5454 (6.8340)  Acc@1:  0.0000 ( 0.1453)  Acc@5:  0.0000 ( 0.9690)
Train: 15 [   0/2 (  0%)]  Loss: 6.837 (6.84)  Time: 1.468s, 1395.45/s  (1.468s, 1395.45/s)  LR: 7.951e-01  Data: 1.100 (1.100)
Train: 15 [   1/2 (100%)]  Loss: 6.927 (6.88)  Time: 0.378s, 5418.54/s  (0.923s, 2219.35/s)  LR: 7.951e-01  Data: 0.000 (0.550)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.107 (1.107)  Loss:  6.8392 (6.8392)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.2207 ( 1.2207)
Test: [   2/2]  Time: 0.019 (0.412)  Loss:  6.5846 (6.8255)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.1143)
Train: 16 [   0/2 (  0%)]  Loss: 6.841 (6.84)  Time: 1.423s, 1439.21/s  (1.423s, 1439.21/s)  LR: 7.944e-01  Data: 1.055 (1.055)
Train: 16 [   1/2 (100%)]  Loss: 6.915 (6.88)  Time: 0.375s, 5461.56/s  (0.899s, 2278.10/s)  LR: 7.944e-01  Data: 0.000 (0.528)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.102 (1.102)  Loss:  6.8297 (6.8297)  Acc@1:  0.1953 ( 0.1953)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.019 (0.411)  Loss:  6.3849 (6.8246)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 0.9932)
Train: 17 [   0/2 (  0%)]  Loss: 6.835 (6.83)  Time: 1.408s, 1454.36/s  (1.408s, 1454.36/s)  LR: 7.937e-01  Data: 1.037 (1.037)
Train: 17 [   1/2 (100%)]  Loss: 6.902 (6.87)  Time: 0.375s, 5460.72/s  (0.892s, 2296.96/s)  LR: 7.937e-01  Data: 0.000 (0.519)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.091 (1.091)  Loss:  6.8172 (6.8172)  Acc@1:  0.3418 ( 0.3418)  Acc@5:  1.7578 ( 1.7578)
Test: [   2/2]  Time: 0.019 (0.407)  Loss:  6.3184 (6.8199)  Acc@1:  0.0000 ( 0.2907)  Acc@5:  3.1250 ( 1.2112)
Train: 18 [   0/2 (  0%)]  Loss: 6.833 (6.83)  Time: 1.404s, 1458.30/s  (1.404s, 1458.30/s)  LR: 7.929e-01  Data: 1.037 (1.037)
Train: 18 [   1/2 (100%)]  Loss: 6.899 (6.87)  Time: 0.375s, 5460.70/s  (0.890s, 2301.87/s)  LR: 7.929e-01  Data: 0.000 (0.518)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.097 (1.097)  Loss:  6.8209 (6.8209)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.6113 ( 1.6113)
Test: [   2/2]  Time: 0.042 (0.417)  Loss:  6.4072 (6.8224)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.1628)
Train: 19 [   0/2 (  0%)]  Loss: 6.836 (6.84)  Time: 1.470s, 1393.58/s  (1.470s, 1393.58/s)  LR: 7.921e-01  Data: 1.085 (1.085)
Train: 19 [   1/2 (100%)]  Loss: 6.883 (6.86)  Time: 0.375s, 5463.16/s  (0.922s, 2220.69/s)  LR: 7.921e-01  Data: 0.000 (0.543)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.087 (1.087)  Loss:  6.8205 (6.8205)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.3184 ( 1.3184)
Test: [   2/2]  Time: 0.019 (0.406)  Loss:  6.4402 (6.8171)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.2597)
Train: 20 [   0/2 (  0%)]  Loss: 6.835 (6.84)  Time: 1.400s, 1462.98/s  (1.400s, 1462.98/s)  LR: 7.913e-01  Data: 1.032 (1.032)
Train: 20 [   1/2 (100%)]  Loss: 6.872 (6.85)  Time: 0.375s, 5459.24/s  (0.888s, 2307.57/s)  LR: 7.913e-01  Data: 0.000 (0.516)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.095 (1.095)  Loss:  6.8155 (6.8155)  Acc@1:  0.5859 ( 0.5859)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.018 (0.408)  Loss:  6.3844 (6.8131)  Acc@1:  0.0000 ( 0.4118)  Acc@5:  0.0000 ( 1.2112)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/resnet152-multi.0/20230510-140535-resnet152-224/checkpoint-20.pth.tar', 0.4118217054263566)

Train: 21 [   0/2 (  0%)]  Loss: 6.825 (6.83)  Time: 1.450s, 1412.72/s  (1.450s, 1412.72/s)  LR: 7.904e-01  Data: 1.082 (1.082)
Train: 21 [   1/2 (100%)]  Loss: 6.873 (6.85)  Time: 0.375s, 5458.81/s  (0.912s, 2244.56/s)  LR: 7.904e-01  Data: 0.000 (0.541)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.081 (1.081)  Loss:  6.8144 (6.8144)  Acc@1:  0.0488 ( 0.0488)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.019 (0.404)  Loss:  6.3628 (6.8142)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  0.0000 ( 1.1628)
Train: 22 [   0/2 (  0%)]  Loss: 6.820 (6.82)  Time: 1.408s, 1454.10/s  (1.408s, 1454.10/s)  LR: 7.894e-01  Data: 1.041 (1.041)
Train: 22 [   1/2 (100%)]  Loss: 6.871 (6.85)  Time: 0.374s, 5469.66/s  (0.891s, 2297.44/s)  LR: 7.894e-01  Data: 0.000 (0.521)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.082 (1.082)  Loss:  6.8121 (6.8121)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  0.7324 ( 0.7324)
Test: [   2/2]  Time: 0.018 (0.403)  Loss:  6.3422 (6.8064)  Acc@1:  0.0000 ( 0.1211)  Acc@5:  0.0000 ( 1.1143)
Train: 23 [   0/2 (  0%)]  Loss: 6.831 (6.83)  Time: 1.428s, 1433.88/s  (1.428s, 1433.88/s)  LR: 7.885e-01  Data: 1.061 (1.061)
Train: 23 [   1/2 (100%)]  Loss: 6.866 (6.85)  Time: 0.377s, 5437.46/s  (0.902s, 2269.33/s)  LR: 7.885e-01  Data: 0.000 (0.531)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.081 (1.081)  Loss:  6.8160 (6.8160)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  1.2207 ( 1.2207)
Test: [   2/2]  Time: 0.018 (0.403)  Loss:  6.4246 (6.8057)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  6.2500 ( 1.1143)
Train: 24 [   0/2 (  0%)]  Loss: 6.824 (6.82)  Time: 1.437s, 1425.63/s  (1.437s, 1425.63/s)  LR: 7.874e-01  Data: 1.069 (1.069)
Train: 24 [   1/2 (100%)]  Loss: 6.863 (6.84)  Time: 0.376s, 5452.64/s  (0.906s, 2260.29/s)  LR: 7.874e-01  Data: 0.001 (0.535)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.075 (1.075)  Loss:  6.8096 (6.8096)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.8789 ( 0.8789)
Test: [   2/2]  Time: 0.020 (0.402)  Loss:  6.4844 (6.8029)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  3.1250 ( 1.1143)
Train: 25 [   0/2 (  0%)]  Loss: 6.825 (6.82)  Time: 1.412s, 1450.73/s  (1.412s, 1450.73/s)  LR: 7.864e-01  Data: 1.045 (1.045)
Train: 25 [   1/2 (100%)]  Loss: 6.859 (6.84)  Time: 0.375s, 5456.45/s  (0.894s, 2292.06/s)  LR: 7.864e-01  Data: 0.000 (0.522)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.098 (1.098)  Loss:  6.8035 (6.8035)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.9766 ( 0.9766)
Test: [   2/2]  Time: 0.019 (0.409)  Loss:  6.4713 (6.8030)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2355)
Train: 26 [   0/2 (  0%)]  Loss: 6.816 (6.82)  Time: 1.434s, 1428.44/s  (1.434s, 1428.44/s)  LR: 7.853e-01  Data: 1.066 (1.066)
Train: 26 [   1/2 (100%)]  Loss: 6.865 (6.84)  Time: 0.375s, 5458.74/s  (0.904s, 2264.35/s)  LR: 7.853e-01  Data: 0.000 (0.533)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.095 (1.095)  Loss:  6.8012 (6.8012)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.5859 ( 0.5859)
Test: [   2/2]  Time: 0.020 (0.409)  Loss:  6.4713 (6.8033)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2112)
Train: 27 [   0/2 (  0%)]  Loss: 6.816 (6.82)  Time: 1.432s, 1429.95/s  (1.432s, 1429.95/s)  LR: 7.841e-01  Data: 1.063 (1.063)
Train: 27 [   1/2 (100%)]  Loss: 6.859 (6.84)  Time: 0.375s, 5464.82/s  (0.903s, 2266.77/s)  LR: 7.841e-01  Data: 0.001 (0.532)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.093 (1.093)  Loss:  6.7894 (6.7894)  Acc@1:  0.1953 ( 0.1953)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.020 (0.408)  Loss:  6.4219 (6.7948)  Acc@1:  0.0000 ( 0.3391)  Acc@5:  0.0000 ( 1.1386)
Train: 28 [   0/2 (  0%)]  Loss: 6.819 (6.82)  Time: 1.443s, 1418.90/s  (1.443s, 1418.90/s)  LR: 7.829e-01  Data: 1.074 (1.074)
Train: 28 [   1/2 (100%)]  Loss: 6.849 (6.83)  Time: 0.374s, 5471.39/s  (0.909s, 2253.41/s)  LR: 7.829e-01  Data: 0.000 (0.537)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.097 (1.097)  Loss:  6.7872 (6.7872)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  0.9277 ( 0.9277)
Test: [   2/2]  Time: 0.019 (0.409)  Loss:  6.3326 (6.7914)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1143)
Train: 29 [   0/2 (  0%)]  Loss: 6.824 (6.82)  Time: 1.437s, 1424.97/s  (1.437s, 1424.97/s)  LR: 7.817e-01  Data: 1.069 (1.069)
Train: 29 [   1/2 (100%)]  Loss: 6.838 (6.83)  Time: 0.376s, 5445.43/s  (0.907s, 2258.84/s)  LR: 7.817e-01  Data: 0.000 (0.535)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.095 (1.095)  Loss:  6.7864 (6.7864)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.4160 ( 1.4160)
Test: [   2/2]  Time: 0.019 (0.408)  Loss:  6.3058 (6.7906)  Acc@1:  0.0000 ( 0.3391)  Acc@5: 15.6250 ( 1.2839)
Train: 30 [   0/2 (  0%)]  Loss: 6.816 (6.82)  Time: 1.450s, 1412.66/s  (1.450s, 1412.66/s)  LR: 7.804e-01  Data: 1.082 (1.082)
Train: 30 [   1/2 (100%)]  Loss: 6.846 (6.83)  Time: 0.374s, 5469.54/s  (0.912s, 2245.39/s)  LR: 7.804e-01  Data: 0.000 (0.541)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.086 (1.086)  Loss:  6.7894 (6.7894)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  0.9277 ( 0.9277)
Test: [   2/2]  Time: 0.019 (0.406)  Loss:  6.2500 (6.7926)  Acc@1:  6.2500 ( 0.2907)  Acc@5: 28.1250 ( 1.2839)
Train: 31 [   0/2 (  0%)]  Loss: 6.826 (6.83)  Time: 1.435s, 1427.11/s  (1.435s, 1427.11/s)  LR: 7.791e-01  Data: 1.044 (1.044)
Train: 31 [   1/2 (100%)]  Loss: 6.844 (6.84)  Time: 0.375s, 5458.57/s  (0.905s, 2262.66/s)  LR: 7.791e-01  Data: 0.001 (0.522)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.086 (1.086)  Loss:  6.7918 (6.7918)  Acc@1:  0.1953 ( 0.1953)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.018 (0.405)  Loss:  6.2606 (6.7911)  Acc@1:  0.0000 ( 0.2665)  Acc@5: 28.1250 ( 1.3324)
Train: 32 [   0/2 (  0%)]  Loss: 6.808 (6.81)  Time: 1.437s, 1424.79/s  (1.437s, 1424.79/s)  LR: 7.778e-01  Data: 1.070 (1.070)
Train: 32 [   1/2 (100%)]  Loss: 6.843 (6.83)  Time: 0.376s, 5450.88/s  (0.907s, 2259.09/s)  LR: 7.778e-01  Data: 0.000 (0.535)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.092 (1.092)  Loss:  6.7956 (6.7956)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  1.0742 ( 1.0742)
Test: [   2/2]  Time: 0.020 (0.408)  Loss:  6.3547 (6.7903)  Acc@1:  0.0000 ( 0.3391)  Acc@5:  0.0000 ( 1.2839)
Train: 33 [   0/2 (  0%)]  Loss: 6.817 (6.82)  Time: 1.454s, 1408.33/s  (1.454s, 1408.33/s)  LR: 7.764e-01  Data: 1.088 (1.088)
Train: 33 [   1/2 (100%)]  Loss: 6.839 (6.83)  Time: 0.376s, 5452.13/s  (0.915s, 2238.45/s)  LR: 7.764e-01  Data: 0.000 (0.544)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.075 (1.075)  Loss:  6.7942 (6.7942)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  0.7324 ( 0.7324)
Test: [   2/2]  Time: 0.019 (0.402)  Loss:  6.4252 (6.7889)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.2839)
Train: 34 [   0/2 (  0%)]  Loss: 6.802 (6.80)  Time: 1.443s, 1418.88/s  (1.443s, 1418.88/s)  LR: 7.749e-01  Data: 1.076 (1.076)
Train: 34 [   1/2 (100%)]  Loss: 6.840 (6.82)  Time: 0.376s, 5439.89/s  (0.910s, 2250.70/s)  LR: 7.749e-01  Data: 0.000 (0.538)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.098 (1.098)  Loss:  6.7937 (6.7937)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  0.9277 ( 0.9277)
Test: [   2/2]  Time: 0.018 (0.409)  Loss:  6.4498 (6.7882)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1870)
Train: 35 [   0/2 (  0%)]  Loss: 6.814 (6.81)  Time: 1.438s, 1424.54/s  (1.438s, 1424.54/s)  LR: 7.734e-01  Data: 1.070 (1.070)
Train: 35 [   1/2 (100%)]  Loss: 6.842 (6.83)  Time: 0.377s, 5432.34/s  (0.907s, 2257.18/s)  LR: 7.734e-01  Data: 0.000 (0.535)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.094 (1.094)  Loss:  6.7948 (6.7948)  Acc@1:  0.4395 ( 0.4395)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.018 (0.408)  Loss:  6.4404 (6.7873)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.1870)
Train: 36 [   0/2 (  0%)]  Loss: 6.815 (6.81)  Time: 1.444s, 1418.21/s  (1.444s, 1418.21/s)  LR: 7.719e-01  Data: 1.077 (1.077)
Train: 36 [   1/2 (100%)]  Loss: 6.835 (6.82)  Time: 0.375s, 5462.74/s  (0.909s, 2251.81/s)  LR: 7.719e-01  Data: 0.000 (0.539)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.084 (1.084)  Loss:  6.7981 (6.7981)  Acc@1:  0.4883 ( 0.4883)  Acc@5:  0.9766 ( 0.9766)
Test: [   2/2]  Time: 0.019 (0.405)  Loss:  6.3947 (6.7866)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  3.1250 ( 1.1628)
Train: 37 [   0/2 (  0%)]  Loss: 6.815 (6.81)  Time: 1.431s, 1430.98/s  (1.431s, 1430.98/s)  LR: 7.703e-01  Data: 1.063 (1.063)
Train: 37 [   1/2 (100%)]  Loss: 6.832 (6.82)  Time: 0.374s, 5471.45/s  (0.903s, 2268.63/s)  LR: 7.703e-01  Data: 0.000 (0.532)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.104 (1.104)  Loss:  6.7972 (6.7972)  Acc@1:  0.1465 ( 0.1465)  Acc@5:  0.9277 ( 0.9277)
Test: [   2/2]  Time: 0.020 (0.412)  Loss:  6.3588 (6.7860)  Acc@1: 21.8750 ( 0.2422)  Acc@5: 28.1250 ( 1.2112)
Train: 38 [   0/2 (  0%)]  Loss: 6.821 (6.82)  Time: 1.459s, 1403.86/s  (1.459s, 1403.86/s)  LR: 7.687e-01  Data: 1.091 (1.091)
Train: 38 [   1/2 (100%)]  Loss: 6.827 (6.82)  Time: 0.376s, 5452.78/s  (0.917s, 2232.86/s)  LR: 7.687e-01  Data: 0.000 (0.545)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.107 (1.107)  Loss:  6.7966 (6.7966)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.6348 ( 0.6348)
Test: [   2/2]  Time: 0.018 (0.412)  Loss:  6.3716 (6.7863)  Acc@1: 25.0000 ( 0.2422)  Acc@5: 28.1250 ( 1.2597)
Train: 39 [   0/2 (  0%)]  Loss: 6.807 (6.81)  Time: 1.442s, 1420.68/s  (1.442s, 1420.68/s)  LR: 7.671e-01  Data: 1.074 (1.074)
Train: 39 [   1/2 (100%)]  Loss: 6.836 (6.82)  Time: 0.375s, 5460.57/s  (0.908s, 2254.74/s)  LR: 7.671e-01  Data: 0.000 (0.537)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.096 (1.096)  Loss:  6.7951 (6.7951)  Acc@1:  0.3418 ( 0.3418)  Acc@5:  1.1230 ( 1.1230)
Test: [   2/2]  Time: 0.019 (0.409)  Loss:  6.3977 (6.7860)  Acc@1:  0.0000 ( 0.1938)  Acc@5:  9.3750 ( 1.2355)
Train: 40 [   0/2 (  0%)]  Loss: 6.821 (6.82)  Time: 1.438s, 1424.09/s  (1.438s, 1424.09/s)  LR: 7.654e-01  Data: 1.070 (1.070)
Train: 40 [   1/2 (100%)]  Loss: 6.819 (6.82)  Time: 0.374s, 5471.67/s  (0.906s, 2259.98/s)  LR: 7.654e-01  Data: 0.000 (0.535)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.092 (1.092)  Loss:  6.7963 (6.7963)  Acc@1:  0.5859 ( 0.5859)  Acc@5:  1.5625 ( 1.5625)
Test: [   2/2]  Time: 0.018 (0.407)  Loss:  6.4097 (6.7853)  Acc@1:  0.0000 ( 0.3149)  Acc@5:  0.0000 ( 1.1870)
Train: 41 [   0/2 (  0%)]  Loss: 6.804 (6.80)  Time: 1.477s, 1386.48/s  (1.477s, 1386.48/s)  LR: 7.637e-01  Data: 1.110 (1.110)
Train: 41 [   1/2 (100%)]  Loss: 6.834 (6.82)  Time: 0.374s, 5472.03/s  (0.926s, 2212.40/s)  LR: 7.637e-01  Data: 0.000 (0.555)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.097 (1.097)  Loss:  6.7972 (6.7972)  Acc@1:  0.5371 ( 0.5371)  Acc@5:  1.0254 ( 1.0254)
Test: [   2/2]  Time: 0.018 (0.408)  Loss:  6.4408 (6.7863)  Acc@1:  0.0000 ( 0.2665)  Acc@5:  0.0000 ( 1.1628)
Train: 42 [   0/2 (  0%)]  Loss: 6.799 (6.80)  Time: 1.603s, 1277.31/s  (1.603s, 1277.31/s)  LR: 7.619e-01  Data: 1.215 (1.215)
Train: 42 [   1/2 (100%)]  Loss: 6.839 (6.82)  Time: 0.374s, 5470.70/s  (0.989s, 2071.06/s)  LR: 7.619e-01  Data: 0.000 (0.607)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.100 (1.100)  Loss:  6.7978 (6.7978)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [   2/2]  Time: 0.019 (0.410)  Loss:  6.4358 (6.7876)  Acc@1:  0.0000 ( 0.3391)  Acc@5:  0.0000 ( 1.3081)
Train: 43 [   0/2 (  0%)]  Loss: 6.802 (6.80)  Time: 1.435s, 1427.17/s  (1.435s, 1427.17/s)  LR: 7.601e-01  Data: 1.065 (1.065)
Train: 43 [   1/2 (100%)]  Loss: 6.830 (6.82)  Time: 0.376s, 5446.16/s  (0.906s, 2261.67/s)  LR: 7.601e-01  Data: 0.000 (0.533)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.095 (1.095)  Loss:  6.7962 (6.7962)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  0.8301 ( 0.8301)
Test: [   2/2]  Time: 0.020 (0.409)  Loss:  6.3798 (6.7849)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.3081)
Train: 44 [   0/2 (  0%)]  Loss: 6.809 (6.81)  Time: 1.442s, 1420.00/s  (1.442s, 1420.00/s)  LR: 7.583e-01  Data: 1.074 (1.074)
Train: 44 [   1/2 (100%)]  Loss: 6.828 (6.82)  Time: 0.374s, 5476.35/s  (0.908s, 2255.23/s)  LR: 7.583e-01  Data: 0.000 (0.537)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.086 (1.086)  Loss:  6.7923 (6.7923)  Acc@1:  0.1465 ( 0.1465)  Acc@5:  0.6836 ( 0.6836)
Test: [   2/2]  Time: 0.020 (0.406)  Loss:  6.3741 (6.7845)  Acc@1:  0.0000 ( 0.2422)  Acc@5:  0.0000 ( 1.1628)
Train: 45 [   0/2 (  0%)]  Loss: 6.818 (6.82)  Time: 1.441s, 1421.70/s  (1.441s, 1421.70/s)  LR: 7.564e-01  Data: 1.072 (1.072)
Train: 45 [   1/2 (100%)]  Loss: 6.816 (6.82)  Time: 0.376s, 5446.99/s  (0.908s, 2254.87/s)  LR: 7.564e-01  Data: 0.001 (0.536)
Distributing BatchNorm running means and vars
Test: [   0/2]  Time: 1.091 (1.091)  Loss:  6.7914 (6.7914)  Acc@1:  0.0488 ( 0.0488)  Acc@5:  0.8301 ( 0.8301)
Test: [   2/2]  Time: 0.028 (0.410)  Loss:  6.3840 (6.7846)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 1.0417)
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686683 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686684 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686685 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686686 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686688 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686689 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686690 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1686691 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/mila/d/delaunap/scratch/milabench/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/mila/d/delaunap/scratch/milabench/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1686551 got signal: 15
