Training with a single process on 1 device (cuda:0).
Model davit_large created, param count:196811752
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.95
	crop_mode: center
Learning rate (0.005) calculated from base learning rate (0.01) and global batch size (128) with linear scaling.
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/32 (  0%)]  Loss: 7.224 (7.22)  Time: 7.433s,   17.22/s  (7.433s,   17.22/s)  LR: 1.000e-05  Data: 0.729 (0.729)
Train: 0 [  31/32 (100%)]  Loss: 7.311 (7.24)  Time: 0.410s,  312.17/s  (0.677s,  188.98/s)  LR: 1.000e-05  Data: 0.001 (0.029)
Test: [   0/32]  Time: 0.728 (0.728)  Loss:  7.1174 (7.1174)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  32/32]  Time: 0.848 (0.189)  Loss:  7.0511 (7.2335)  Acc@1:  0.0000 ( 0.0969)  Acc@5:  0.0000 ( 0.5329)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D0/20230510-140900-davit_large-224/checkpoint-0.pth.tar', 0.09689922480620156)

Train: 1 [   0/32 (  0%)]  Loss: 7.414 (7.41)  Time: 1.317s,   97.21/s  (1.317s,   97.21/s)  LR: 1.008e-03  Data: 0.553 (0.553)
Train: 1 [  31/32 (100%)]  Loss: 6.997 (7.05)  Time: 0.409s,  312.76/s  (0.445s,  287.76/s)  LR: 1.008e-03  Data: 0.000 (0.023)
Test: [   0/32]  Time: 0.603 (0.603)  Loss:  6.8693 (6.8693)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.040 (0.161)  Loss:  6.7070 (6.8682)  Acc@1:  0.0000 ( 0.2907)  Acc@5:  3.1250 ( 1.2839)
Current checkpoints:
 ('/Tmp/slurm.3188069.0/milabench_dev/results/extra/timm/8xA100-SXM-80Go.2023-05-10_13:37:18.387537/davit_large.D0/20230510-140900-davit_large-224/checkpoint-1.pth.tar', 0.29069767441860467)

Train: 2 [   0/32 (  0%)]  Loss: 6.886 (6.89)  Time: 0.950s,  134.80/s  (0.950s,  134.80/s)  LR: 2.006e-03  Data: 0.514 (0.514)
Train: 2 [  31/32 (100%)]  Loss: 6.949 (6.95)  Time: 0.409s,  312.82/s  (0.434s,  295.24/s)  LR: 2.006e-03  Data: 0.000 (0.022)
Test: [   0/32]  Time: 0.599 (0.599)  Loss:  6.7509 (6.7509)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  2.3438 ( 2.3438)
Test: [  32/32]  Time: 0.040 (0.160)  Loss:  6.4905 (6.8227)  Acc@1:  3.1250 ( 0.2422)  Acc@5:  6.2500 ( 1.1143)
Train: 3 [   0/32 (  0%)]  Loss: 6.861 (6.86)  Time: 1.005s,  127.33/s  (1.005s,  127.33/s)  LR: 3.004e-03  Data: 0.578 (0.578)
Train: 3 [  31/32 (100%)]  Loss: 7.003 (6.96)  Time: 0.411s,  311.80/s  (0.435s,  294.49/s)  LR: 3.004e-03  Data: 0.000 (0.024)
Test: [   0/32]  Time: 0.598 (0.598)  Loss:  6.8218 (6.8218)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.7812 ( 0.7812)
Test: [  32/32]  Time: 0.040 (0.161)  Loss:  6.3690 (6.8168)  Acc@1:  0.0000 ( 0.1696)  Acc@5:  3.1250 ( 1.0174)
Train: 4 [   0/32 (  0%)]  Loss: 6.862 (6.86)  Time: 0.973s,  131.60/s  (0.973s,  131.60/s)  LR: 4.002e-03  Data: 0.530 (0.530)
