_llm:
  voir:
    options:
      stop: 30
  
  max_duration: 1200

  inherits: _defaults
  definition: .
  install-variant: unpinned
  install_group: torch
  plan:
    method: per_gpu


llm-qlora-single:
  inherits: _llm
  definition: .
  install-variant: unpinned
  plan:
    method: per_gpu

  argv:
    "{milabench_code}/recipes/lora_finetune_single_device.py": true
    --config: "{milabench_code}/configs/llama3_8B_lora_single_device.yaml"
    epochs=1: true
    tokenizer.path={milabench_data}/{milabench_name}/chckpt/original/tokenizer.model: true
    output_dir={milabench_extra}/output: true
    checkpointer.checkpoint_dir={milabench_data}/{milabench_name}/chckpt/original: true
    checkpointer.output_dir={milabench_extra}/chckpt: true
    metric_logger.log_dir={milabench_extra}/metrics: true
    repo_id="meta-llama/Meta-Llama-3.1-8B": true
    batch_size=8: true
    gradient_accumulation_steps=8: true


llm-qlora-ddp-gpus:
  inherits: _llm
  definition: .
  install-variant: unpinned
  plan:
    method: njobs
    n: 1


llm-qlora-ddp-nodes:
  inherits: _llm
  definition: .
  install-variant: unpinned
  plan:
    method: njobs
    n: 1


llm-qlora-mp-gpus:
  inherits: _llm
  definition: .
  install-variant: unpinned
  plan:
    method: njobs
    n: 1


llm-qlora-mp-nodes:
  inherits: _llm
  definition: .
  install-variant: unpinned
  plan:
    method: njobs
    n: 1
