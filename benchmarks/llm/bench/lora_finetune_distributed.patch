diff --git a/original/lora_finetune_distributed.py b/lora_finetune_distributed.py
old mode 100644
new mode 100755
index 5e1766e..3989e54
--- a/original/lora_finetune_distributed.py
+++ b/lora_finetune_distributed.py
@@ -15,7 +15,7 @@ import torch
 from omegaconf import DictConfig, ListConfig
 
 from torch import nn
-from torch.distributed import destroy_process_group, init_process_group
+from torch.distributed import destroy_process_group
 from torch.distributed.tensor import DTensor
 
 from torch.optim import Optimizer
@@ -148,7 +148,9 @@ class LoRAFinetuneRecipeDistributed(FTRecipeInterface):
             offload_ops_to_cpu=self.fsdp_cpu_offload
             or self._enable_async_checkpointing,
         )
-        init_process_group(self.distributed_backend)
+
+        import torchcompat as acc
+        acc.init_process_group(self.distributed_backend)
 
         self.world_size, self.rank = utils.get_world_size_and_rank()
 
@@ -802,6 +804,7 @@ class LoRAFinetuneRecipeDistributed(FTRecipeInterface):
                     self.global_step += 1
 
                     loss_to_log = running_loss.detach().item() / num_tokens
+                    self.log_loss(loss_to_log)
                     pbar.update(1)
                     pbar.set_description(
                         f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
@@ -953,6 +956,9 @@ class LoRAFinetuneRecipeDistributed(FTRecipeInterface):
             self._metric_logger.close()
         destroy_process_group()
 
+    def log_loss(self, loss):
+        pass
+
 
 @config.parse
 def recipe_main(cfg: DictConfig) -> None:
@@ -973,11 +979,27 @@ def recipe_main(cfg: DictConfig) -> None:
         # speed up when benchmarking fused AdamW on CPU
         training.set_torch_num_threads()
 
+    # <<<<
+    import sys, os
+    current_dir = os.path.dirname(os.path.abspath(__file__))
+    sys.path.append(os.path.join(current_dir))
+    # >>>>
+
     config.log_config(recipe_name="LoRAFinetuneRecipeDistributed", cfg=cfg)
 
     recipe = LoRAFinetuneRecipeDistributed(cfg=cfg)
     recipe.setup(cfg=cfg)
-    recipe.train()
+    # <<<<
+    from voir.phase import StopProgram
+    try:
+        from .utils import prepare_voir
+        _, monitor = prepare_voir(recipe)
+        with monitor():
+            recipe.train()
+    
+    except StopProgram:
+        print("early stopping")
+    # >>>>
     recipe.cleanup()
 
 
