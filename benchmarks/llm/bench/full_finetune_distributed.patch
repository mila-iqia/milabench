diff --git a/original/full_finetune_distributed.py b/full_finetune_distributed.py
old mode 100644
new mode 100755
index ea11f23..80aa14d
--- a/original/full_finetune_distributed.py
+++ b/full_finetune_distributed.py
@@ -16,7 +16,7 @@ import torch
 from omegaconf import DictConfig, ListConfig
 
 from torch import nn
-from torch.distributed import destroy_process_group, init_process_group
+from torch.distributed import destroy_process_group
 from torch.distributed.tensor import DTensor
 from torch.distributed.tensor.parallel import parallelize_module
 from torch.optim import Optimizer
@@ -148,7 +148,9 @@ class FullFinetuneRecipeDistributed(FTRecipeInterface):
             offload_ops_to_cpu=self.fsdp_cpu_offload
             or self._enable_async_checkpointing,
         )
-        init_process_group(self.distributed_backend)
+
+        import torchcompat as acc
+        acc.init_process_group(self.distributed_backend)
 
         # Initialize distributed variables
         self.world_size, self.rank = utils.get_world_size_and_rank()
@@ -1067,6 +1069,8 @@ class FullFinetuneRecipeDistributed(FTRecipeInterface):
                         precompute_float8_dynamic_scale_for_fsdp(self._model)
 
                     loss_to_log = running_loss.detach().item() / num_tokens
+                    self.log_loss(loss_to_log)
+            
                     pbar.update(1)
                     pbar.set_description(
                         f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
@@ -1148,6 +1152,9 @@ class FullFinetuneRecipeDistributed(FTRecipeInterface):
             self._metric_logger.close()
         destroy_process_group()
 
+    def log_loss(self, loss):
+        pass
+
 
 @config.parse
 def recipe_main(cfg: DictConfig) -> None:
@@ -1158,10 +1165,29 @@ def recipe_main(cfg: DictConfig) -> None:
         - Parameters specified in config (see available configs through ``tune ls``)
         - Overwritten by arguments from the command-line
     """
+
+    # <<<<
+    import sys, os
+    current_dir = os.path.dirname(os.path.abspath(__file__))
+    sys.path.append(os.path.join(current_dir))
+    # >>>>
+
     config.log_config(recipe_name="FullFinetuneRecipeDistributed", cfg=cfg)
     recipe = FullFinetuneRecipeDistributed(cfg=cfg)
     recipe.setup(cfg=cfg)
-    recipe.train()
+
+    # <<<<
+    from voir.phase import StopProgram
+    try:
+        from .utils import prepare_voir
+        _, monitor = prepare_voir(recipe)
+        with monitor():
+            recipe.train()
+    
+    except StopProgram:
+        print("early stopping")
+    # >>>>
+
     recipe.cleanup()
 
 
